{
  "hash": "8f8c4bd106641afeafad16456d8f8294",
  "result": {
    "engine": "knitr",
    "markdown": "# Ordinary Least Squares {#sec-ols}\n\n## In-class Code\n\nRemember that our goal is to estimate the linear relationship between data observations of response variable, $y$, and its measured covariate, $x$, following: $Y = XB + \\epsilon$, where $\\epsilon \\sim N(0, \\sigma^2 I).$ Our coefficients to estimate are therefore $\\hat{B}$, which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), $\\hat{\\sigma}$. To estimate the coefficients, we are attempting to minimize the residual sum of squares, $|| \\epsilon || ^ 2$. See [Footnotes @sec-crossprod] for more information regarding this notation.\n\n## Generate the data\n\nWe'll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we'll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    1    5\n[4,]    1    1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    8\n[2,]   10\n[3,]   14\n[4,]    6\n```\n\n\n:::\n\n```{.r .cell-code}\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon\n```\n:::\n\n\n## Plot the relationship\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Estimate the coefficients using R's `lm()` function {#sec-lm-output}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 1 + x1)\n\nResiduals:\n     1      2      3      4 \n-1.029 -1.657  1.086  1.600 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   5.7714     2.0500   2.815    0.106\nx1            1.6286     0.6565   2.481    0.131\n\nResidual standard error: 1.942 on 2 degrees of freedom\nMultiple R-squared:  0.7547,\tAdjusted R-squared:  0.6321 \nF-statistic: 6.153 on 1 and 2 DF,  p-value: 0.1313\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          x1 \n   5.771429    1.628571 \n```\n\n\n:::\n:::\n\n\n## Estimate the coefficients manually\n\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, $Y$. Remember that we estimate the coefficient vector, $\\hat{B}$ from: $$X^TX \\hat{B} = X^T Y$$ $$\\hat{B} = (X^TX)^{-1} X^T Y$$ These equations include the multiplicative inverse matrix, $(X^TX)^{-1}$. See the [Footnotes @sec-solve] for more information about inverse matrices and the `solve()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?\n```\n:::\n\n\n## Plot the *estimated* relationships {#sec-est-plot}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Why are the $\\hat{B}$ different from true $B$?\n\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), $|| \\epsilon ||^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# True sum of squares:\nsum(epsilon^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.542857\n```\n\n\n:::\n\n```{.r .cell-code}\n## From manual OLS\nsum( (y - xmat %*% bhat_solve)^2 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.542857\n```\n\n\n:::\n:::\n\nYou can see that the OLS strategy minimized SSE, but this is actually lower than the true SSE.\n\n\n## Understanding Uncertainty in $\\hat{B}$\n\nWhile the OLS analysis estimates the regression coefficients, $\\hat{B}$, from the observed data $Y$, our estimates of these coefficients have error (i.e., uncertainty), such that the estimates are only as good as the data. Specifically, if we have fewer data points (i.e., $n$ is low), we have less certainty in $\\hat{B}$. In lecture, we showed, that:\n$$\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right), $$\nand we know that $\\hat{\\sigma}^2$ depends on sample size $n$, following:\n$$\\hat{\\sigma}^2 \\quad = \\quad \\frac{1}{n-p} (Y_{obs} - Y_{pred})^T (Y_{obs} - Y_{pred}) \\quad = \\quad \\frac{1}{n-p} \\hat{\\epsilon}^T \\hat{\\epsilon}$$\n\nUsing these equations, we showed then that $SE(\\beta_i) = \\sqrt{diag\\left( (X^TX)^{-1} \\right)_i \\hat{\\sigma}^2}$. Let's calculate this manually and compare to the output of the `lm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the model summary, which has useful components\nlm_out_summary = summary(lm_out)\n# Extract the estimated residual standard deviation, sigma\nest_sigma = lm_out_summary$sigma\nest_sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.942017\n```\n\n\n:::\n\n```{.r .cell-code}\n# We already calculated (X^T X)^{-1} as inv_xtx\nbeta_cov_mat = inv_xtx * est_sigma^2\nbeta_cov_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]       [,2]\n[1,]  4.202449 -1.1853061\n[2,] -1.185306  0.4310204\n```\n\n\n:::\n\n```{.r .cell-code}\nse_beta = sqrt(diag(beta_cov_mat))\nse_beta\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.0499876 0.6565214\n```\n\n\n:::\n:::\n\nCompare these values to the output of the `summary()` of @sec-lm-output in the column labelled `Std. Error`.  \n\n## Confidence Intervals for $\\hat{B}$ {#sec-conf-beta}\n\nTo calculate confidence intervals for $\\hat{B}$, we first must understand the $t$ (a.k.a. Student's $t$) probability distribution. This distribution represents the case when we are estimating the mean of a normally distributed variable and either the sample size is small or the variable's standard deviation is unknown. Essentially, the $t$ distribution increases the uncertainty (i.e., variance) in cases of low sample size (i.e., small $n$). With low sample size (and/or high number of parameters), the degrees of freedom of the $t$-distribution, $\\nu$ is low, whereas with high sample size, $\\nu$ is large. As $\\nu$ approaches infinity, the $t$-distribution approximates the standard normal distribution (i.e., $N(\\mu, \\sigma)|\\mu=0,\\sigma=1$). \n\n::: {.cell}\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nIt is the case for $\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right)$ that we do not know the mean ($B$), and we are estimating the variance, $\\hat{\\sigma}^2$. Specifically, we are estimating the true mean vector, $B$, as $\\hat{B}$, and we are estimating the variance of the residuals as $\\hat{\\sigma}^2$. We can therefore re-write the uncertainty in $\\hat{B}$ as a multivariate $t$ distribution: \n$$(\\hat{B} - B) \\sim t_{\\nu} \\left( 0, \\Sigma \\right),$$\nwhere the means are zero, $\\nu$ is the degrees of freedom (i.e., $n-p$), and $\\Sigma = (X^TX)^{-1} \\hat{\\sigma}^2$. $(\\hat{B} - B)$ represents the deviation of the estimated coefficients from the true coefficients, which is why the distribution is centered around zero. It is perhaps easier to separate the individual estimated coefficients, $\\beta_i$, into their separate $t$-distributions:\n$$\\frac{(\\hat{\\beta}_i - \\beta_i)}{SE(\\hat{\\beta}_i)} \\sim t_{\\nu}$$\n$$(\\hat{\\beta}_i - \\beta_i) \\sim t_{\\nu} SE(\\hat{\\beta}_i),$$\nwhich shows that the $t$-distribution that describes the deviation of regression coefficients from the true value of those coefficients is scaled by the uncertainty in the estimated coefficients $SE(\\hat{\\beta}_i)$. As shown in Dr. Barber's materials, using this information, we can derive the confidence interval (at the $\\alpha$ confidence level) calculation for $\\hat{\\beta}_i$ as: \n$$ \\hat{\\beta}_i \\pm t \\left(\\frac{1-\\alpha}{2}, \\nu \\right) SE(\\hat{\\beta}_i),$$\nwhere the $t()$ notation represents the *critical value* of the $t$-distribution, $t_{crit}$, with $\\nu$ degrees of freedom, for which $P(z \\le t_{crit}) = \\frac{1-\\alpha}{2}$, and $z$ is a continuous, random variable. This critical value can be calculated in R using the `qt()` function, which we show below. \n\n::: {.callout-note}\n## Covariance of $\\hat{\\beta}_i$\n\nAlthough it is convenient and easier to digest the confidence interval of individual $\\hat{\\beta}_i$, we must realize that the estimates of the $\\beta_i$ can covary (i.e., have non-zero covariance), which is quantified in the variance-covariance matrix of $\\hat{B}$, $(X^TX)^{-1} \\hat{\\sigma}^2$. We will show why this is important below.\n:::\n\nLet's manually calculate the 95% confidence intervals in $\\hat{B}$ and compare to R's internal function `confint()`.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract the degrees of freedom from the model (\\nu)\n# which can also be calculated as n - p\nt_df = lm_out$df.residual\n\n# Calculate t critical for alpha = 0.05\n# This will give us the 95% conf interval (CI)\nt_crit = qt(1-(0.05/2), df = t_df)\n\n# Calculate the upper and lower CI for both betas\nci_int = lm_coef[1] + c(-1,1)*t_crit*se_beta[1]\nci_slope = lm_coef[2] + c(-1,1)*t_crit*se_beta[2]\n\n# Construct a table of values\nci_mat = \n    rbind(c(lm_coef[1], ci_int),\n          c(lm_coef[2], ci_slope))\ncolnames(ci_mat) = c(\"coef\", \"lowCI\", \"highCI\")\nrownames(ci_mat) = c(\"intercept\", \"slope\")\nci_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare these manual calculations to built-in\n# function confint(), which by default extracts the \n# 95% CI for a lm() model's coefficients\nconfint(lm_out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                2.5 %    97.5 %\n(Intercept) -3.048956 14.591813\nx1          -1.196212  4.453355\n```\n\n\n:::\n:::\n\n## Propagate uncertainty in $\\hat{B}$ for predictions of $Y$\n\nThere are several ways to calculate and visualize our uncertainty in model predictions of observed data $Y$ and unobserved data of the dependent variable (i.e., interpolation). The colored lines drawn on the figure in @sec-est-plot represent the expected values of $Y$ based on the OLS analysis' estimate of $\\hat{B}$, but this line does not include uncertainty in these coefficient values. \n\n### Multivariate $t$-distribution method\nFirst, we will calculate uncertainty by sampling from the multivariate $t$ distribution that represents error in regression coefficients, $\\hat{B}$. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We will \"bootstrap\" 1000 samples of intercept and slope\nset.seed(3)\nn_samp = 500\n\n# Draw from the multivariate t \n# which represents (\\hat{B} - B)\ntest_mat_deviates = \n  mnormt::rmt(n_samp, mean = c(0,0), S = beta_cov_mat, df = t_df)\n\n# Now calculate the realized intercept and slope\n# using the t-distributed deviates\ntest_mat_t = cbind(\n  lm_coef[1] + c(test_mat_deviates[,1]),\n  lm_coef[2] + c(test_mat_deviates[,2])\n)\n\n# Calculate the 95% quantiles and compare to the \n# calculated 95% confidence intervals from above\napply(test_mat_t, \n      MARGIN = 2, # applies function (FUN) to columns (dim 2)\n      FUN = quantile, probs = c(0.025, 0.5, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]       [,2]\n2.5%  -3.820226 -0.5319227\n50%    5.809501  1.5967318\n97.5% 13.564890  4.1240734\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare\nci_mat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the relationship between intercept and slope\n# Notice the covariance\nplot(test_mat_t, xlab = \"Intercept\", ylab = \"Slope\")\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nNext, for each pair of intercept and slope randomly drawn above, we will calculate the expected values of $Y$ across the range of covariate $x$. We will then summarize the 95% quantile of expected $Y$ at each value of $x$ in this interpolation. To do this, we need a function to calculate the expected value of $Y$. This function will have the intercept and slope as inputs and will output the expected values of $Y$ across a range of $x$. Then, we will `apply()` this function using all of the values of intercept and slope, in a vectorized and therefore very efficient manner, rather than using any `for` loops. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a matrix that holds the values of x\n# over which we want to interpolate the expected\n# values of Y\nx_fake_mat = \n  cbind(\n    rep(1, times = 100),\n    seq(0,max(x1),length.out = 100)\n  )\n\n# Create a function that will calculate the expected values\ny_hat_fun = function(x, x_mat){\n  x_mat %*% x\n}\n\n# Apply this function to all intercepts and slopes that\n# we drew from the multivariate t\ny_pred_mt = apply(test_mat_t, 1, y_hat_fun, x_mat=x_fake_mat)\n\n# Summarize the 95% quantile of the expected value of Y\n# at each value of x \ny_pred_mt_summary = apply(y_pred_mt, 1, quantile, probs = c(0.025, 0.975))\nstr(y_pred_mt_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:2, 1:100] -3.82 13.56 -3.62 13.58 -3.41 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"2.5%\" \"97.5%\"\n  ..$ : NULL\n```\n\n\n:::\n:::\n\n\n### `predict()` function method\n\nR has a built-in function `predict()` (see specific variant `predict.lm()`) which calculates expected values of the dependent variable from a linear regression model estimated using the function `lm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note that 'newdata' must be a data frame that includes the ranges\n# of each covariate in the regression model for which you want \n# to generate interpolated or predicted values of the dependent variable\n\n# Here we are calculated the expected values as well as the \n# 95% confidence intervals for those expected values\ny_predict = predict(lm_out,\n                 newdata = data.frame(x1 = c(x_fake_mat[,2])),\n                 interval = \"confidence\", level = 0.95)\nstr(y_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:100, 1:3] 5.77 5.85 5.94 6.02 6.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:3] \"fit\" \"lwr\" \"upr\"\n```\n\n\n:::\n:::\n\n\n### Compare the two methods \n\nLet's visualize the output of the two methods to compare. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot\nplot(x=NA,y=NA,xlab = \"x\", ylab = \"y\",\n     xlim = c(0,max(x1)), ylim = c(-5, 25), pch = 19)\n# Plot the expected values of Y for each pair of int/slope \nfor(i in 1:n_samp){\n  lines(y_pred_mt[,i] ~ x_fake_mat[,2],\n        # Reduce the opacity of each line\n        col = scales::alpha(\"black\", alpha = 0.1), lwd = 2)\n}\n# Add the data points\npoints(y ~ x1, col = 'orange', pch = 19, cex = 2)\n# Add the expected values of Y from \\hat{B}\nabline(coef = lm_coef, col = \"orange\", lwd = 3)\n# Add the conf int of expected Y using multivariate t\nlines(y_pred_mt_summary[1,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\nlines(y_pred_mt_summary[2,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\n# Add the conf int of expected Y using predict() function\nlines(y_predict[,\"lwr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\nlines(y_predict[,\"upr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\nThere is yet a third option to calculate the uncertainty in predicted (i.e., interpolated or extrapolated) values of \n$Y$, which is to derive an exact calculation of the confidence interval using the $t$ distribution, similar to that shown in @sec-conf-beta. See Ch4.1 of Dr. Barber's book for this derivation. \n\n## Multiple Linear Regression\n\nSo far, we have only discussed a single input variable in our model, which is a simple linear regression. When we have multiple input variables, we are dealing with multiple linear regression analysis, so the model looks like:\n$$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_{p-1} x_{p-1,i} + \\epsilon_i$$\nwhere $p$ is the number of model coefficients and $p-1$ is the number of input variables. Still, in matrix notation the model is $Y = XB + \\epsilon$, so the least squares regression analysis approach still works. However, our interpretation of the model coefficients becomes a bit more challenging. \n\nLet's look at a data set within the `faraway` package.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(faraway)\ndata(gala)\n\n# Plot the raw data relationships\npar(mfrow=c(1,3))\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Area, xlab = \"Area\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Adjacent, xlab = \"Adjacent\", ylab = \"Species\", pch = 19)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n\n# Conduct multiple and single linear regressionm, focusing on Elevation\nm1 = lm(Species ~ Elevation + Area + Adjacent, data = gala)\nm2 = lm(Species ~ Elevation, data = gala)\ncoef(m1); coef(m2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   Elevation        Area    Adjacent \n-5.71892681  0.31498110 -0.02031217 -0.07527974 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)   Elevation \n 11.3351132   0.2007922 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nabline(coef=coef(m1)[1:2])\nabline(coef=coef(m2)[1:2], lty = 2)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWhat we see above is how the addition of `Area` and `Adjacent` input variables into the model \"adjusts\" the effect of `Elevation`, leading to two unique estimates of the slope (i.e., effect) of `Elevation`. Let's probe multiple linear regression more closely by using simulated data.\n\nFirst, let's simulate a model with 80 data points that correspond to observations of 4 input variables and one outcome variable. Note that in [Footnotes @sec-mlr], we show a case with a categorical/binary input variable. \n\n::: {.cell}\n\n```{.r .cell-code}\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    hist(xmat[,i], main = paste(\"covariate \", i-1))\n}\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\npar(mfrow=c(1,1))\nhist(y)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-16-2.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-16-3.png){width=672}\n:::\n:::\n\n\nHow do we figure out the expected value of $y$ for a particular situation? Here's an example. What is the expected value of $y$ when $x_2 = 0.5$, but the rest of the input variables are at their average values?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Written out long-ways:\npred_y = \n    betas[1]*1 + \n    betas[2]*mean(xmat[,2]) + \n    betas[3]*0.5 + \n    betas[4]*mean(xmat[,4]) + \n    betas[5]*mean(xmat[,5]) \npred_y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22.6006\n```\n\n\n:::\n:::\n\n\nNow let's use ordinary least squares regression to estimate our model coefficients from the data, and then compare these to our \"known\" values of the model parameters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the model:\nm1 = lm(y ~ 0 + xmat)\n# Note that the following two models give the same results\n#m2 = lm(y ~ 0 + X1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\n#m3 = lm(y ~ 1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + xmat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n       Estimate Std. Error t value Pr(>|t|)    \nxmat1  1.589838   1.759860   0.903    0.369    \nxmat2  0.737086   0.035629  20.688   <2e-16 ***\nxmat3 -1.295274   0.044252 -29.270   <2e-16 ***\nxmat4 -0.003676   0.028481  -0.129    0.898    \nxmat5  1.826125   0.088971  20.525   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9763,\tAdjusted R-squared:  0.9747 \nF-statistic: 616.7 on 5 and 75 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n#summary(m2)\n#summary(m3)\n\n# Compare known `betas` to estimated coefficients\ncbind(betas, coef(m1)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      betas             \nxmat1  1.00  1.589837527\nxmat2  0.75  0.737085861\nxmat3 -1.20 -1.295274007\nxmat4  0.00 -0.003676167\nxmat5  1.80  1.826125438\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot the regression lines with abline\ncoef_m1 = coef(m1)\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    \n    plot(y ~ xmat[,i], pch=19,\n         xlab = paste(\"covariate \", i-1),\n         ylab = \"y\",\n         ylim = range(y))\n    abline(coef=coef_m1[c(1,i)])\n}\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWell, those regression lines do not look correct. That is because we are interpretting the slopes and intercepts a little incorrectly and not plotting them in the correct manner. \n\n::: {.callout-note}\n## How to plot the output of `lm()` for multiple linear regression\n\nWhen we isolate and visualize the relationship between the outcome and a single input variable, what we are really observing is the adjusted relationship, after accounting for the other input variables in the model. To understand the expected value of $y$ for any particular value of the single input variable, we really need to set the other input variables to their mean value. Let's demonstrate this below with the `predict()` function.\n:::\n\nLet's determine the expected values of $y$ for input variable 2 ($x_2$) and plot it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prediction for covariate 2 when all other input vars at mean\nmy_df = data.frame(xmat[,2:5])\nhead(my_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          X1         X2       X3 X4\n1 -1.7268438 13.8180926 61.27634  8\n2 16.0748747  6.7393185 58.15099 14\n3 -5.0439349  0.8145552 36.82198 16\n4  5.5611421 18.1722388 38.24042 10\n5 18.6915270 16.7070212 51.91376 15\n6  0.1767361 12.9778881 45.11988 16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Re-run the model but with just the input variables, \n# and the intercept is implicit\nm2 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\n# Now let's try to predict y across a range of \n# input variable 2,\n# while holding the other input variables at\n# their average values\n\nn_pred = 100\nnew_df = data.frame(\n  X1 = rep(mean(my_df$X1), n_pred),\n  X2 = seq(0, 20, length.out = n_pred),\n  X3 = rep(mean(my_df$X3), n_pred),\n  X4 = rep(mean(my_df$X4), n_pred)\n)\n\ny_pred2 = predict(m2, newdata = new_df)\n\n# Now plot:\npar(mfrow=c(1,1))\nplot(y ~ my_df$X2, pch = 19,\n     xlab = \"covariate 2\", ylab = \"y\")\nlines(y_pred2 ~ new_df$X2)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nNow we see that the `predict()` function shows a more intuitive relationship between input variable $x_2$ and outcome $y$, *while accounting for the effects of the three other input variables*.\n\n\n## Footnotes\n\n### Euclidean norm & cross product {#sec-crossprod}\n\nWe often see the syntax, $|| a ||$, which is the Euclidean norm of the $n$-sized vector $a$: $$|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,$$ so that when we see $|| a ||^2$, this results in the sum of squares of vector $a$, $\\sum_{i=1}^{n} a_i^2$.\n\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, $\\epsilon_i$, are in vector, $\\epsilon$. The sum of squares of vector $\\epsilon$ is therefore $|| \\epsilon ||^2$. Algebraically, we can find this value as the cross product of $\\epsilon$, which is $\\epsilon^{T}\\epsilon$. Let's do a coded example with vector $x$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Evaluated as cross-product\nt(x) %*% x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   30\n```\n\n\n:::\n\n```{.r .cell-code}\n## Or with crossprod()\ncrossprod(x,x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   30\n```\n\n\n:::\n\n```{.r .cell-code}\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 30\n```\n\n\n:::\n:::\n\n\n### `solve()` and Inverse of matrix {#sec-solve}\n\nSuppose we have matrices $A$, $X$, and $B$, and the following expression is true: $$AX=B.$$\n\nThen, suppose $X$ is unknown, such that we want to find the solution for $X$, when we rearrange: $$X = A^{-1} B,$$ where $A^{-1}$ is the multiplicative inverse of matrix $A$. To figure this out computationally, we can use the `solve()` function in R, as long as $A$ is a square matrix and has an inverse.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    2\n[2,]    3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    2\n[2,]    3\n```\n\n\n:::\n:::\n\n\nWe can see, then, that `solve()` is internally evaluating $A^{-1}$. Remember that $A^{-1}$ is not trivial to calculate, as it is the matrix that must satisfy: $AA^{-1} = I$, where $I$ is an identity matrix. In fact, `solve(A)` returns the inverse of $A$, if it exists.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    2\n[2,]    3\n```\n\n\n:::\n\n```{.r .cell-code}\nX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]    2\n[2,]    3\n```\n\n\n:::\n:::\n\n\n### Multiple linear regression with a categorical input {#sec-mlr}\n\nLet's simulate a case in which we have one categorical input variable that takes on values \"low\", \"medium\", and \"high\", and one continuous input variable. \n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\nn=90\nsigma = 0.8\n\n# Xmatrix\n## Intercept\nx0 = rep(1, times = n) \n## Categorical input variable\n### Note that we need to code this as \"0\" \"1\" \"2\" to \n### simulate our outcome variable \"y\"\nx1 = rep(c(0,1,2), each=n/3)\nx1L = factor(x1, labels = c(\"low\", \"med\", \"high\"))\n## Continuous input variable\nx2 = rnorm(n, 0, 2.5)\nxmat = cbind(x0,x1,x2)\nhead(xmat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     x0 x1        x2\n[1,]  1  0  5.718118\n[2,]  1  0 -2.991929\n[3,]  1  0 -1.735731\n[4,]  1  0 -1.030732\n[5,]  1  0 -2.426683\n[6,]  1  0 -2.368200\n```\n\n\n:::\n\n```{.r .cell-code}\n# Intercept and 2 slopes\nbetas=c(1.5, 1.2, -1.5)\n\n# Simulate outcome variable, as usual\ny2 = xmat %*% betas + rnorm(n,0,sigma)\n\n# Plot the relationships\npar(mfrow=c(1,2))\nplot(y2~x1)\nplot(y2~x2)\n```\n\n::: {.cell-output-display}\n![](ols_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Run the model\n## Note that we us the \"factor\" input variable\n## \"x1L\", which has \"levels\"\nm_cat = lm(y2 ~ 1 + x1L + x2)\nsummary(m_cat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y2 ~ 1 + x1L + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9182 -0.5032  0.1465  0.5061  1.2139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.66504    0.13741  12.117  < 2e-16 ***\nx1Lmed       1.15971    0.19113   6.068 3.38e-08 ***\nx1Lhigh      2.29169    0.19170  11.954  < 2e-16 ***\nx2          -1.51229    0.03275 -46.171  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7314 on 86 degrees of freedom\nMultiple R-squared:  0.9674,\tAdjusted R-squared:  0.9663 \nF-statistic: 851.1 on 3 and 86 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\ncoef(m_cat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)      x1Lmed     x1Lhigh          x2 \n   1.665040    1.159710    2.291690   -1.512293 \n```\n\n\n:::\n:::\n\n\nHow do we interpret the slopes, because we see there is a separate slope for `x1Lmed` and `x1Lhigh`? We can understand better by seeing how the linear model addes up. For instance, what is the expected value of the outcome variable when $x_1$ is `high`, and $x_2 = 2.0$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Using m1_binL:\ny2_pred = \n    1*coef(m_cat)[1] + # Global average (intercept)\n    0*coef(m_cat)[2] + # Not \"med\"\n    1*coef(m_cat)[3] + # Yes \"high\"\n    2.0*coef(m_cat)[4] # x2=2.0 * slope\nas.numeric(y2_pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9321446\n```\n\n\n:::\n:::\n\n\nWhen we assigned the slope of the categorical input variable as $1.2$, remember this is the expected change in $y$ as the input variable changes by a value of $1.0$. In the model, we code the $x_1$ variable as taking numerical values $0$, $1$, and $2$ to represent categories, \"low\", \"medium\", and \"high\". So, the slope for `x1med` is the expected change in $y$ as the input variable changes from \"low\" to \"medium\", an effective change of $1.0$. Then, the slope for `x1high` is the expected change in $y$ as the input variable changes from \"low\" to \"high\", an effective change of $2.0$; hence, this slope is estimated as $2.29$, with standard error $0.19$. Notice how this slope is approximately twice our \"known\" slope for the input variable, which was $1.2$.",
    "supporting": [
      "ols_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}