{
  "hash": "4b7fca8c2f4d0b4b276c5495a34bfbc4",
  "result": {
    "engine": "knitr",
    "markdown": "# Probability Distributions {#sec-prob}\n\n## Gaussian (Normal) distribution\n\nAs we learned in lecture, the normal distribution is defined by two parameters, the mean $\\mu$ and the standard deviation $\\sigma$. Here, we will use the normal distribution to demonstrate some of R's functions to describe probability distributions and to draw random numbers from probability distributions. Let's assume that random variable $x$ follows a normal distribution, $x_i \\sim N(\\mu, \\sigma)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the parameters\nmu = 10\nsigma = 2.5\n\n# Visualize the probability density function (pdf)\nx_vals = seq(0, 50, by = 0.1)\nnorm_pdf = dnorm(x_vals, mean = mu, sd = sigma)\n\n# Let's use some of the other R functions to describe the distribution\n\n## What is the probability density of specific values?\n## mean\np_mu = dnorm(mu, mean = mu, sd = sigma)\n## The next two values will describe the 95% probability density bounds\n## (Low) 2.5% cut off \nx_low95 = qnorm(0.025, mean = mu, sd = sigma)\np_low95 = dnorm(x_low95, mean = mu, sd = sigma)\n## (High) 97.5% cut off\nx_high95 = qnorm(0.975, mean = mu, sd = sigma)\np_high95 = dnorm(x_high95, mean = mu, sd = sigma)\n\n# So, what is the P(x <= x_high95)??\npnorm(x_high95, mean = mu, sd = sigma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.975\n```\n\n\n:::\n:::\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n## Plot the pdf with segments\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"x\", ylab = expression(\"P(x |\"~mu~\",\"~sigma~\")\"))\nlines(norm_pdf ~ x_vals)\nsegments(x0 = c(x_low95, mu, x_high95), x1 = c(x_low95, mu, x_high95),\n         y0 = rep(0, times = 3), y1 = c(p_low95, p_mu, p_high95))\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Now, let's draw random samples from this normal distribution\nn_rand = 1000\nx_rand = rnorm(n_rand, mean = mu, sd = sigma)\n\n# Plot a histogram and overlay the approximate expectations\n## The line below assumes you draw 'n_rand' samples\nhist(x_rand, breaks = 20, main = \"\")\nlines(norm_pdf*n_rand ~ x_vals)\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n## Multivariate normal distribution\n\n### Relation to residuals, $\\epsilon$\n\nRecall our linear model in matrix notation: $Y = XB + \\epsilon$. We use the multivariate normal distribution to describe the probability density of the residuals, $\\epsilon$. Recall that each individual residual, $\\epsilon_i$ follows a normal distribution with mean zero and standard deviation equal to the residual error, $\\sigma$: $\\epsilon_i \\sim N(0, \\sigma)$. Also recall that the linear regression analysis assumes that $\\epsilon_i$ are I.I.D. (independent and identically distributed). $\\epsilon_i \\sim N(0, \\sigma)$ implies the identical distribution (i.e., each residual follows the same normal distribution). The \"independent\" part means that the residual values are not correlated in any way, meaning that they do not covariance is zero. Thus, we can use vector notation to say that the vector $\\epsilon$ follows a multivariate normal distribution with all means equal to zero and covariance matrix $\\Sigma = \\sigma^2 I$, where $I$ is a square identity matrix: $\\epsilon \\sim N(0, \\sigma^2 I)$. More about covariance and covariance matrices is available below ([Footnotes -@sec-covariance]).\n\nThe multivariate normal probability distribution is hard to visualize, because it is in multiple dimensions. But we can use similar R functions to understand the distribution. These functions are not in the base installation of R, so we need another package, `MASS`. We'll also need the `Matrix` package later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install packages if you don't already have them, e.g., \n# install.packages(\"MASS\", dependencies = TRUE)\nlibrary(MASS)\nlibrary(Matrix)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Matrix' was built under R version 4.3.1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Define mean and st.dev.\nmu_epsilon = 0\nsigma_epsilon = 2.0\n\n# sample size\nn_resid = 1000\n\n# we need a vector of means\nmu_vec = rep(0, n_resid)\n\n# we need an identity matrix\nI_mat = matrix(0, nrow = n_resid, ncol = n_resid)\n## specify the diagonal = 1\ndiag(I_mat) = 1\n\n# Draw randomly from the multivariate normal\nmvn_epsilon = MASS::mvrnorm(n = 1, \n                            mu = mu_vec,\n                            Sigma = sigma_epsilon^2*I_mat)\n# We can see that an entire array of size n_resid is drawn\nstr(mvn_epsilon)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n num [1:1000] 4.106 2.124 2.305 1.28 -0.331 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# How does this compare to drawing them independently?\nnorm_epsilon = rnorm(n_resid, mean = mu_epsilon, sd = sigma_epsilon)\nc(mean(mvn_epsilon), mean(norm_epsilon))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.06278693  0.07863296\n```\n\n\n:::\n\n```{.r .cell-code}\nc(sd(mvn_epsilon), sd(norm_epsilon))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.989344 1.997697\n```\n\n\n:::\n:::\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Compare these two vectors visually:\nhist(mvn_epsilon)\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(norm_epsilon)\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\n### Multivariate normal distribution with non-independent variables\n\nLet's explore the multivariate normal a bit more. Suppose we have three random variates $a$, $b$, and $c$. Suppose further that $a$ and $b$ are positively correlated with each other, but $c$ is not correlated with either other variate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Establish means and variances of a, b, and c\nmu_vec = c(1.0, 2.2, 1.5)\nsd_vec = c(1.5, 0.5, 0.75)\n\n# Manually construct the covariance matrix:\ncov_mat_test = matrix(\n    data = c(0.0, 0.6, 0.0,\n             0.6, 0.0, 0.0,\n             0.0, 0.0, 0.0),\n    ncol = 3, nrow = 3,\n    byrow = TRUE\n)\ndiag(cov_mat_test) = sd_vec^2\n\n# Matrix must be positive definite (PD). \n# This gives closest PD\ncov_mat = Matrix::nearPD(cov_mat_test)$mat\n# Look if you want: str(cov_mat)\n\n# Draw some random vectors:\nabc_array = mvrnorm(n = 100, mu = mu_vec, Sigma = cov_mat)\n# Look at structure if you want; str(abc_array)\n\n# Visualize the relationships between a, b, and c:\ncolnames(abc_array) = letters[1:3]\npairs(abc_array)\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-5-1.png){width=624}\n:::\n:::\n\n\n## Poisson distribution\n\nThe normal and multivariate normal probability distributions have PDFs related to *continuous* random variables. In many cases our data are not continuous, but are instead *discrete*. The Poisson distribution represents the PMF (probability *mass* function) of count data and is described by a single parameter, $\\lambda$, which is equal to the mean and variance of the distribution. In regression, we can use the Poisson distribution to analyze a generalized linear model between a discrete response variable (e.g., count data) and its covariates, but we will not deal with that in our class.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Define the parameter\nlambda = 8\n\n# Visualize the probability density function (pdf)\n## Remember this is a discrete distribution\nk_vals = c(0:20)\npois_pdf = dpois(k_vals, lambda = lambda)\n\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"k\", ylab = expression(\"P(k |\"~lambda~\")\"))\npoints(pois_pdf ~ k_vals)\nsegments(x0 = k_vals, x1 = k_vals,\n         y0 = 0, y1 = pois_pdf)\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Compare to randomly drawn values:\nk_rand = rpois(n_rand, lambda = lambda)\nhist(k_rand, breaks = 25, main = \"\")\npoints(pois_pdf*n_rand ~ k_vals, pch = 19)\n```\n\n::: {.cell-output-display}\n![](prob_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# On your own, use the ppois() and qpois() functions to understand their inputs/outputs\n```\n:::\n\n\n## Footnotes\n\n### Covariance matrix {#sec-covariance}\n\nAs reminder, the variance of a random variable, $x$, with sample size $n$ is: $$\\sigma^2_x = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(x_i - \\bar{x}) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})^2.$$ And $\\bar{x}$ is the sample mean. Similarly, then, the covariance of samples from two random variables, $x$ and $y$, can be calculated as: $$\\sigma(x,y) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y}).$$ The syntax for the covariance of a sample population with itself is, for example, $\\sigma(x, x)$, which is simply equal to the variance $\\sigma_x^2$. The covariance matrix for these two sample populations would be: $$C = \\begin{bmatrix}\n\\sigma(x,x) & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma(y,y)\n\\end{bmatrix}.$$ This can be simplified using the variance notation: $$C = \\begin{bmatrix}\n\\sigma^2_x & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma^2_y\n\\end{bmatrix}.$$\n",
    "supporting": [
      "prob_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}