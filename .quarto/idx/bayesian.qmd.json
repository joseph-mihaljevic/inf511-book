{"title":"Bayesian Inference","markdown":{"headingText":"Bayesian Inference","headingAttr":{"id":"sec-bayesian","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Background\n\nAs a reminder, suppose we are estimating a single parameter in a model, $\\theta$. In Bayesian inference, we have:\n\n$$P(\\theta | \\text{Data}) \\propto P(\\theta)P(\\text{Data}|\\theta)$$\nIn words, this means that the posterior probability distribution of the parameter, $P(\\theta | \\text{Data})$, is proportional to the prior probability distribution of the parameter, $P(\\theta)$, multiplied by the likelihood of the data, given the parameter, $P(\\text{Data}|\\theta)$. \n\nThe prior probability distribution of the parameter quantifies what we believe the parameter's true value may be, prior to collecting data. Remember that this prior probability distribution can be \"vague,\" meaning that we don't have high confidence in what the parameter value is prior to collecting data. Or, the prior can be \"informative,\" meaning that we have some level of certainty in what values are most likely for the parameter. \n\nThe likelihood is the same quantity that we discussed in the sections on Maximum Likelihood. The data likelihood represents how well a model matches the data, given a particular parameter value.\n\nFinally, the posterior probability distribution represents a type of weighted likelihood - the likelihood weighted by our prior knowledge of what the parameter value might be.\n\n## Estimating the mean of a sample\n\nHere is an example of how we can visualize the relationship between the prior, the likelihood, and the posterior of a parameter. Imagine that we collect a sample of data, $y$, from the population $Y$. Our goal is to estimate the mean of that population, $Y$. Obviously this can be done by calculating the mean outright, but here we want to quantify the posterior probability distribution of the mean, so that we can simultaneously understand the central estimate as well as the uncertainty around that estimate. To do this, we must specify the likelihood of the data. We'll assume that:\n$$y_i \\sim N(\\mu, \\sigma)$$\nWe'll assume we know $\\sigma$ with certainty. Our goal then is to estimate the posterior of $\\mu$, $P(\\mu | y)$. \n\nFirst we'll generate data points $y$ from a \"known\" distribution.\n```{r}\nset.seed(3)\nn_obs = 15\nmu_known = 8.2\nsigma_fixed = 2.5\ny = rnorm(n_obs, mean = mu_known, sd = sigma_fixed)\n\nhist(y)\n```\nObviously in this easy example we could calculate a point estimate of the mean of $Y$ using the mean of sample $y$:\n```{r}\nmean(y)\n```\nBut, this estimate leads to no understanding of the certainty in our estimate of the true mean of population $Y$. Instead, let's use Bayesian inference. For instance, we know that the true mean of $Y$ is $8.2$, which we simulated. So this estimate of $\\mu$ has error, especially because of low sample size. \n\nFirst, let's specify a vague prior probability distribution for $\\mu$. We'll assume:\n$$\\mu \\sim N(0, 50)$$\nWe can visualize this prior probability density function.\n```{r}\nmu_guess = seq(0, 20, length.out = 200)\n# Prior prob distribution\nmu_prior = dnorm(mu_guess, 0, 50, log = TRUE)\n\nplot(exp(mu_prior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\")\"))\n```\n\nNotice that because we made the prior so \"vague\", all of the possible values of $\\mu$ that we plotted (ranging from 0 to 20), all have very low probabilities, because basically all possible values of $\\mu$ (ranging negative to positive infinity) have equally low probability with this vauge prior. I'm exaggerating a little bit to make a point. \n\nNow, we need to create a function to calculate the likelihood of any particular \"guess\" of $\\mu$. \n\n```{r}\nmu_likelihood = function(this_mu, data){\n    \n    log_lhood = \n        sum(\n            dnorm(data, \n                  mean = this_mu,\n                  sd = sigma_fixed,\n                  log = TRUE)\n        )\n    return(log_lhood)\n}\n```\nWe've seen these sorts of functions before. Now, let's calculate the likelihood for each of our \"guesses\" of $\\mu$. \n\n```{r}\n# Store the likelihoods:\nmu_lhood = NULL\nfor(i in 1:length(mu_guess)){\n    mu_lhood[i] = mu_likelihood(mu_guess[i], y)\n}\n\n# Plot on ln scale:\nplot(exp(mu_lhood) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~y~\"|\"~mu~\")\"))\n```\nNow we can calculate the posterior as the product of the prior and the likelihood (or the sum of the log-scale values of these distributions). \n\n```{r}\nmu_posterior = mu_prior + mu_lhood\n\nplot(exp(mu_posterior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\"|\"~y~\")\"))\nabline(v = mu_known, lty = 1)\nabline(v = mean(y), lty = 2)\n```\nWhat we see here is that with the vague prior, the posterior basically reflects the data likelihood. The prior gave no additional information to the analysis. \n\nNow let's see how the posterior might change with a more informative prior that is actually biased to the incorrect value of $\\mu$, such as $\\mu \\sim N(2, 0.75)$.\n\n```{r}\n# Prior prob distribution\nmu_prior = dnorm(mu_guess, 2, 0.75, log = TRUE)\n\nplot(exp(mu_prior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\")\"))\n\nmu_posterior = mu_prior + mu_lhood\n\nplot(exp(mu_posterior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\"|\"~y~\")\"))\nabline(v = mu_known, lty = 1)\nabline(v = mean(y), lty = 2)\n```\nNow what we see more clearly is that the posterior is the data likelihood *weighted* by the prior. In this case, because we have very few data points, the posterior is particularly sensitive to the prior of $\\mu$. \n\n## Using Monte Carlo sampling\n\nIn reality, we are estimating more than one parameter in a model. Therefore, estimating the posterior of the model means estimating the joint posterior of the model parameters, so that we can quantify the marginal posterior estimate of each model parameter. \n\nTherefore we use an algorithm to \"sample from\" the joint posterior. As discussed in lecture, these algorithms typically employ a variant of Monte Carlo sampling (e.g., Markov chain Monte Carlo (MCMC) or Hamiltonian Monte Carlo (HMC)). The statistical programming language `Stan` uses HMC, and we will employ `Stan` via the R package `rstan`. See [Footnotes @sec-mcmc] for a coded Metropolis-Hastings MCMC, which we described in lecture. \n\nNote that because we are using Quarto documents, we need to set up the `Stan` model in a very particular way, to get it to work with code chunks. Usually, when using a `.R` file, we create a separate `.stan` file, and then we run the `rstan::stan()` function in the `.R` file, while referencing the `.stan` file that should be saved in our working directory. Here, we will create and compile the `.stan` file in one place (in the Quarto document). I will put an example set of `.R` and `.stan` files on BBLearn that compliment the following examples. \n\n### Estimating the mean of a sample using Stan\n\nFirst, we will load `rstan` and set some required options.\n```{r}\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n```\n\nNext, we will create a `.stan` model that allows us to estimate the mean only.\n\n```{stan output.var = \"estimate_mu\"}\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  real<lower=0> sigma_fixed;\n}\n\n// The parameters accepted by the model. \nparameters {\n  real mu;\n}\n\n// The model to be estimated.\nmodel {\n  // priors\n  mu ~ normal(0, 50.0);\n  \n  // likelihood\n  y ~ normal(mu, sigma_fixed);\n}\n\n```\n\nWhen the chunk above is run, it compiles the `Stan` model into `C++` code that gets run in the background. Next, we will use `R` code to set up and run the `Stan` model to estimate the parameter $\\mu$.\n\n```{r}\n\n# create a list for stan\nmu_fit_data = list(\n    N = length(y),\n    y = y,\n    sigma_fixed = sigma_fixed\n)\n\n# Fit the compiled model using stan defaults\nfit = sampling(estimate_mu, # this is generated from the previous code-chunk\n               data = mu_fit_data)\n\n# Summarize the output\nprint(fit)\n```\n\nThis `print()` format shows us the mean and median (`50%`) estimates of $\\mu$. The output also shows various levels of the \"credible intervals\". For instance, the `2.5%` and `97.5%` would give us the ends of the \"95% credible interval\", whereas the `25%` and `75%` would give us the ends of the \"50% credible interval\". \n\nWe can also generate various summary visualizations. \n```{r}\nplot(fit)\nplot(fit, show_density = TRUE)\nplot(fit, plotfun = \"hist\")\n```\n\nThese three plots summarize the posterior estimate of $\\mu$ in various ways. \n\nNext, we can observe the \"traceplot\" which shows the outcome of the 4 HMC chains that sample from the posterior. \n```{r}\nplot(fit, plotfun = \"trace\")\n```\nWe can also show the \"warmup\" phase, which emphasizes how the initial \"proposal\" can be far outside of the true posterior. \n\n```{r}\n# Include the warmup phase\nplot(fit, plotfun = \"trace\", inc_warmup = TRUE)\n```\n\n### Estimating the mean and the standard deviation using Stan\n\nNow let's assume the more likely case in which we do not know the mean nor the standard deviation of the sample, so we need to estimate $\\mu$ and $\\sigma$ of the normal distribution. \n\n\n```{stan output.var = \"estimate_mu_sigma\"}\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\n\n// The parameters accepted by the model. \nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n\n// The model to be estimated.\nmodel {\n  // priors\n  mu ~ normal(0, 50.0);\n  sigma ~ cauchy(0, 1);\n  \n  // likelihood\n  y ~ normal(mu, sigma);\n}\n\n```\n\nNotice how now we have two parameters in the `parameters` code block. We also are specifying two prior distributions, one for $\\mu$ and one for $\\sigma$. We are using the `cauchy` probability distribution for $\\sigma$, which is useful in part because this distribution ensures that $\\sigma > 0$. \n\nNext, we will use `R` code to set up and run the `Stan` model.\n```{r}\n# create a list for stan\nmu_sigma_fit_data = list(\n    N = length(y),\n    y = y\n)\n\n# Fit the \"model\" using stan defaults\nfit2 = sampling(estimate_mu_sigma, \n                data = mu_sigma_fit_data)\n\n# Summarize the output\nprint(fit2)\n```\n\nNow the `print()` statement shows the output for both $\\mu$ and $\\sigma$. And we can see that the true values of both parameters lie within the 95% credible interval of the posterior. \n\nWe can also generate various summary visualizations. \n```{r}\nplot(fit2, plotfun = \"hist\")\nplot(fit2, show_density = TRUE)\nplot(fit2, plotfun = \"trace\")\n```\n\n## Footnotes\n\n### An MH-MCMC algorithm {#sec-mcmc}\n\nBelow is a coded MH-MCMC algorithm for estimating one parameter. In this case the mean of a normal distribution:\n$$y \\sim N(\\theta, \\sigma^2)$$\nHere $\\theta$ is the mean of the normal distribution, which is unknown, but $\\sigma$ is known. \n\n```{r}\nset.seed(10)\n\n# We will estimate true_theta\n# Assume:\ntrue_theta = 3.0\nsd_known = 2.0\n#-----------------------------------\n#-----------------------------------\n# Test data set\ny_test = rnorm(150, true_theta, sd_known)\n#-----------------------------------\n#-----------------------------------\n\n# SET UP THE MCMC \n\n# Parameters of the normal prior on true_theta\n# i.e. true_theta ~ Normal(mu_prior, sd_prior)\nmu_prior = 3.0\nsd_prior = 5.0\n\n# Same with the proposal distribution\n## Notice that the width of the proposal \n## is 25% greater than the prior\nmu_prop = 3.0\nsd_prop = sd_prior * 1.25\n\n# Number of MCMC iterations\nn_iter = 15000\n\n# Set up storage (vector)\nchosen_theta = vector(\"numeric\", length = n_iter)\n\nfor(i in 1:n_iter){\n  \n  if(i == 1){ # First iteration\n    # Choose theta from proposal distribution\n    old_theta = rnorm(1, mu_prop, sd_prop)\n  }\n  \n  # Draw new theta from proposal:\n  new_theta = rnorm(1, mu_prop, sd_prop)\n  \n  # Calculate proposal adjustment:\n  old_prop_adj = dnorm(old_theta, mu_prop, sd_prop, log = TRUE)\n  new_prop_adj = dnorm(new_theta, mu_prop, sd_prop, log = TRUE)\n  \n  # Calculate prior prob:\n  old_prior = dnorm(old_theta, mu_prior, sd_prior, log = TRUE)\n  new_prior = dnorm(new_theta, mu_prior, sd_prior, log = TRUE)\n  \n  # Calculate data likelihood:\n  old_lik = sum(dnorm(y_test, old_theta, sd_known, log = TRUE))\n  new_lik = sum(dnorm(y_test, new_theta, sd_known, log = TRUE))\n  \n  # Calculate posterior density:\n  old_post = old_prior + old_lik\n  new_post = new_prior + new_lik\n  \n  # Calculate acceptance ratio:\n  log_ratio = (new_post - new_prop_adj) - (old_post - old_prop_adj)\n  ratio = exp(log_ratio)\n  \n  # Make decision:\n  if(ratio > 1){\n    # Keep new theta\n    chosen_theta[i] = new_theta\n  }else{\n    \n    rand = runif(1, min = 0, max = 1)\n    \n    if(ratio <= rand){\n      # Reject new theta (i.e., keep old_theta)\n      chosen_theta[i] = old_theta\n    }else{\n      chosen_theta[i] = new_theta\n    }\n    \n  }\n  \n  # Update what is \"old\" value\n  old_theta = chosen_theta[i]\n  \n}\n\n#-----------------------------------\n#-----------------------------------\n\n# Plot the trace:\nplot(chosen_theta ~ c(1:n_iter), type = \"l\",\n     xlab = \"Iteration\", ylab = expression(theta~\"|\"~y))\n\n# Plot the histogram:\n## Cut out the warmup or \"burn-in\" phase\nn_burn = n_iter / 2\nhist(chosen_theta[n_burn:n_iter], breaks = 25)\nabline(v = true_theta, lwd = 3)\n\n# Quantiles:\nquantile(chosen_theta[n_burn:n_iter],\n         probs = c(0.025, 0.5, 0.975))\nsd(chosen_theta[n_burn:n_iter])\n\n```\nWe can see that the true value of $\\theta$ is within the 95% credible interval of the marginal posterior distribution. \n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":{"toggle":true},"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"printing","output-file":"bayesian.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.537","crossref":{"appendix-delim":":"},"bibliography":["references.bib"],"theme":"cosmo","smooth-scroll":true,"code-block-bg":true},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}