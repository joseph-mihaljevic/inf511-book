{"title":"Ordinary Least Squares","markdown":{"headingText":"Ordinary Least Squares","headingAttr":{"id":"sec-ols","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n## Lecture material\n\nPlease download and print the lecture materials from [Bblearn](https://bblearn.nau.edu/){target=\"_blank\"}. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section.\n\n## In-class Code {.unnumbered}\n\nRemember that our goal is to estimate the linear relationship between data observations of response variable, $y$, and its measured covariate, $x$, following: $Y = XB + \\epsilon$, where $\\epsilon \\sim N(0, \\sigma^2 I).$ Our coefficients to estimate are therefore $\\hat{B}$, which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), $\\hat{\\sigma}$. To estimate the coefficients, we are attempting to minimize the residual sum of squares, $|| \\epsilon || ^ 2$. See [Footnotes @sec-crossprod] for more information regarding this notation.\n\n## Generate the data\n\nWe'll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we'll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\n```{r}\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon\n```\n\n## Plot the relationship\n\n```{r}\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)\n```\n\n## Estimate the coefficients using R's `lm()` function {#sec-lm-output}\n\n```{r}\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n```\n\n## Estimate the coefficients manually\n\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, $Y$. Remember that we estimate the coefficient vector, $\\hat{B}$ from: $$X^TX \\hat{B} = X^T Y$$ $$\\hat{B} = (X^TX)^{-1} X^T Y$$ These equations include the multiplicative inverse matrix, $(X^TX)^{-1}$. See the [Footnotes @sec-solve] for more information about inverse matrices and the `solve()` function.\n\n```{r}\n\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?\n```\n\n## Plot the *estimated* relationships {#sec-est-plot}\n\n```{r}\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)\n```\n\n## Why are the $\\hat{B}$ different from true $B$?\n\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), $|| \\epsilon ||^2$.\n\n```{r}\n# True sum of squares:\nsum(epsilon)^2\n\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals)^2\n\n## From manual OLS\nsum( (y - xmat %*% bhat_solve) )^2\n\n```\nYou can see that the OLS strategy effectively minimized the SSE to zero.\n\n\n## Understanding Uncertainty in $\\hat{B}$\n\nWhile the OLS analysis estimates the regression coefficients, $\\hat{B}$, from the observed data $Y$, our estimates of these coefficients have error (i.e., uncertainty), such that the estimates are only as good as the data. Specifically, if we have fewer data points (i.e., $n$ is low), we have less certainty in $\\hat{B}$. In lecture, we showed, that:\n$$\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right), $$\nand we know that $\\hat{\\sigma}^2$ depends on sample size $n$, following:\n$$\\hat{\\sigma}^2 \\quad = \\quad \\frac{1}{n-p} (Y_{obs} - Y_{pred})^T (Y_{obs} - Y_{pred}) \\quad = \\quad \\frac{1}{n-p} \\hat{\\epsilon}^T \\hat{\\epsilon}$$\n\nUsing these equations, we showed then that $SE(\\beta_i) = \\sqrt{diag\\left( (X^TX)^{-1} \\right)_i \\hat{\\sigma}^2}$. Let's calculate this manually and compare to the output of the `lm()` function.\n\n```{r}\n# Extract the model summary, which has useful components\nlm_out_summary = summary(lm_out)\n# Extract the estimated residual standard deviation, sigma\nest_sigma = lm_out_summary$sigma\nest_sigma\n\n# We already calculated (X^T X)^{-1} as inv_xtx\nbeta_cov_mat = inv_xtx * est_sigma^2\nbeta_cov_mat\nse_beta = sqrt(diag(beta_cov_mat))\nse_beta\n```\nCompare these values to the output of the `summary()` of @sec-lm-output in the column labelled `Std. Error`.  \n\n## Confidence Intervals for $\\hat{B}$ {#sec-conf-beta}\n\nTo calculate confidence intervals for $\\hat{B}$, we first must understand the $t$ (a.k.a. Student's $t$) probability distribution. This distribution represents the case when we are estimating the mean of a normally distributed variable and either the sample size is small or the variable's standard deviation is unknown. Essentially, the $t$ distribution increases the uncertainty (i.e., variance) in cases of low sample size (i.e., small $n$). With low sample size (and/or high number of parameters), the degrees of freedom of the $t$-distribution, $\\nu$ is low, whereas with high sample size, $\\nu$ is large. As $\\nu$ approaches infinity, the $t$-distribution approximates the standard normal distribution (i.e., $N(\\mu, \\sigma)|\\mu=0,\\sigma=1$). \n```{r}\n#| echo: false\n\n# Generate x sequence\nn_seq = 1000\nx_seq = seq(-10, 10, length.out = n_seq)\n\nt_pdf_nu2 = dt(x_seq, df = 2)\nt_pdf_nu100 = dt(x_seq, df = 100)\nnorm_pdf = dnorm(x_seq, mean = 0, sd = 1)\n\n# compare to t-100\nplot(NA,NA, xlab = \"x\", ylab = \"P(x)\", \n     xlim = range(x_seq), ylim = c(0, 0.5))\nlines(norm_pdf~x_seq, col = \"red\", lwd = 3)\nlines(t_pdf_nu2~x_seq, col = \"blue\", lwd = 3)\nlines(t_pdf_nu100~x_seq, col = \"blue\", lwd = 2.5, lty = 2)\nlegend(x = 2.5, y = 0.4,\n       legend = c(\"Standard Normal\", expression(italic(t)[2]), expression(italic(t)[100])),\n       lty = c(1,1,2), lwd= c(2,2,2), col = c(\"red\", \"blue\", \"blue\"), bty = \"n\")\n```\nIt is the case for $\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right)$ that we do not know the mean ($B$), and we are estimating the variance, $\\hat{\\sigma}^2$. Specifically, we are estimating the true mean vector, $B$, as $\\hat{B}$, and we are estimating the variance of the residuals as $\\hat{\\sigma}^2$. We can therefore re-write the uncertainty in $\\hat{B}$ as a multivariate $t$ distribution: \n$$(\\hat{B} - B) \\sim t_{\\nu} \\left( 0, \\Sigma \\right),$$\nwhere the means are zero, $\\nu$ is the degrees of freedom (i.e., $n-p$), and $\\Sigma = (X^TX)^{-1} \\hat{\\sigma}^2$. $(\\hat{B} - B)$ represents the deviation of the estimated coefficients from the true coefficients, which is why the distribution is centered around zero. It is perhaps easier to separate the individual estimated coefficients, $\\beta_i$, into their separate $t$-distributions:\n$$\\frac{(\\hat{\\beta}_i - \\beta_i)}{SE(\\hat{\\beta}_i)} \\sim t_{\\nu}$$\n$$(\\hat{\\beta}_i - \\beta_i) \\sim t_{\\nu} SE(\\hat{\\beta}_i),$$\nwhich shows that the $t$-distribution that describes the deviation of regression coefficients from the true value of those coefficients is scaled by the uncertainty in the estimated coefficients $SE(\\hat{\\beta}_i)$. As shown in Dr. Barber's materials, using this information, we can derive the confidence interval (at the $\\alpha$ confidence level) calculation for $\\hat{\\beta}_i$ as: \n$$ \\hat{\\beta}_i \\pm t \\left(\\frac{1-\\alpha}{2}, \\nu \\right) SE(\\hat{\\beta}_i),$$\nwhere the $t()$ notation represents the *critical value* of the $t$-distribution, $t_{crit}$, with $\\nu$ degrees of freedom, for which $P(z \\le t_{crit}) = \\frac{1-\\alpha}{2}$, and $z$ is a continuous, random variable. This critical value can be calculated in R using the `qt()` function, which we show below. \n\n::: {.callout-note}\n## Covariance of $\\hat{\\beta}_i$\n\nAlthough it is convenient and easier to digest the confidence interval of individual $\\hat{\\beta}_i$, we must realize that the estimates of the $\\beta_i$ can covary (i.e., have non-zero covariance), which is quantified in the variance-covariance matrix of $\\hat{B}$, $(X^TX)^{-1} \\hat{\\sigma}^2$. We will show why this is important below.\n:::\n\nLet's manually calculate the 95% confidence intervals in $\\hat{B}$ and compare to R's internal function `confint()`.\n```{r}\n# Extract the degrees of freedom from the model (\\nu)\n# which can also be calculated as n - p\nt_df = lm_out$df.residual\n\n# Calculate t critical for alpha = 0.05\n# This will give us the 95% conf interval (CI)\nt_crit = qt(1-(0.05/2), df = t_df)\n\n# Calculate the upper and lower CI for both betas\nci_int = lm_coef[1] + c(-1,1)*t_crit*se_beta[1]\nci_slope = lm_coef[2] + c(-1,1)*t_crit*se_beta[2]\n\n# Construct a table of values\nci_mat = \n    rbind(c(lm_coef[1], ci_int),\n          c(lm_coef[2], ci_slope))\ncolnames(ci_mat) = c(\"coef\", \"lowCI\", \"highCI\")\nrownames(ci_mat) = c(\"intercept\", \"slope\")\nci_mat\n\n# Compare these manual calculations to built-in\n# function confint(), which by default extracts the \n# 95% CI for a lm() model's coefficients\nconfint(lm_out)\n```\n## Propagate uncertainty in $\\hat{B}$ for predictions of $Y$\n\nThere are several ways to calculate and visualize our uncertainty in model predictions of observed data $Y$ and unobserved data of the dependent variable (i.e., interpolation). The colored lines drawn on the figure in @sec-est-plot represent the expected values of $Y$ based on the OLS analysis' estimate of $\\hat{B}$, but this line does not include uncertainty in these coefficient values. \n\n### Multivariate $t$-distribution method\nFirst, we will calculate uncertainty by sampling from the multivariate $t$ distribution that represents error in regression coefficients, $\\hat{B}$. \n\n```{r}\n# We will \"bootstrap\" 1000 samples of intercept and slope\nset.seed(3)\nn_samp = 500\n\n# Draw from the multivariate t \n# which represents (\\hat{B} - B)\ntest_mat_deviates = \n  mnormt::rmt(n_samp, mean = c(0,0), S = beta_cov_mat, df = t_df)\n\n# Now calculate the realized intercept and slope\n# using the t-distributed deviates\ntest_mat_t = cbind(\n  lm_coef[1] + c(test_mat_deviates[,1]),\n  lm_coef[2] + c(test_mat_deviates[,2])\n)\n\n# Calculate the 95% quantiles and compare to the \n# calculated 95% confidence intervals from above\napply(test_mat_t, \n      MARGIN = 2, # applies function (FUN) to columns (dim 2)\n      FUN = quantile, probs = c(0.025, 0.5, 0.975))\n\n# Compare\nci_mat\n\n# Plot the relationship between intercept and slope\n# Notice the covariance\nplot(test_mat_t, xlab = \"Intercept\", ylab = \"Slope\")\n```\n\nNext, for each pair of intercept and slope randomly drawn above, we will calculate the expected values of $Y$ across the range of covariate $x$. We will then summarize the 95% quantile of expected $Y$ at each value of $x$ in this interpolation. To do this, we need a function to calculate the expected value of $Y$. This function will have the intercept and slope as inputs and will output the expected values of $Y$ across a range of $x$. Then, we will `apply()` this function using all of the values of intercept and slope, in a vectorized and therefore very efficient manner, rather than using any `for` loops. \n\n```{r}\n# Create a matrix that holds the values of x\n# over which we want to interpolate the expected\n# values of Y\nx_fake_mat = \n  cbind(\n    rep(1, times = 100),\n    seq(0,max(x1),length.out = 100)\n  )\n\n# Create a function that will calculate the expected values\ny_hat_fun = function(x, x_mat){\n  x_mat %*% x\n}\n\n# Apply this function to all intercepts and slopes that\n# we drew from the multivariate t\ny_pred_mt = apply(test_mat_t, 1, y_hat_fun, x_mat=x_fake_mat)\n\n# Summarize the 95% quantile of the expected value of Y\n# at each value of x \ny_pred_mt_summary = apply(y_pred_mt, 1, quantile, probs = c(0.025, 0.975))\nstr(y_pred_mt_summary)\n```\n\n### `predict()` function method\n\nR has a built-in function `predict()` (see specific variant `predict.lm()`) which calculates expected values of the dependent variable from a linear regression model estimated using the function `lm()`.\n\n```{r}\n# Note that 'newdata' must be a data frame that includes the ranges\n# of each covariate in the regression model for which you want \n# to generate interpolated or predicted values of the dependent variable\n\n# Here we are calculated the expected values as well as the \n# 95% confidence intervals for those expected values\ny_predict = predict(lm_out,\n                 newdata = data.frame(x1 = c(x_fake_mat[,2])),\n                 interval = \"confidence\", level = 0.95)\nstr(y_predict)\n```\n\n### Compare the two methods \n\nLet's visualize the output of the two methods to compare. \n\n```{r}\n# plot\nplot(x=NA,y=NA,xlab = \"x\", ylab = \"y\",\n     xlim = c(0,max(x1)), ylim = c(-5, 25), pch = 19)\n# Plot the expected values of Y for each pair of int/slope \nfor(i in 1:n_samp){\n  lines(y_pred_mt[,i] ~ x_fake_mat[,2],\n        # Reduce the opacity of each line\n        col = scales::alpha(\"black\", alpha = 0.1), lwd = 2)\n}\n# Add the data points\npoints(y ~ x1, col = 'orange', pch = 19, cex = 2)\n# Add the expected values of Y from \\hat{B}\nabline(coef = lm_coef, col = \"orange\", lwd = 3)\n# Add the conf int of expected Y using multivariate t\nlines(y_pred_mt_summary[1,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\nlines(y_pred_mt_summary[2,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\n# Add the conf int of expected Y using predict() function\nlines(y_predict[,\"lwr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\nlines(y_predict[,\"upr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\n```\nThere is yet a third option to calculate the uncertainty in predicted (i.e., interpolated or extrapolated) values of \n$Y$, which is to derive an exact calculation of the confidence interval using the $t$ distribution, similar to that shown in @sec-conf-beta. See Ch4.1 of Dr. Barber's book for this derivation. \n\n## Multiple Linear Regression\n\nSo far, we have only discussed a single input variable in our model, which is a simple linear regression. When we have multiple input variables, we are dealing with multiple linear regression analysis, so the model looks like:\n$$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_{p-1} x_{p-1,i} + \\epsilon_i$$\nwhere $p$ is the number of model coefficients and $p-1$ is the number of input variables. Still, in matrix notation the model is $Y = XB + \\epsilon$, so the least squares regression analysis approach still works. However, our interpretation of the model coefficients becomes a bit more challenging. \n\nLet's look at a data set within the `faraway` package.\n```{r}\n#| message: false\n#| warning: false\n#| fig-width: 8\n#| fig-height: 3.5\n\nlibrary(faraway)\ndata(gala)\n\n# Plot the raw data relationships\npar(mfrow=c(1,3))\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Area, xlab = \"Area\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Adjacent, xlab = \"Adjacent\", ylab = \"Species\", pch = 19)\npar(mfrow=c(1,1))\n\n# Conduct multiple and single linear regressionm, focusing on Elevation\nm1 = lm(Species ~ Elevation + Area + Adjacent, data = gala)\nm2 = lm(Species ~ Elevation, data = gala)\ncoef(m1); coef(m2)\n```\n\n```{r}\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nabline(coef=coef(m1)[1:2])\nabline(coef=coef(m2)[1:2], lty = 2)\n```\n\nWhat we see above is how the addition of `Area` and `Adjacent` input variables into the model \"adjusts\" the effect of `Elevation`, leading to two unique estimates of the slope (i.e., effect) of `Elevation`. Let's probe multiple linear regression more closely by using simulated data.\n\nFirst, let's simulate a model with 80 data points that correspond to observations of 4 input variables and one outcome variable. Note that in [Footnotes @sec-mlr], we show a case with a categorical/binary input variable. \n```{r}\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    hist(xmat[,i], main = paste(\"covariate \", i-1))\n}\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\npar(mfrow=c(1,1))\nhist(y)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n```\n\nHow do we figure out the expected value of $y$ for a particular situation? Here's an example. What is the expected value of $y$ when $x_2 = 0.5$, but the rest of the input variables are at their average values?\n\n```{r}  \n# Written out long-ways:\npred_y = \n    betas[1]*1 + \n    betas[2]*mean(xmat[,2]) + \n    betas[3]*0.5 + \n    betas[4]*mean(xmat[,4]) + \n    betas[5]*mean(xmat[,5]) \npred_y\n```\n\nNow let's use ordinary least squares regression to estimate our model coefficients from the data, and then compare these to our \"known\" values of the model parameters. \n\n```{r}\n# Run the model:\nm1 = lm(y ~ 0 + xmat)\n# Note that the following two models give the same results\n#m2 = lm(y ~ 0 + X1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\n#m3 = lm(y ~ 1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\nsummary(m1)\n#summary(m2)\n#summary(m3)\n\n# Compare known `betas` to estimated coefficients\ncbind(betas, coef(m1)) \n\n# plot the regression lines with abline\ncoef_m1 = coef(m1)\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    \n    plot(y ~ xmat[,i], pch=19,\n         xlab = paste(\"covariate \", i-1),\n         ylab = \"y\",\n         ylim = range(y))\n    abline(coef=coef_m1[c(1,i)])\n}\n```\n\nWell, those regression lines do not look correct. That is because we are interpretting the slopes and intercepts a little incorrectly and not plotting them in the correct manner. \n\n::: {.callout-note}\n## How to plot the output of `lm()` for multiple linear regression\n\nWhen we isolate and visualize the relationship between the outcome and a single input variable, what we are really observing is the adjusted relationship, after accounting for the other input variables in the model. To understand the expected value of $y$ for any particular value of the single input variable, we really need to set the other input variables to their mean value. Let's demonstrate this below with the `predict()` function.\n:::\n\nLet's determine the expected values of $y$ for input variable 2 ($x_2$) and plot it.\n\n```{r}\n# Prediction for covariate 2 when all other input vars at mean\nmy_df = data.frame(xmat[,2:5])\nhead(my_df)\n\n# Re-run the model but with just the input variables, \n# and the intercept is implicit\nm2 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\n# Now let's try to predict y across a range of \n# input variable 2,\n# while holding the other input variables at\n# their average values\n\nn_pred = 100\nnew_df = data.frame(\n  X1 = rep(mean(my_df$X1), n_pred),\n  X2 = seq(0, 20, length.out = n_pred),\n  X3 = rep(mean(my_df$X3), n_pred),\n  X4 = rep(mean(my_df$X4), n_pred)\n)\n\ny_pred2 = predict(m2, newdata = new_df)\n\n# Now plot:\npar(mfrow=c(1,1))\nplot(y ~ my_df$X2, pch = 19,\n     xlab = \"covariate 2\", ylab = \"y\")\nlines(y_pred2 ~ new_df$X2)\n```\n\nNow we see that the `predict()` function shows a more intuitive relationship between input variable $x_2$ and outcome $y$, *while accounting for the effects of the three other input variables*.\n\n\n## Footnotes\n\n### Euclidean norm & cross product {#sec-crossprod}\n\nWe often see the syntax, $|| a ||$, which is the Euclidean norm of the $n$-sized vector $a$: $$|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,$$ so that when we see $|| a ||^2$, this results in the sum of squares of vector $a$, $\\sum_{i=1}^{n} a_i^2$.\n\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, $\\epsilon_i$, are in vector, $\\epsilon$. The sum of squares of vector $\\epsilon$ is therefore $|| \\epsilon ||^2$. Algebraically, we can find this value as the cross product of $\\epsilon$, which is $\\epsilon^{T}\\epsilon$. Let's do a coded example with vector $x$.\n\n```{r}\n\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n\n# Evaluated as cross-product\nt(x) %*% x\n## Or with crossprod()\ncrossprod(x,x)\n\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n\n```\n\n### `solve()` and Inverse of matrix {#sec-solve}\n\nSuppose we have matrices $A$, $X$, and $B$, and the following expression is true: $$AX=B.$$\n\nThen, suppose $X$ is unknown, such that we want to find the solution for $X$, when we rearrange: $$X = A^{-1} B,$$ where $A^{-1}$ is the multiplicative inverse of matrix $A$. To figure this out computationally, we can use the `solve()` function in R, as long as $A$ is a square matrix and has an inverse.\n\n```{r}\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n```\n\nWe can see, then, that `solve()` is internally evaluating $A^{-1}$. Remember that $A^{-1}$ is not trivial to calculate, as it is the matrix that must satisfy: $AA^{-1} = I$, where $I$ is an identity matrix. In fact, `solve(A)` returns the inverse of $A$, if it exists.\n\n```{r}\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\nX\n```\n\n### Multiple linear regression with a categorical input {#sec-mlr}\n\nLet's simulate a case in which we have one categorical input variable that takes on values \"low\", \"medium\", and \"high\", and one continuous input variable. \n```{r}\nset.seed(7)\nn=90\nsigma = 0.8\n\n# Xmatrix\n## Intercept\nx0 = rep(1, times = n) \n## Categorical input variable\n### Note that we need to code this as \"0\" \"1\" \"2\" to \n### simulate our outcome variable \"y\"\nx1 = rep(c(0,1,2), each=n/3)\nx1L = factor(x1, labels = c(\"low\", \"med\", \"high\"))\n## Continuous input variable\nx2 = rnorm(n, 0, 2.5)\nxmat = cbind(x0,x1,x2)\nhead(xmat)\n\n# Intercept and 2 slopes\nbetas=c(1.5, 1.2, -1.5)\n\n# Simulate outcome variable, as usual\ny2 = xmat %*% betas + rnorm(n,0,sigma)\n\n# Plot the relationships\npar(mfrow=c(1,2))\nplot(y2~x1)\nplot(y2~x2)\n\n# Run the model\n## Note that we us the \"factor\" input variable\n## \"x1L\", which has \"levels\"\nm_cat = lm(y2 ~ 1 + x1L + x2)\nsummary(m_cat)\ncoef(m_cat)\n```\n\nHow do we interpret the slopes, because we see there is a separate slope for `x1Lmed` and `x1Lhigh`? We can understand better by seeing how the linear model addes up. For instance, what is the expected value of the outcome variable when $x_1$ is `high`, and $x_2 = 2.0$?\n\n```{r}\n## Using m1_binL:\ny2_pred = \n    1*coef(m_cat)[1] + # Global average (intercept)\n    0*coef(m_cat)[2] + # Not \"med\"\n    1*coef(m_cat)[3] + # Yes \"high\"\n    2.0*coef(m_cat)[4] # x2=2.0 * slope\nas.numeric(y2_pred)\n```\n\nWhen we assigned the slope of the categorical input variable as $1.2$, remember this is the expected change in $y$ as the input variable changes by a value of $1.0$. In the model, we code the $x_1$ variable as taking numerical values $0$, $1$, and $2$ to represent categories, \"low\", \"medium\", and \"high\". So, the slope for `x1med` is the expected change in $y$ as the input variable changes from \"low\" to \"medium\", an effective change of $1.0$. Then, the slope for `x1high` is the expected change in $y$ as the input variable changes from \"low\" to \"high\", an effective change of $2.0$; hence, this slope is estimated as $2.29$, with standard error $0.19$. Notice how this slope is approximately twice our \"known\" slope for the input variable, which was $1.2$."},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":{"toggle":true},"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"printing","output-file":"ols.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","crossref":{"appendix-delim":":"},"bibliography":["references.bib"],"theme":"cosmo","smooth-scroll":true,"code-block-bg":true},"extensions":{"book":{"multiFile":true}}}}}