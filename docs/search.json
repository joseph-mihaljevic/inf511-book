[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "INF511: Modern Regression I",
    "section": "",
    "text": "Preface\nWelcome to INF511: Modern Regression I. In this course, we will do a deep dive into three fundamental methods for estimating the linear relationships between random variables (i.e., linear regression analysis): 4  Ordinary Least Squares, 6  Maximum Likelihood, and 9  Bayesian Inference. We will also explore 5  Hypothesis Testing, and linear models with categorical covariates via 8  ANOVA. This online book serves as a living document of resources for our class. The chapters are complimentary to the lecture handouts. Note that handouts should be downloaded (from Canvas) and printed prior to class. The coded examples on this website will be helpful for solving problem set and homework assignments. Problem sets will have dedicated in-class time, whereas homework assignments will be conducted entirely outside of class time.\nPlease refer to the Appendix A: Syllabus for the course schedule, learning objectives, grading structure, course policies, etc.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "INF511: Modern Regression I",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.\nThis website is published using Github Pages.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "1  Software",
    "section": "",
    "text": "You will need to have all of the following free software downloaded and in working order on your laptop.\n\n\n\n\n\n\nPrior to first lecture\n\n\n\nYou must have the following on your laptops prior to the first lecture.\n\n\n\nCompatible version of R software environment\nLatest version of RStudio Desktop IDE\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\n\n\n\n\n\n\n\nPrior to Bayesian inference\n\n\n\nYou must have the following on your laptops prior to the sections on Bayesian inference.\n\n\n\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software</span>"
    ]
  },
  {
    "objectID": "Rintro.html#load-a-package",
    "href": "Rintro.html#load-a-package",
    "title": "2  Introduction to R",
    "section": "\n2.1 Load a package",
    "text": "2.1 Load a package\n\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)"
  },
  {
    "objectID": "Rintro.html#load-a-data-set",
    "href": "Rintro.html#load-a-data-set",
    "title": "2  Introduction to R",
    "section": "\n2.2 Load a data set",
    "text": "2.2 Load a data set\n\n# R has many built-in data sets for educational purposes.\n# The data() function will show all of these, with descriptions\ndata() # Do this on your own.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "Rintro.html#manipulate-and-visualize-the-data",
    "href": "Rintro.html#manipulate-and-visualize-the-data",
    "title": "2  Introduction to R",
    "section": "\n2.3 Manipulate and visualize the data",
    "text": "2.3 Manipulate and visualize the data\n\n# Load the 'iris' data set, which contains lengths and widths of flower petals\n# We choose this, because these lengths and widths are often linearly associated\ndata(iris)\n\nis.data.frame(iris) # Validate as data frame object\n\n[1] TRUE\n\nis.vector(iris) # Not a vector object\n\n[1] FALSE\n\nstr(iris) # Show general object characteristics\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nglimpse(iris) # Similar to str() but more detailed/relevant\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# glimpse() is a function within the dplyr package. \n# We can call the function like this, because dplyr was loaded already\n# Alternatively we can use the following syntax to call non-base functions\ndplyr::glimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n# This way, you are less prone to errors, especially if \n# functions in different packages are named the same or similarly (masking)\n\nFrom the glimpse() function we see that there are five columns within the iris data frame, and there are 150 total data points (rows). We also see that four columns are numeric doubles, while one (Species) is a factor.\n\n# Subset column vectors from the data frame using the '$' symbol\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n# R can do 'vectorized' functions \niris$Petal.Width * 2\n\n  [1] 0.4 0.4 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.2 0.4 0.4 0.2 0.2 0.4 0.8 0.8 0.6\n [19] 0.6 0.6 0.4 0.8 0.4 1.0 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.8 0.2 0.4 0.4 0.4\n [37] 0.4 0.2 0.4 0.4 0.6 0.6 0.4 1.2 0.8 0.6 0.4 0.4 0.4 0.4 2.8 3.0 3.0 2.6\n [55] 3.0 2.6 3.2 2.0 2.6 2.8 2.0 3.0 2.0 2.8 2.6 2.8 3.0 2.0 3.0 2.2 3.6 2.6\n [73] 3.0 2.4 2.6 2.8 2.8 3.4 3.0 2.0 2.2 2.0 2.4 3.2 3.0 3.2 3.0 2.6 2.6 2.6\n [91] 2.4 2.8 2.4 2.0 2.6 2.4 2.6 2.6 2.2 2.6 5.0 3.8 4.2 3.6 4.4 4.2 3.4 3.6\n[109] 3.6 5.0 4.0 3.8 4.2 4.0 4.8 4.6 3.6 4.4 4.6 3.0 4.6 4.0 4.0 3.6 4.2 3.6\n[127] 3.6 3.6 4.2 3.2 3.8 4.0 4.4 3.0 2.8 4.6 4.8 3.6 3.6 4.2 4.8 4.6 3.8 4.6\n[145] 5.0 4.6 3.8 4.0 4.6 3.6\n\n# Check if factor\nis.factor(iris$Species)\n\n[1] TRUE\n\nis.character(iris$Species)\n\n[1] FALSE\n\n\nLet’s make some exploratory visualizations.\n# Histogram\nhist(iris$Petal.Length)\nhist(iris$Petal.Width)\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Explore association between random variables\n# formula method: y ~ x \n# Read the above like: \n# y-variable 'modeled by' x-variable, or\n# y-variable 'as a function of' x-variable\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19) #pch = plot character",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "Rintro.html#is-there-a-linear-association",
    "href": "Rintro.html#is-there-a-linear-association",
    "title": "2  Introduction to R",
    "section": "\n2.4 Is there a linear association?",
    "text": "2.4 Is there a linear association?\nThe goal of regression is to determine the functional association between random variables. With linear regression the specific goal is to test whether there is a linear relationship between a response variable (y) and one or more covariates (x). The form of the functional relationship is: \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i ,\\] where \\(y_i\\) is the \\(i\\)-th data point, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope, and \\(x\\) is the single covariate in the model. In matrix form we have: \\[\\bf{y} = \\bf{X} \\bf{B} + \\boldsymbol\\epsilon\\] For hypothesis testing, we are testing the null hypothesis that the slope of the relationship between \\(x\\) and \\(y\\) is zero (i.e., no detectable linear relationship, \\(\\beta_1 = 0\\)).\nWe can conduct linear regression in R using the lm() function, where ‘lm’ stands for ‘linear model’. This function specifically estimates the model parameter (slope, intercept, and residual variance), using the ordinary least squares approach, which we will soon learn in lecture.\n\nmy_model = lm(formula = Petal.Width ~ Petal.Length,\n              data = iris)\n\nThe line above stores the output of the linear model in the my_model object. We can then manipulate the my_model object and apply various functions to help us understand the outcome of the linear regression analysis.\n\nstr(my_model)\n\nList of 12\n $ coefficients : Named num [1:2] -0.363 0.416\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"Petal.Length\"\n $ residuals    : Named num [1:150] -0.019 -0.019 0.0226 -0.0606 -0.019 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:150] -14.6888 8.9588 0.0257 -0.0576 -0.0159 ...\n  ..- attr(*, \"names\")= chr [1:150] \"(Intercept)\" \"Petal.Length\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:150] 0.219 0.219 0.177 0.261 0.219 ...\n  ..- attr(*, \"names\")= chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:150] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"Petal.Length\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.08 1.1\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 148\n $ xlevels      : Named list()\n $ call         : language lm(formula = Petal.Width ~ Petal.Length, data = iris)\n $ terms        :Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. ..$ : chr \"Petal.Length\"\n  .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n $ model        :'data.frame':  150 obs. of  2 variables:\n  ..$ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n  ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language Petal.Width ~ Petal.Length\n  .. .. ..- attr(*, \"variables\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"Petal.Width\" \"Petal.Length\"\n  .. .. .. .. ..$ : chr \"Petal.Length\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"Petal.Length\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(Petal.Width, Petal.Length)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"Petal.Width\" \"Petal.Length\"\n - attr(*, \"class\")= chr \"lm\"\n\n\nObviously, the output of the analysis is a complicated data structure with many elements. There are, however, some convenient functions to summarize these outputs for us.\n\nsummary(my_model)\n\n\nCall:\nlm(formula = Petal.Width ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.56515 -0.12358 -0.01898  0.13288  0.64272 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.363076   0.039762  -9.131  4.7e-16 ***\nPetal.Length  0.415755   0.009582  43.387  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2065 on 148 degrees of freedom\nMultiple R-squared:  0.9271,    Adjusted R-squared:  0.9266 \nF-statistic:  1882 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\nAbove is the main outcome that we care about. The summary() function tells us the parameter estimates (with estimates of parameter uncertainty). It also conducts null-hypothesis testing, providing p-values, and shows the goodness of model fit, using R-squared.\n\n\n\n\n\n\nTip\n\n\n\nThe goal of the first part of this course is to understand in sufficient detail how this analysis is conducted, so that we can interpret the results from a well-informed standpoint.\n\n\n\nplot(iris$Petal.Width ~ iris$Petal.Length,\n     xlab = \"Length\",\n     ylab = \"Width\",\n     pch = 19)\n# Add the estimated linear relationship\nabline(reg = my_model)\n\n\n\nData with fitted linear relationship.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "prob.html#lecture-material",
    "href": "prob.html#lecture-material",
    "title": "\n3  Probability distributions\n",
    "section": "\n3.1 Lecture material",
    "text": "3.1 Lecture material\nPlease download and print the lecture material from here. After lecture, the recording will also appear in this section."
  },
  {
    "objectID": "prob.html#gaussian-normal-distribution",
    "href": "prob.html#gaussian-normal-distribution",
    "title": "3  Probability Distributions",
    "section": "",
    "text": "## Plot the pdf with segments\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"x\", ylab = expression(\"P(x |\"~mu~\",\"~sigma~\")\"))\nlines(norm_pdf ~ x_vals)\nsegments(x0 = c(x_low95, mu, x_high95), x1 = c(x_low95, mu, x_high95),\n         y0 = rep(0, times = 3), y1 = c(p_low95, p_mu, p_high95))\n# Now, let's draw random samples from this normal distribution\nn_rand = 1000\nx_rand = rnorm(n_rand, mean = mu, sd = sigma)\n\n# Plot a histogram and overlay the approximate expectations\n## The line below assumes you draw 'n_rand' samples\nhist(x_rand, breaks = 20, main = \"\")\nlines(norm_pdf*n_rand ~ x_vals)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob.html#multivariate-normal-distribution",
    "href": "prob.html#multivariate-normal-distribution",
    "title": "3  Probability Distributions",
    "section": "\n3.2 Multivariate normal distribution",
    "text": "3.2 Multivariate normal distribution\n\n3.2.1 Relation to residuals, \\(\\epsilon\\)\n\nRecall our linear model in matrix notation: \\(Y = XB + \\epsilon\\). We use the multivariate normal distribution to describe the probability density of the residuals, \\(\\epsilon\\). Recall that each individual residual, \\(\\epsilon_i\\) follows a normal distribution with mean zero and standard deviation equal to the residual error, \\(\\sigma\\): \\(\\epsilon_i \\sim N(0, \\sigma)\\). Also recall that the linear regression analysis assumes that \\(\\epsilon_i\\) are I.I.D. (independent and identically distributed). \\(\\epsilon_i \\sim N(0, \\sigma)\\) implies the identical distribution (i.e., each residual follows the same normal distribution). The “independent” part means that the residual values are not correlated in any way, meaning that they do not covariance is zero. Thus, we can use vector notation to say that the vector \\(\\epsilon\\) follows a multivariate normal distribution with all means equal to zero and covariance matrix \\(\\Sigma = \\sigma^2 I\\), where \\(I\\) is a square identity matrix: \\(\\epsilon \\sim N(0, \\sigma^2 I)\\). More about covariance and covariance matrices is available below (Footnotes 3.4.1).\nThe multivariate normal probability distribution is hard to visualize, because it is in multiple dimensions. But we can use similar R functions to understand the distribution. These functions are not in the base installation of R, so we need another package, MASS. We’ll also need the Matrix package later.\n\n# Install packages if you don't already have them, e.g., \n# install.packages(\"MASS\", dependencies = TRUE)\nlibrary(MASS)\nlibrary(Matrix)\n\nWarning: package 'Matrix' was built under R version 4.3.1\n\n# Define mean and st.dev.\nmu_epsilon = 0\nsigma_epsilon = 2.0\n\n# sample size\nn_resid = 1000\n\n# we need a vector of means\nmu_vec = rep(0, n_resid)\n\n# we need an identity matrix\nI_mat = matrix(0, nrow = n_resid, ncol = n_resid)\n## specify the diagonal = 1\ndiag(I_mat) = 1\n\n# Draw randomly from the multivariate normal\nmvn_epsilon = MASS::mvrnorm(n = 1, \n                            mu = mu_vec,\n                            Sigma = sigma_epsilon^2*I_mat)\n# We can see that an entire array of size n_resid is drawn\nstr(mvn_epsilon)\n\n num [1:1000] 2.296 1.396 0.767 -3.057 -1.028 ...\n\n# How does this compare to drawing them independently?\nnorm_epsilon = rnorm(n_resid, mean = mu_epsilon, sd = sigma_epsilon)\nc(mean(mvn_epsilon), mean(norm_epsilon))\n\n[1] -0.10146125  0.09209139\n\nc(sd(mvn_epsilon), sd(norm_epsilon))\n\n[1] 2.011679 1.994997\n\n\n# Compare these two vectors visually:\nhist(mvn_epsilon)\nhist(norm_epsilon)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.2 Multivariate normal distribution with non-independent variables\nLet’s explore the multivariate normal a bit more. Suppose we have three random variates \\(a\\), \\(b\\), and \\(c\\). Suppose further that \\(a\\) and \\(b\\) are positively correlated with each other, but \\(c\\) is not correlated with either other variate.\n\n# Establish means and variances of a, b, and c\nmu_vec = c(1.0, 2.2, 1.5)\nsd_vec = c(1.5, 0.5, 0.75)\n\n# Manually construct the covariance matrix:\ncov_mat_test = matrix(\n    data = c(0.0, 0.6, 0.0,\n             0.6, 0.0, 0.0,\n             0.0, 0.0, 0.0),\n    ncol = 3, nrow = 3,\n    byrow = TRUE\n)\ndiag(cov_mat_test) = sd_vec^2\n\n# Matrix must be positive definite (PD). \n# This gives closest PD\ncov_mat = Matrix::nearPD(cov_mat_test)$mat\n# Look if you want: str(cov_mat)\n\n# Draw some random vectors:\nabc_array = mvrnorm(n = 100, mu = mu_vec, Sigma = cov_mat)\n# Look at structure if you want; str(abc_array)\n\n# Visualize the relationships between a, b, and c:\ncolnames(abc_array) = letters[1:3]\npairs(abc_array)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob.html#poisson-distribution",
    "href": "prob.html#poisson-distribution",
    "title": "3  Probability Distributions",
    "section": "\n3.3 Poisson distribution",
    "text": "3.3 Poisson distribution\nThe normal and multivariate normal probability distributions have PDFs related to continuous random variables. In many cases our data are not continuous, but are instead discrete. The Poisson distribution represents the PMF (probability mass function) of count data and is described by a single parameter, \\(\\lambda\\), which is equal to the mean and variance of the distribution. In regression, we can use the Poisson distribution to analyze a generalized linear model between a discrete response variable (e.g., count data) and its covariates, but we will not deal with that in our class.\n# Define the parameter\nlambda = 8\n\n# Visualize the probability density function (pdf)\n## Remember this is a discrete distribution\nk_vals = c(0:20)\npois_pdf = dpois(k_vals, lambda = lambda)\n\nplot(x = NA, y = NA, xlim = c(0, 20), ylim = c(0, 0.2),\n     xlab = \"k\", ylab = expression(\"P(k |\"~lambda~\")\"))\npoints(pois_pdf ~ k_vals)\nsegments(x0 = k_vals, x1 = k_vals,\n         y0 = 0, y1 = pois_pdf)\n## Compare to randomly drawn values:\nk_rand = rpois(n_rand, lambda = lambda)\nhist(k_rand, breaks = 25, main = \"\")\npoints(pois_pdf*n_rand ~ k_vals, pch = 19)\n# On your own, use the ppois() and qpois() functions to understand their inputs/outputs",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob.html#footnotes",
    "href": "prob.html#footnotes",
    "title": "3  Probability Distributions",
    "section": "\n3.4 Footnotes",
    "text": "3.4 Footnotes\n\n3.4.1 Covariance matrix\nAs reminder, the variance of a random variable, \\(x\\), with sample size \\(n\\) is: \\[\\sigma^2_x = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(x_i - \\bar{x}) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})^2.\\] And \\(\\bar{x}\\) is the sample mean. Similarly, then, the covariance of samples from two random variables, \\(x\\) and \\(y\\), can be calculated as: \\[\\sigma(x,y) = \\frac{1}{n-1} \\sum_i^n (x_i - \\bar{x})(y_i - \\bar{y}).\\] The syntax for the covariance of a sample population with itself is, for example, \\(\\sigma(x, x)\\), which is simply equal to the variance \\(\\sigma_x^2\\). The covariance matrix for these two sample populations would be: \\[C = \\begin{bmatrix}\n\\sigma(x,x) & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma(y,y)\n\\end{bmatrix}.\\] This can be simplified using the variance notation: \\[C = \\begin{bmatrix}\n\\sigma^2_x & \\sigma(x,y)\\\\\n\\sigma(y,x) & \\sigma^2_y\n\\end{bmatrix}.\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "ols.html#lecture-material",
    "href": "ols.html#lecture-material",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.1 Lecture material",
    "text": "4.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "ols.html#in-class-code",
    "href": "ols.html#in-class-code",
    "title": "4  Ordinary Least Squares",
    "section": "In-class Code",
    "text": "In-class Code\nRemember that our goal is to estimate the linear relationship between data observations of response variable, \\(y\\), and its measured covariate, \\(x\\), following: \\(Y = XB + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2 I).\\) Our coefficients to estimate are therefore \\(\\hat{B}\\), which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), \\(\\hat{\\sigma}\\). To estimate the coefficients, we are attempting to minimize the residual sum of squares, \\(|| \\epsilon || ^ 2\\). See Footnotes 4.12.1 for more information regarding this notation."
  },
  {
    "objectID": "ols.html#generate-the-data",
    "href": "ols.html#generate-the-data",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.2 Generate the data",
    "text": "4.2 Generate the data\nWe’ll start with a very small data set to emphasize the basics, and then the in-class activity will go into more depth. Here, we’ll implement the OLS estimation with a single covariate that we demonstrated in lecture.\n\nn = 4 # number observations\np = 2 # number of parameters\n\n# Covariate:\nx0 = c(1,1,1,1) # placeholder for intercept\nx1 = c(2,3,5,1) # value of x\nxmat = matrix(data = c(x0,x1), \n               nrow = n, \n               ncol = p)\nxmat\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    3\n[3,]    1    5\n[4,]    1    1\n\n# Coefficients:\n## betas[1]: intercept\n## betas[2]: slope\nbetas = c(4, 2)\n\nxmat %*% betas\n\n     [,1]\n[1,]    8\n[2,]   10\n[3,]   14\n[4,]    6\n\n# residuals\nepsilon = c(0, -1, 1, 3)\n\n# Data observations:\ny = xmat %*% betas + epsilon",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#plot-the-relationship",
    "href": "ols.html#plot-the-relationship",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.3 Plot the relationship",
    "text": "4.3 Plot the relationship\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(x=NA,y=NA, xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)), xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas, col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(x0 = x1, x1 = x1,\n         y0 = y, y1 = y - epsilon)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1, cex = 1.25)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#sec-lm-output",
    "href": "ols.html#sec-lm-output",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.4 Estimate the coefficients using R’s lm() function",
    "text": "4.4 Estimate the coefficients using R’s lm() function\n\n# Run the model:\nlm_out = lm(y ~ 1 + x1)\n# Show the summary output\nsummary(lm_out)\n\n\nCall:\nlm(formula = y ~ 1 + x1)\n\nResiduals:\n     1      2      3      4 \n-1.029 -1.657  1.086  1.600 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   5.7714     2.0500   2.815    0.106\nx1            1.6286     0.6565   2.481    0.131\n\nResidual standard error: 1.942 on 2 degrees of freedom\nMultiple R-squared:  0.7547,    Adjusted R-squared:  0.6321 \nF-statistic: 6.153 on 1 and 2 DF,  p-value: 0.1313\n\n# Extract the estimated coefficients\nlm_coef = coef(lm_out)\nlm_coef\n\n(Intercept)          x1 \n   5.771429    1.628571",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#estimate-the-coefficients-manually",
    "href": "ols.html#estimate-the-coefficients-manually",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.5 Estimate the coefficients manually",
    "text": "4.5 Estimate the coefficients manually\nNow we will use the matrix algebra and derivation of normal equations to estimate the intercept and slope from the observations, \\(Y\\). Remember that we estimate the coefficient vector, \\(\\hat{B}\\) from: \\[X^TX \\hat{B} = X^T Y\\] \\[\\hat{B} = (X^TX)^{-1} X^T Y\\] These equations include the multiplicative inverse matrix, \\((X^TX)^{-1}\\). See the Footnotes 4.12.2 for more information about inverse matrices and the solve() function.\n\n# Let's break up the normal equations into intermediates:\nxtx = t(xmat) %*% xmat\n\n## Use solve() to find inverse of xtx\n## why solve()? See Appendix, linked above.\ninv_xtx = solve(xtx)\nxty = t(xmat) %*% y\n\nbhat = inv_xtx %*% xty\n\n# More efficient:\n# Remember, xtx * bhat = xty\n# So we can use solve() again\nbhat_solve = solve(xtx, xty)\n\n# Are they the same?\n\n# How does this manual solution compare to lm()'s solution?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#sec-est-plot",
    "href": "ols.html#sec-est-plot",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.6 Plot the estimated relationships",
    "text": "4.6 Plot the estimated relationships\n\n# Plot in layers\n## Create a blank plotting canvas, specifying axis limits\nplot(NA,NA,\n     xlab = \"x\", ylab = \"y\",\n     ylim = c(0,max(y)),\n     xlim = c(0,max(x1)))\n## Add data points\npoints(y ~ x1, pch = 19, cex = 2)\n## Add known linear relationship\nabline(coef = betas,\n       col = \"black\", lwd = 2)\n\n# Show the residuals:\nsegments(\n  x0 = x1,\n  x1 = x1,\n  y0 = y,\n  y1 = y - epsilon,\n)\n\n# Show the model predictions, \\hat{y}:\ny_hat = xmat %*% betas\npoints(y_hat ~ x1,\n       cex = 1.25)\n\n# Add the lm() estimate:\nabline(coef = lm_coef,\n       col = \"orange\", lty = 2, lwd = 2)\n\n# Add the manual OLS estimate:\nabline(coef = bhat_solve,\n       col = \"purple\", lty = 3, lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#why-are-the-hatb-different-from-true-b",
    "href": "ols.html#why-are-the-hatb-different-from-true-b",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?",
    "text": "4.7 Why are the \\(\\hat{B}\\) different from true \\(B\\)?\nRemember, we are estimating the coefficients by minimizing the sum of squared errors (SSE), \\(|| \\epsilon ||^2\\).\n\n# True sum of squares:\nsum(epsilon^2)\n\n[1] 11\n\n# Estimated (i.e., minimized sum of squares):\n## From lm()\nsum(lm_out$residuals^2)\n\n[1] 7.542857\n\n## From manual OLS\nsum( (y - xmat %*% bhat_solve)^2 )\n\n[1] 7.542857\n\n\nYou can see that the OLS strategy minimized SSE, but this is actually lower than the true SSE.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#understanding-uncertainty-in-hatb",
    "href": "ols.html#understanding-uncertainty-in-hatb",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.8 Understanding Uncertainty in \\(\\hat{B}\\)\n",
    "text": "4.8 Understanding Uncertainty in \\(\\hat{B}\\)\n\nWhile the OLS analysis estimates the regression coefficients, \\(\\hat{B}\\), from the observed data \\(Y\\), our estimates of these coefficients have error (i.e., uncertainty), such that the estimates are only as good as the data. Specifically, if we have fewer data points (i.e., \\(n\\) is low), we have less certainty in \\(\\hat{B}\\). In lecture, we showed, that: \\[\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right), \\] and we know that \\(\\hat{\\sigma}^2\\) depends on sample size \\(n\\), following: \\[\\hat{\\sigma}^2 \\quad = \\quad \\frac{1}{n-p} (Y_{obs} - Y_{pred})^T (Y_{obs} - Y_{pred}) \\quad = \\quad \\frac{1}{n-p} \\hat{\\epsilon}^T \\hat{\\epsilon}\\]\nUsing these equations, we showed then that \\(SE(\\beta_i) = \\sqrt{diag\\left( (X^TX)^{-1} \\right)_i \\hat{\\sigma}^2}\\). Let’s calculate this manually and compare to the output of the lm() function.\n\n# Extract the model summary, which has useful components\nlm_out_summary = summary(lm_out)\n# Extract the estimated residual standard deviation, sigma\nest_sigma = lm_out_summary$sigma\nest_sigma\n\n[1] 1.942017\n\n# We already calculated (X^T X)^{-1} as inv_xtx\nbeta_cov_mat = inv_xtx * est_sigma^2\nbeta_cov_mat\n\n          [,1]       [,2]\n[1,]  4.202449 -1.1853061\n[2,] -1.185306  0.4310204\n\nse_beta = sqrt(diag(beta_cov_mat))\nse_beta\n\n[1] 2.0499876 0.6565214\n\n\nCompare these values to the output of the summary() of Section 4.4 in the column labelled Std. Error.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#sec-conf-beta",
    "href": "ols.html#sec-conf-beta",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.9 Confidence Intervals for \\(\\hat{B}\\)\n",
    "text": "4.9 Confidence Intervals for \\(\\hat{B}\\)\n\nTo calculate confidence intervals for \\(\\hat{B}\\), we first must understand the \\(t\\) (a.k.a. Student’s \\(t\\)) probability distribution. This distribution represents the case when we are estimating the mean of a normally distributed variable and either the sample size is small or the variable’s standard deviation is unknown. Essentially, the \\(t\\) distribution increases the uncertainty (i.e., variance) in cases of low sample size (i.e., small \\(n\\)). With low sample size (and/or high number of parameters), the degrees of freedom of the \\(t\\)-distribution, \\(\\nu\\) is low, whereas with high sample size, \\(\\nu\\) is large. As \\(\\nu\\) approaches infinity, the \\(t\\)-distribution approximates the standard normal distribution (i.e., \\(N(\\mu, \\sigma)|\\mu=0,\\sigma=1\\)).\n\n\n\n\n\n\n\n\nIt is the case for \\(\\hat{B} \\sim N \\left( B, (X^TX)^{-1} \\hat{\\sigma}^2 \\right)\\) that we do not know the mean (\\(B\\)), and we are estimating the variance, \\(\\hat{\\sigma}^2\\). Specifically, we are estimating the true mean vector, \\(B\\), as \\(\\hat{B}\\), and we are estimating the variance of the residuals as \\(\\hat{\\sigma}^2\\). We can therefore re-write the uncertainty in \\(\\hat{B}\\) as a multivariate \\(t\\) distribution: \\[(\\hat{B} - B) \\sim t_{\\nu} \\left( 0, \\Sigma \\right),\\] where the means are zero, \\(\\nu\\) is the degrees of freedom (i.e., \\(n-p\\)), and \\(\\Sigma = (X^TX)^{-1} \\hat{\\sigma}^2\\). \\((\\hat{B} - B)\\) represents the deviation of the estimated coefficients from the true coefficients, which is why the distribution is centered around zero. It is perhaps easier to separate the individual estimated coefficients, \\(\\beta_i\\), into their separate \\(t\\)-distributions: \\[\\frac{(\\hat{\\beta}_i - \\beta_i)}{SE(\\hat{\\beta}_i)} \\sim t_{\\nu}\\] \\[(\\hat{\\beta}_i - \\beta_i) \\sim t_{\\nu} SE(\\hat{\\beta}_i),\\] which shows that the \\(t\\)-distribution that describes the deviation of regression coefficients from the true value of those coefficients is scaled by the uncertainty in the estimated coefficients \\(SE(\\hat{\\beta}_i)\\). As shown in Dr. Barber’s materials, using this information, we can derive the confidence interval (at the \\(\\alpha\\) confidence level) calculation for \\(\\hat{\\beta}_i\\) as: \\[ \\hat{\\beta}_i \\pm t \\left(\\frac{1-\\alpha}{2}, \\nu \\right) SE(\\hat{\\beta}_i),\\] where the \\(t()\\) notation represents the critical value of the \\(t\\)-distribution, \\(t_{crit}\\), with \\(\\nu\\) degrees of freedom, for which \\(P(z \\le t_{crit}) = \\frac{1-\\alpha}{2}\\), and \\(z\\) is a continuous, random variable. This critical value can be calculated in R using the qt() function, which we show below.\n\n\n\n\n\n\nCovariance of \\(\\hat{\\beta}_i\\)\n\n\n\nAlthough it is convenient and easier to digest the confidence interval of individual \\(\\hat{\\beta}_i\\), we must realize that the estimates of the \\(\\beta_i\\) can covary (i.e., have non-zero covariance), which is quantified in the variance-covariance matrix of \\(\\hat{B}\\), \\((X^TX)^{-1} \\hat{\\sigma}^2\\). We will show why this is important below.\n\n\nLet’s manually calculate the 95% confidence intervals in \\(\\hat{B}\\) and compare to R’s internal function confint().\n\n# Extract the degrees of freedom from the model (\\nu)\n# which can also be calculated as n - p\nt_df = lm_out$df.residual\n\n# Calculate t critical for alpha = 0.05\n# This will give us the 95% conf interval (CI)\nt_crit = qt(1-(0.05/2), df = t_df)\n\n# Calculate the upper and lower CI for both betas\nci_int = lm_coef[1] + c(-1,1)*t_crit*se_beta[1]\nci_slope = lm_coef[2] + c(-1,1)*t_crit*se_beta[2]\n\n# Construct a table of values\nci_mat = \n    rbind(c(lm_coef[1], ci_int),\n          c(lm_coef[2], ci_slope))\ncolnames(ci_mat) = c(\"coef\", \"lowCI\", \"highCI\")\nrownames(ci_mat) = c(\"intercept\", \"slope\")\nci_mat\n\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n\n# Compare these manual calculations to built-in\n# function confint(), which by default extracts the \n# 95% CI for a lm() model's coefficients\nconfint(lm_out)\n\n                2.5 %    97.5 %\n(Intercept) -3.048956 14.591813\nx1          -1.196212  4.453355",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#propagate-uncertainty-in-hatb-for-predictions-of-y",
    "href": "ols.html#propagate-uncertainty-in-hatb-for-predictions-of-y",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.10 Propagate uncertainty in \\(\\hat{B}\\) for predictions of \\(Y\\)\n",
    "text": "4.10 Propagate uncertainty in \\(\\hat{B}\\) for predictions of \\(Y\\)\n\nThere are several ways to calculate and visualize our uncertainty in model predictions of observed data \\(Y\\) and unobserved data of the dependent variable (i.e., interpolation). The colored lines drawn on the figure in Section 4.6 represent the expected values of \\(Y\\) based on the OLS analysis’ estimate of \\(\\hat{B}\\), but this line does not include uncertainty in these coefficient values.\n\n4.10.1 Multivariate \\(t\\)-distribution method\nFirst, we will calculate uncertainty by sampling from the multivariate \\(t\\) distribution that represents error in regression coefficients, \\(\\hat{B}\\).\n\n# We will \"bootstrap\" 1000 samples of intercept and slope\nset.seed(3)\nn_samp = 500\n\n# Draw from the multivariate t \n# which represents (\\hat{B} - B)\ntest_mat_deviates = \n  mnormt::rmt(n_samp, mean = c(0,0), S = beta_cov_mat, df = t_df)\n\n# Now calculate the realized intercept and slope\n# using the t-distributed deviates\ntest_mat_t = cbind(\n  lm_coef[1] + c(test_mat_deviates[,1]),\n  lm_coef[2] + c(test_mat_deviates[,2])\n)\n\n# Calculate the 95% quantiles and compare to the \n# calculated 95% confidence intervals from above\napply(test_mat_t, \n      MARGIN = 2, # applies function (FUN) to columns (dim 2)\n      FUN = quantile, probs = c(0.025, 0.5, 0.975))\n\n           [,1]       [,2]\n2.5%  -3.820226 -0.5319227\n50%    5.809501  1.5967318\n97.5% 13.564890  4.1240734\n\n# Compare\nci_mat\n\n              coef     lowCI    highCI\nintercept 5.771429 -3.048956 14.591813\nslope     1.628571 -1.196212  4.453355\n\n# Plot the relationship between intercept and slope\n# Notice the covariance\nplot(test_mat_t, xlab = \"Intercept\", ylab = \"Slope\")\n\n\n\n\n\n\n\nNext, for each pair of intercept and slope randomly drawn above, we will calculate the expected values of \\(Y\\) across the range of covariate \\(x\\). We will then summarize the 95% quantile of expected \\(Y\\) at each value of \\(x\\) in this interpolation. To do this, we need a function to calculate the expected value of \\(Y\\). This function will have the intercept and slope as inputs and will output the expected values of \\(Y\\) across a range of \\(x\\). Then, we will apply() this function using all of the values of intercept and slope, in a vectorized and therefore very efficient manner, rather than using any for loops.\n\n# Create a matrix that holds the values of x\n# over which we want to interpolate the expected\n# values of Y\nx_fake_mat = \n  cbind(\n    rep(1, times = 100),\n    seq(0,max(x1),length.out = 100)\n  )\n\n# Create a function that will calculate the expected values\ny_hat_fun = function(x, x_mat){\n  x_mat %*% x\n}\n\n# Apply this function to all intercepts and slopes that\n# we drew from the multivariate t\ny_pred_mt = apply(test_mat_t, 1, y_hat_fun, x_mat=x_fake_mat)\n\n# Summarize the 95% quantile of the expected value of Y\n# at each value of x \ny_pred_mt_summary = apply(y_pred_mt, 1, quantile, probs = c(0.025, 0.975))\nstr(y_pred_mt_summary)\n\n num [1:2, 1:100] -3.82 13.56 -3.62 13.58 -3.41 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:2] \"2.5%\" \"97.5%\"\n  ..$ : NULL\n\n\n\n4.10.2 predict() function method\nR has a built-in function predict() (see specific variant predict.lm()) which calculates expected values of the dependent variable from a linear regression model estimated using the function lm().\n\n# Note that 'newdata' must be a data frame that includes the ranges\n# of each covariate in the regression model for which you want \n# to generate interpolated or predicted values of the dependent variable\n\n# Here we are calculated the expected values as well as the \n# 95% confidence intervals for those expected values\ny_predict = predict(lm_out,\n                 newdata = data.frame(x1 = c(x_fake_mat[,2])),\n                 interval = \"confidence\", level = 0.95)\nstr(y_predict)\n\n num [1:100, 1:3] 5.77 5.85 5.94 6.02 6.1 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:100] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:3] \"fit\" \"lwr\" \"upr\"\n\n\n\n4.10.3 Compare the two methods\nLet’s visualize the output of the two methods to compare.\n\n# plot\nplot(x=NA,y=NA,xlab = \"x\", ylab = \"y\",\n     xlim = c(0,max(x1)), ylim = c(-5, 25), pch = 19)\n# Plot the expected values of Y for each pair of int/slope \nfor(i in 1:n_samp){\n  lines(y_pred_mt[,i] ~ x_fake_mat[,2],\n        # Reduce the opacity of each line\n        col = scales::alpha(\"black\", alpha = 0.1), lwd = 2)\n}\n# Add the data points\npoints(y ~ x1, col = 'orange', pch = 19, cex = 2)\n# Add the expected values of Y from \\hat{B}\nabline(coef = lm_coef, col = \"orange\", lwd = 3)\n# Add the conf int of expected Y using multivariate t\nlines(y_pred_mt_summary[1,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\nlines(y_pred_mt_summary[2,] ~ x_fake_mat[,2], lty = 2, lwd = 3, col = \"orange\")\n# Add the conf int of expected Y using predict() function\nlines(y_predict[,\"lwr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\nlines(y_predict[,\"upr\"]~ x_fake_mat[,2], lty = 3, lwd = 3, col = \"purple\")\n\n\n\n\n\n\n\nThere is yet a third option to calculate the uncertainty in predicted (i.e., interpolated or extrapolated) values of \\(Y\\), which is to derive an exact calculation of the confidence interval using the \\(t\\) distribution, similar to that shown in Section 4.9. See Ch4.1 of Dr. Barber’s book for this derivation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "ols.html#footnotes",
    "href": "ols.html#footnotes",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.12 Footnotes",
    "text": "4.12 Footnotes\n\n4.12.1 Euclidean norm & cross product\nWe often see the syntax, \\(|| a ||\\), which is the Euclidean norm of the \\(n\\)-sized vector \\(a\\): \\[|| a || = \\left( \\sum_{i=1}^{n} a_i^2 \\right) ^ {1/2} ,\\] so that when we see \\(|| a ||^2\\), this results in the sum of squares of vector \\(a\\), \\(\\sum_{i=1}^{n} a_i^2\\).\nIn the context of least squares regression, we are trying to minimize the residual sum of squares, where the residuals, \\(\\epsilon_i\\), are in vector, \\(\\epsilon\\). The sum of squares of vector \\(\\epsilon\\) is therefore \\(|| \\epsilon ||^2\\). Algebraically, we can find this value as the cross product of \\(\\epsilon\\), which is \\(\\epsilon^{T}\\epsilon\\). Let’s do a coded example with vector \\(x\\).\n\n# Vector of real numbers\nx = c(1, 2, 3, 4)\n\n# sum of squares\nsum(x^2)\n\n[1] 30\n\n# Evaluated as cross-product\nt(x) %*% x\n\n     [,1]\n[1,]   30\n\n## Or with crossprod()\ncrossprod(x,x)\n\n     [,1]\n[1,]   30\n\n# Euclidean norm also known as the 2-norm\n# so sum of squares is 2-norm, squared\nnorm(x, type = \"2\") ^ 2\n\n[1] 30\n\n\n\n4.12.2 solve() and Inverse of matrix\nSuppose we have matrices \\(A\\), \\(X\\), and \\(B\\), and the following expression is true: \\[AX=B.\\]\nThen, suppose \\(X\\) is unknown, such that we want to find the solution for \\(X\\), when we rearrange: \\[X = A^{-1} B,\\] where \\(A^{-1}\\) is the multiplicative inverse of matrix \\(A\\). To figure this out computationally, we can use the solve() function in R, as long as \\(A\\) is a square matrix and has an inverse.\n\n# Create A and known X\nA = matrix(c(1,1,\n             5,2), ncol = 2)\nX = matrix(c(2,3), ncol = 1)\n\n# Dot product to calculate B\nB = A %*% X\n\n# Suppose you have A and B, but want to find X\nX_solve = solve(A, B)\n\n# Did it work?\nX; X_solve\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\nWe can see, then, that solve() is internally evaluating \\(A^{-1}\\). Remember that \\(A^{-1}\\) is not trivial to calculate, as it is the matrix that must satisfy: \\(AA^{-1} = I\\), where \\(I\\) is an identity matrix. In fact, solve(A) returns the inverse of \\(A\\), if it exists.\n\ninv_A = solve(A)\n\n#Did it work?\n(inv_A %*% B)\n\n     [,1]\n[1,]    2\n[2,]    3\n\nX\n\n     [,1]\n[1,]    2\n[2,]    3\n\n\n\n4.12.3 Multiple linear regression with a categorical input\nLet’s simulate a case in which we have one categorical input variable that takes on values “low”, “medium”, and “high”, and one continuous input variable.\n\nset.seed(7)\nn=90\nsigma = 0.8\n\n# Xmatrix\n## Intercept\nx0 = rep(1, times = n) \n## Categorical input variable\n### Note that we need to code this as \"0\" \"1\" \"2\" to \n### simulate our outcome variable \"y\"\nx1 = rep(c(0,1,2), each=n/3)\nx1L = factor(x1, labels = c(\"low\", \"med\", \"high\"))\n## Continuous input variable\nx2 = rnorm(n, 0, 2.5)\nxmat = cbind(x0,x1,x2)\nhead(xmat)\n\n     x0 x1        x2\n[1,]  1  0  5.718118\n[2,]  1  0 -2.991929\n[3,]  1  0 -1.735731\n[4,]  1  0 -1.030732\n[5,]  1  0 -2.426683\n[6,]  1  0 -2.368200\n\n# Intercept and 2 slopes\nbetas=c(1.5, 1.2, -1.5)\n\n# Simulate outcome variable, as usual\ny2 = xmat %*% betas + rnorm(n,0,sigma)\n\n# Plot the relationships\npar(mfrow=c(1,2))\nplot(y2~x1)\nplot(y2~x2)\n\n\n\n\n\n\n# Run the model\n## Note that we us the \"factor\" input variable\n## \"x1L\", which has \"levels\"\nm_cat = lm(y2 ~ 1 + x1L + x2)\nsummary(m_cat)\n\n\nCall:\nlm(formula = y2 ~ 1 + x1L + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9182 -0.5032  0.1465  0.5061  1.2139 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.66504    0.13741  12.117  &lt; 2e-16 ***\nx1Lmed       1.15971    0.19113   6.068 3.38e-08 ***\nx1Lhigh      2.29169    0.19170  11.954  &lt; 2e-16 ***\nx2          -1.51229    0.03275 -46.171  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7314 on 86 degrees of freedom\nMultiple R-squared:  0.9674,    Adjusted R-squared:  0.9663 \nF-statistic: 851.1 on 3 and 86 DF,  p-value: &lt; 2.2e-16\n\ncoef(m_cat)\n\n(Intercept)      x1Lmed     x1Lhigh          x2 \n   1.665040    1.159710    2.291690   -1.512293 \n\n\nHow do we interpret the slopes, because we see there is a separate slope for x1Lmed and x1Lhigh? We can understand better by seeing how the linear model addes up. For instance, what is the expected value of the outcome variable when \\(x_1\\) is high, and \\(x_2 = 2.0\\)?\n\n## Using m1_binL:\ny2_pred = \n    1*coef(m_cat)[1] + # Global average (intercept)\n    0*coef(m_cat)[2] + # Not \"med\"\n    1*coef(m_cat)[3] + # Yes \"high\"\n    2.0*coef(m_cat)[4] # x2=2.0 * slope\nas.numeric(y2_pred)\n\n[1] 0.9321446\n\n\nWhen we assigned the slope of the categorical input variable as \\(1.2\\), remember this is the expected change in \\(y\\) as the input variable changes by a value of \\(1.0\\). In the model, we code the \\(x_1\\) variable as taking numerical values \\(0\\), \\(1\\), and \\(2\\) to represent categories, “low”, “medium”, and “high”. So, the slope for x1med is the expected change in \\(y\\) as the input variable changes from “low” to “medium”, an effective change of \\(1.0\\). Then, the slope for x1high is the expected change in \\(y\\) as the input variable changes from “low” to “high”, an effective change of \\(2.0\\); hence, this slope is estimated as \\(2.29\\), with standard error \\(0.19\\). Notice how this slope is approximately twice our “known” slope for the input variable, which was \\(1.2\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "hypothesis.html",
    "href": "hypothesis.html",
    "title": "5  Hypothesis Testing",
    "section": "",
    "text": "5.1 Generate the data\nHere we are going to build on the lecture material by comparing manual calculations of hypothesis tests versus the metrics reported by the lm() function.\nFirst let’s generate data, in the same way we did for multiple linear regression in Chapter 4.\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n\n\n\n# Create a data.frame\nmy_df = data.frame(y, xmat[,2:5])\nhead(my_df)\n\n          y         X1         X2       X3 X4\n1 -4.586806 -1.7268438 13.8180926 61.27634  8\n2 27.662332 16.0748747  6.7393185 58.15099 14\n3 26.466919 -5.0439349  0.8145552 36.82198 16\n4  0.843986  5.5611421 18.1722388 38.24042 10\n5 18.891783 18.6915270 16.7070212 51.91376 15\n6 12.660545  0.1767361 12.9778881 45.11988 16\n\n# Run the model, report the summary\nm1 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\nm1_summary = summary(m1)\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.589838   1.759860   0.903    0.369    \nX1           0.737086   0.035629  20.688   &lt;2e-16 ***\nX2          -1.295274   0.044252 -29.270   &lt;2e-16 ***\nX3          -0.003676   0.028481  -0.129    0.898    \nX4           1.826125   0.088971  20.525   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9497,    Adjusted R-squared:  0.9471 \nF-statistic: 354.3 on 4 and 75 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "max-lik.html",
    "href": "max-lik.html",
    "title": "6  Maximum Likelihood",
    "section": "",
    "text": "6.1 Generate some data\nFirst, let’s generate some data for the case of a simple linear regression.\n### PARAMS\nbeta0 = 1.5\nbeta1 = 0.5\nsigma = 0.4\nn = 30\n\n### GENERATE DATA\nset.seed(5)\nx = runif(n, -1.5, 1.5)\nexp_y = beta0 + beta1*x\ny = exp_y + rnorm(n, mean=0, sd=sigma)\n\n# Create a data frame \nmy_df = data.frame(y = y, x = x)\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "8  ANOVA",
    "section": "",
    "text": "8.1 One-way ANOVA\nRemember that Analysis of Variance (ANOVA) is using linear regression to analyze how discrete input variables affect an outcome. Because the input variables are comprised of discrete “levels” (e.g., treatment groups), we are really comparing how the mean of an outcome varies among discrete groups. This is therefore a generalization of two-sample \\(t\\)-tests to the case of more than two samples/groups.\nOne-way ANOVA is the special case in which we only have one input variable, which has more than two “levels”. The ANOVA linear model can be written in several ways, but for me the easiest way to think about is as follows: \\[y_{i,l} = \\mu_l + \\epsilon_{i,l}\\] \\[\\epsilon \\sim N(0, \\sigma^2 I)\\]\nIn this case \\(l = 1,\\dots,L\\), where \\(L\\) is the number of levels in the input variable \\(x\\), and \\(\\mu_l\\) is the mean outcome of group level, \\(l\\). Thus, each data observation \\(y_{i,l}\\) varies about a group level mean, with residuals \\(\\epsilon_{i,l}\\), which are normally and independently distributed.\nPerhaps this is easier to visualize and see the code. Let’s simulate a case in which we have a single input variable \\(x\\) that has 3 levels.\nset.seed(5)\n\nn_levels = 3\nn_obs_per_level = 25\n\n# Construct X\nx = rep(c(1:n_levels), \n        each = n_obs_per_level)\n\n# Assign group-wise means\nmus = NULL\nmus[1] = 5.0\nmus[2] = 7.5\nmus[3] = 5.8\nsigma = 2.5 # residual sigma\n\n# Simulate data obs\ny = NULL\nfor(i in 1:length(x)){\n    this_level = x[i]\n    y[i] =  \n        mus[this_level] +\n        # residual deviation from group-wise mean:\n        rnorm(1, mean = 0, sd = sigma)\n}\n\n# Store in data frame\none_way_df = data.frame(\n    y = y,\n    x = factor(x) # store as factor/categorical\n)\n\n# Plot the means:\n## Defaults to boxplot() when x is discrete\nplot(y~x, data = one_way_df)\nWe can see that - at least visually - level 2 has a higher mean outcome than the other levels (e.g., \\(\\bar{y}_2\\) seems largest). Let’s run the ANOVA model and see the summary.\n# Use the aov() function\nm_one_way = aov(y ~ 1 + x, data = one_way_df)\n\nsummary(m_one_way)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx            2  125.5   62.74   10.32 0.000115 ***\nResiduals   72  437.9    6.08                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "bayesian.html",
    "href": "bayesian.html",
    "title": "9  Bayesian Inference",
    "section": "",
    "text": "9.1 Background\nAs a reminder, suppose we are estimating a single parameter in a model, \\(\\theta\\). In Bayesian inference, we have:\n\\[P(\\theta | \\text{Data}) \\propto P(\\theta)P(\\text{Data}|\\theta)\\] In words, this means that the posterior probability distribution of the parameter, \\(P(\\theta | \\text{Data})\\), is proportional to the prior probability distribution of the parameter, \\(P(\\theta)\\), multiplied by the likelihood of the data, given the parameter, \\(P(\\text{Data}|\\theta)\\).\nThe prior probability distribution of the parameter quantifies what we believe the parameter’s true value may be, prior to collecting data. Remember that this prior probability distribution can be “vague,” meaning that we don’t have high confidence in what the parameter value is prior to collecting data. Or, the prior can be “informative,” meaning that we have some level of certainty in what values are most likely for the parameter.\nThe likelihood is the same quantity that we discussed in the sections on Maximum Likelihood. The data likelihood represents how well a model matches the data, given a particular parameter value.\nFinally, the posterior probability distribution represents a type of weighted likelihood - the likelihood weighted by our prior knowledge of what the parameter value might be.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "syllabus.html#bblearn-collaborate-ultra-recorded-lectures",
    "href": "syllabus.html#bblearn-collaborate-ultra-recorded-lectures",
    "title": "Appendix A: Syllabus",
    "section": "\nA.1 BbLearn, Collaborate Ultra & Recorded Lectures",
    "text": "A.1 BbLearn, Collaborate Ultra & Recorded Lectures\nWe will use the learning management system, BbLearn, to conduct some course business, including assignment disbursement and submitting. We will use BbLearn’s Collaborate Ultra to record lectures for future viewing."
  },
  {
    "objectID": "syllabus.html#inf511-book-website",
    "href": "syllabus.html#inf511-book-website",
    "title": "Appendix A: Syllabus",
    "section": "\nA.2 INF511 Book Website",
    "text": "A.2 INF511 Book Website\nI have compiled a course website that has supplemental text and coded examples that we will walk through in class. This website essentially serves as the course textbook and is required reading. There will be other required reading materials (see Section A.8).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-purpose",
    "href": "syllabus.html#course-purpose",
    "title": "Appendix A: Syllabus",
    "section": "\nA.3 Course Purpose",
    "text": "A.3 Course Purpose\nINF 511 Modern Regression I is the first course in a two-semester sequence required for the MS and PhD in Informatics and Computing (INF). (See INF 512 Modern Regression II.) These courses are designed to serve the computationally oriented statistical analysis needs of the INF graduate program. Through a series of hands-on individual or team-based assignments, students will master statistical analyses, from preparing data, exploring data using numerical and/or graphical methods, modeling data, diagnosing model assumptions, remodeling and final inference. This course will provide INF graduate students with the necessary foundation for more specialized statistical methods and applications that students will encounter in subsequent INF courses, such as INF 626 Applied Bayesian Modeling and the more prediction-oriented INF 504 Data Mining and Machine Learning. More generally, INF 511 provides skills widely applicable to analysis of data across science and engineering.\nINF 511 Modern Regression I covers fundamental probability models and their use in the analysis of independent data with linear models within both frequentist and Bayesian statistical frameworks. Random variables, expectation, variance, covariance, correlation. Joint, conditional and marginal distributions. Linear combinations of random variables; central limit theorem; matrices, vectors, basic matrix arithmetic, matrix formulation of linear statistical models (regression and ANOVA) for independent data, normal likelihood, least squares, Gauss-Markov theorem, \\(t\\) and \\(F\\) sampling distribution-based inference for linear combinations of parameters. Corresponding Bayesian analysis including prior and posterior distributions, introductory Markov chain Monte Carlo methods. Diagnostics, including graphical residual analysis. Scope of inference (randomization and causality, random sampling and population).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-student-learning-outcomes",
    "href": "syllabus.html#course-student-learning-outcomes",
    "title": "Appendix A: Syllabus",
    "section": "\nA.4 Course Student Learning Outcomes",
    "text": "A.4 Course Student Learning Outcomes\nThe overall learning outcome for this course is a demonstrated acquisition of skills and conceptual understanding of statistical methods to enable complete and valid statistical analyses of primarily independent data modeled with relatively traditional linear statistical models from both a frequentist and Bayesian perspective at a level commensurate with high expectations of a well-trained graduate student in the quantitatively and computationally intensive field of informatics. For these data, and using the methods and concepts detailed in the Course Purpose, students should be able to:\n\nUse numerical and graphical exploratory data analysis tools to prepare data and to develop conceptual mod- els of data\nTransform conceptual models of data into formal linear statistical models of data\nImplement linear model methods in modern software, such as R or Stan, to analyze data\nDemonstrate an understanding that models are an abstracted simplification of real processes by effectively using linear model diagnostics, remedial measures, remodeling and final model validation/confirmation methods.\nDemonstrate an understanding of the limitations of data and methods by communicating how the data and methods relate to randomization and causality, random selection and population, over-fitting and exploratory/confirmatory analyses, and the trade-off between efficiency of inference and robustness to departures from method assumptions\nDemonstrate an ability to work effectively in a team environment to solve realistic problems, which may be beyond any one individual!s ability to address, as indicated by peer review or other assessments of team- work.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#sec-assess",
    "href": "syllabus.html#sec-assess",
    "title": "Appendix A: Syllabus",
    "section": "\nA.5 Assessments of Course Student Learning Outcomes",
    "text": "A.5 Assessments of Course Student Learning Outcomes\nThere will be three assessment strategies: problem sets, homework, and quizzes. Problem sets will have dedicated in-class time to complete, whereas homework assignments will be done entirely outside of class. Problem sets will be primarily assessed as complete/incomplete, whereas Homework assignments will be graded in full. Assignment format is designed in part to mimic and reinforce the similar presentation of analyses in class/notes and to encourage discussion among students. There will be $$5 in-class Quizzes, one every $$3 weeks, in lieu of exams. Attendance & Participation will also be used to assess your course performance.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#grading-system",
    "href": "syllabus.html#grading-system",
    "title": "Appendix A: Syllabus",
    "section": "\nA.6 Grading System",
    "text": "A.6 Grading System\n\n\nProblem Sets\nHomework\nQuizzes\nAttendance\n\n\n10%\n50%\n35%\n5%\n\n\n\n\nAttendance & Participation. In-person attendance is required. See University Policy on excused absences. You are responsible to plan with your fellow classmates to obtain in-class material not received due to your absence. (Recall that lectures will be recorded and will be available in Canvas) Participation in the form of responding to questions in class, asking questions, and attending office hours may be used to determine “borderline” grade cases.\n\n\n\n\n\n\n\nPrior notification of absence\n\n\n\nA student must notify the instructor prior to absence. Students should notify the instructor of an upcoming absence via email, and the instructor will evaluate whether the absence will be counted as excused or unexcused.\n\n\n\n\nAssignments. There are two categories of assignments, Problem Sets and Homework. See Section A.5 for the distinctions. See Section A.9 for due-dates. Assignments will be posted periodically via Canvas. Assignments are to be submitted electronically, via Canvas, by the due date/time indicated in Canvas.\n\n\n\n\n\n\n\nLate assignments\n\n\n\nLate assignments will not be accepted. Late assignments will receive a score of zero.\n\n\n\n\nQuizzes. There will be approximately five in-class quizzes that are all cumulative. Each quiz will be designed to take approximately 20-30 minutes, and each will be scaled to 100 points so that they are equally weighted.\n\n\n\n\n\n\n\nIn-class quizzes\n\n\n\nIf a student does not attend a class when a quiz is given, that quiz will receive zero points. The only exception is if a student notifies the instructor, before class, of an impending absence. The absence must be formally excused in writing by the instructor before class for a make-up quiz to be considered. Therefore, the notification of absence must be received at least several hours prior to class time.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-grades",
    "href": "syllabus.html#course-grades",
    "title": "Appendix A: Syllabus",
    "section": "\nA.7 Course Grades",
    "text": "A.7 Course Grades\nOverall course grades will follow a typical scale:\n\n\nTo earn the letter grade -&gt;\nA\nB\nC\nD\nF\n\n\nYou need at least this score\n90\n80\n70\n60\n0\n\n\nWhile you should be able to compute an estimate of your current grade using the information above, I will attempt to use the Grade feature in Canvas so that you are able to check your grades. Grading mistakes may occur, and students are encouraged to discuss such concerns with the instructor during office hours or by appointment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#sec-readings",
    "href": "syllabus.html#sec-readings",
    "title": "Appendix A: Syllabus",
    "section": "\nA.8 Readings and Materials",
    "text": "A.8 Readings and Materials\n\nLecture Materials: Lecture topics, course notes, readings, and assignments will be made available as the semester progresses. For each lecture, a document will be posted on Canvas. During lecture, we will work together to fill out this document with written notes presented on the (virtual) whiteboard. Therefore, it is essential to download and print these materials prior to attending class.\n\nRequired Text:\n\nBarber, J.J. (2022). INF511 Modern Regression I - Lecture Slides. Posted on Canvas. Referred to as JB in Section A.9.\nMihaljevic, J (2023). INF511 Modern Regression I - Online Book. https://joseph-mihaljevic.github.io/inf511-book/. Referred to as JM in Section A.9.\n\n\n\nSuggested Text:\n\nFaraway, J.J. (2014). Linear Models with R (2nd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/b17144 Referred to as FAR in Section A.9.\n\n\n\n\nComputing. Each student must bring their laptop to class with the following (freely available) software pre-installed:\n\nLatest version of RStudio Desktop IDE\n\nCompatible version of R software environment\n\n\nQuarto publishing system (for documents with integrated code).\nYou must have a functional PDF Engine to render Quarto (.qmd) documents into PDF. See this section on PDF Engines, and be sure to test whether you can render an example .qmd file into a PDF.\nStan programming language, via the rstan package for R.\nWe will potentially use the R package rstanarm, but this is a straightforward package to download using the install.packages() function.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#sec-schedule",
    "href": "syllabus.html#sec-schedule",
    "title": "Appendix A: Syllabus",
    "section": "\nA.9 Living course schedule",
    "text": "A.9 Living course schedule\nThis schedule will be consistently updated throughout the course. Check back often.\n\n\n\n\nWeek\nDate\nTopic\nReading_Due\nAssign_Due\nQuiz\n\n\n\nWeek 1\n16-Jan\nIntroduction\nSyllabus, JM(Ch1&2), JB(1-20, AppA)\n\n\n\n\nWeek 1\n18-Jan\nProbability distributions\nJB(pg1-20), JM(Ch3), FAR(Ch1)\nPS-0\n\n\n\nWeek 2\n23-Jan\nProbability distributions\n\n\n\n\n\nWeek 2\n25-Jan\nLeast Squares\nJB(pg20-38, AppB), JM(Ch4)\nPS-1, HW-1\n\n\n\nWeek 3\n30-Jan\nLeast Squares\nFAR(Ch2)\n\nQuiz 1\n\n\nWeek 3\n1-Feb\nLeast Squares\nJB(pg41-60)\nPS-2\n\n\n\nWeek 4\n6-Feb\nLeast Squares\n\nHW-2\n\n\n\nWeek 4\n8-Feb\nSnow Day - NO CLASS\n\n\n\n\n\nWeek 5\n13-Feb\nLeast Squares\nJB(pg157-170), FAR(Ch3)\n\n\n\n\nWeek 5\n15-Feb\nHypothesis Testing\nJB(pg61-79), JM(Ch5), FAR(Ch10)\nPS-3\n\n\n\nWeek 6\n20-Feb\nHypothesis Testing\nJB(pg80-127), FAR(Ch4)\n\n\n\n\nWeek 6\n22-Feb\nHypothesis Testing\n\nHW-3\nQuiz 2\n\n\nWeek 7\n27-Feb\nMaximum Likelihood\nJB(pg38-40), JM(Ch6)\n\n\n\n\nWeek 7\n29-Feb\nMaximum Likelihood\nFAR(Ch6), JB(skimCh6)\n\n\n\n\nWeek 8\n5-Mar\nMaximum Likelihood\nFAR(Ch6), JB(skimCh6)\nPS-4\n\n\n\nWeek 8\n7-Mar\nMaximum Likelihood\nFAR(Ch7)\n\n\n\n\nWeek 9\n12-Mar\nSpring Break - NO CLASS\n\n\n\n\n\nWeek 9\n14-Mar\nSpring Break - NO CLASS\n\n\n\n\n\nWeek 10\n19-Mar\nMaximum Likelihood\nFAR(Ch8)\nHW-4\nQuiz 3\n\n\nWeek 10\n21-Mar\nCLASS CANCELLED\n\n\n\n\n\nWeek 11\n26-Mar\nModel Comparison\nJB(refreshCh3.2), FAR(Ch10, Ch 11)\n\n\n\n\nWeek 11\n28-Mar\nModel Comparison\nJB(pg170-223)\nPS-5\n\n\n\nWeek 12\n2-Apr\nANOVA\nJB(Ch14), JM(Ch7), FAR(Ch14)\n\n\n\n\nWeek 12\n4-Apr\nANOVA\nJB(Ch15.1-15.3)\nHW-5\n\n\n\nWeek 13\n9-Apr\nANOVA\nJB(Ch15.4-15.5), FAR(Ch15)\n\nQuiz 4\n\n\nWeek 13\n11-Apr\nANOVA\n\nPS-6\n\n\n\nWeek 14\n16-Apr\nANOVA\nFAR(Ch16)\n\n\n\n\nWeek 14\n18-Apr\nBayesian Inference\n\nHW-6\n\n\n\nWeek 15\n23-Apr\nBayesian Inference\n\n\n\n\n\nWeek 15\n25-Apr\nBayesian Inference\n\nPS-7\n\n\n\nWeek 16\n30-Apr\nBayesian Inference\n\n\n\n\n\nWeek 16\n2-May\nBayesian Inference\n\nHW-7\nQuiz 5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Appendix A: Syllabus",
    "section": "\nA.10 Course Policies",
    "text": "A.10 Course Policies\n\nStudents are encouraged to attend the office hours of the instructor. If a student cannot attend regular office hours with the instructor, an appointment may be considered if made via email with sufficient advanced notice.\nEmails addressed to the instructor must be respectful and professional. The instructor will respond to emails promptly, within 2 business days. The instructor will generally not respond to emails on weekends or after working hours (i.e., in the evenings), so please plan accordingly.\nCheating, including plagiarism of writing or computer code, will not be tolerated. All academic integrity violations are treated seriously. Academic integrity violations will result in penalties including, but not limited to, a zero on the assignment, a failing grade in the class, or expulsion from NAU. The University’s Academic Integrity policies (Section A.11) will be strictly enforced.\nThe paramount policy of this course is that each student is required to demonstrate respect towards their peers and the instructor. The behavior of the instructor is held to the same standard. Students and instructors come from all walks of life, and may identify with a variety of ethnic, racial, religious, gender and sexual identities. Diversity of thought and perspective enhances our science.\nAttendance is required and repeated, unexcused absences may affect the student’s grade.\nThe instructor will not provide copies of course notes. These materials should be sought from the students’ peers or by watching the recorded lectures.\nElectronic device usage must support learning in the class. All cell phones, PDAs, music players and other entertainment devices must be turned off (or put on silent) during lecture.\nGrades will be entered in Canvas. Please check LOUIE for your final grade.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#sec-univ-policy",
    "href": "syllabus.html#sec-univ-policy",
    "title": "Appendix A: Syllabus",
    "section": "\nA.11 University Policies",
    "text": "A.11 University Policies\nPlease see this document for all of the required Syllabus Policy Statements that equally apply to this course.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "ols.html#multiple-linear-regression",
    "href": "ols.html#multiple-linear-regression",
    "title": "4  Ordinary Least Squares",
    "section": "\n4.11 Multiple Linear Regression",
    "text": "4.11 Multiple Linear Regression\nSo far, we have only discussed a single input variable in our model, which is a simple linear regression. When we have multiple input variables, we are dealing with multiple linear regression analysis, so the model looks like: \\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dots + \\beta_{p-1} x_{p-1,i} + \\epsilon_i\\] where \\(p\\) is the number of model coefficients and \\(p-1\\) is the number of input variables. Still, in matrix notation the model is \\(Y = XB + \\epsilon\\), so the least squares regression analysis approach still works. However, our interpretation of the model coefficients becomes a bit more challenging.\nLet’s look at a data set within the faraway package.\n\nlibrary(faraway)\ndata(gala)\n\n# Plot the raw data relationships\npar(mfrow=c(1,3))\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Area, xlab = \"Area\", ylab = \"Species\", pch = 19)\nplot(gala$Species ~ gala$Adjacent, xlab = \"Adjacent\", ylab = \"Species\", pch = 19)\n\n\n\n\n\n\npar(mfrow=c(1,1))\n\n# Conduct multiple and single linear regressionm, focusing on Elevation\nm1 = lm(Species ~ Elevation + Area + Adjacent, data = gala)\nm2 = lm(Species ~ Elevation, data = gala)\ncoef(m1); coef(m2)\n\n(Intercept)   Elevation        Area    Adjacent \n-5.71892681  0.31498110 -0.02031217 -0.07527974 \n\n\n(Intercept)   Elevation \n 11.3351132   0.2007922 \n\n\n\nplot(gala$Species ~ gala$Elevation, xlab = \"Elevation\", ylab = \"Species\", pch = 19)\nabline(coef=coef(m1)[1:2])\nabline(coef=coef(m2)[1:2], lty = 2)\n\n\n\n\n\n\n\nWhat we see above is how the addition of Area and Adjacent input variables into the model “adjusts” the effect of Elevation, leading to two unique estimates of the slope (i.e., effect) of Elevation. Let’s probe multiple linear regression more closely by using simulated data.\nFirst, let’s simulate a model with 80 data points that correspond to observations of 4 input variables and one outcome variable. Note that in Footnotes 4.12.3, we show a case with a categorical/binary input variable.\n\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    hist(xmat[,i], main = paste(\"covariate \", i-1))\n}\n\n\n\n\n\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\npar(mfrow=c(1,1))\nhist(y)\n\n\n\n\n\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n\n\n\n\nHow do we figure out the expected value of \\(y\\) for a particular situation? Here’s an example. What is the expected value of \\(y\\) when \\(x_2 = 0.5\\), but the rest of the input variables are at their average values?\n\n# Written out long-ways:\npred_y = \n    betas[1]*1 + \n    betas[2]*mean(xmat[,2]) + \n    betas[3]*0.5 + \n    betas[4]*mean(xmat[,4]) + \n    betas[5]*mean(xmat[,5]) \npred_y\n\n[1] 22.6006\n\n\nNow let’s use ordinary least squares regression to estimate our model coefficients from the data, and then compare these to our “known” values of the model parameters.\n\n# Run the model:\nm1 = lm(y ~ 0 + xmat)\n# Note that the following two models give the same results\n#m2 = lm(y ~ 0 + X1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\n#m3 = lm(y ~ 1 + X2 + X3 + X4 + X5, data = data.frame(xmat))\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 0 + xmat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \nxmat1  1.589838   1.759860   0.903    0.369    \nxmat2  0.737086   0.035629  20.688   &lt;2e-16 ***\nxmat3 -1.295274   0.044252 -29.270   &lt;2e-16 ***\nxmat4 -0.003676   0.028481  -0.129    0.898    \nxmat5  1.826125   0.088971  20.525   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9763,    Adjusted R-squared:  0.9747 \nF-statistic: 616.7 on 5 and 75 DF,  p-value: &lt; 2.2e-16\n\n#summary(m2)\n#summary(m3)\n\n# Compare known `betas` to estimated coefficients\ncbind(betas, coef(m1)) \n\n      betas             \nxmat1  1.00  1.589837527\nxmat2  0.75  0.737085861\nxmat3 -1.20 -1.295274007\nxmat4  0.00 -0.003676167\nxmat5  1.80  1.826125438\n\n# plot the regression lines with abline\ncoef_m1 = coef(m1)\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    \n    plot(y ~ xmat[,i], pch=19,\n         xlab = paste(\"covariate \", i-1),\n         ylab = \"y\",\n         ylim = range(y))\n    abline(coef=coef_m1[c(1,i)])\n}\n\n\n\n\n\n\n\nWell, those regression lines do not look correct. That is because we are interpretting the slopes and intercepts a little incorrectly and not plotting them in the correct manner.\n\n\n\n\n\n\nHow to plot the output of lm() for multiple linear regression\n\n\n\nWhen we isolate and visualize the relationship between the outcome and a single input variable, what we are really observing is the adjusted relationship, after accounting for the other input variables in the model. To understand the expected value of \\(y\\) for any particular value of the single input variable, we really need to set the other input variables to their mean value. Let’s demonstrate this below with the predict() function.\n\n\nLet’s determine the expected values of \\(y\\) for input variable 2 (\\(x_2\\)) and plot it.\n\n# Prediction for covariate 2 when all other input vars at mean\nmy_df = data.frame(xmat[,2:5])\nhead(my_df)\n\n          X1         X2       X3 X4\n1 -1.7268438 13.8180926 61.27634  8\n2 16.0748747  6.7393185 58.15099 14\n3 -5.0439349  0.8145552 36.82198 16\n4  5.5611421 18.1722388 38.24042 10\n5 18.6915270 16.7070212 51.91376 15\n6  0.1767361 12.9778881 45.11988 16\n\n# Re-run the model but with just the input variables, \n# and the intercept is implicit\nm2 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\n# Now let's try to predict y across a range of \n# input variable 2,\n# while holding the other input variables at\n# their average values\n\nn_pred = 100\nnew_df = data.frame(\n  X1 = rep(mean(my_df$X1), n_pred),\n  X2 = seq(0, 20, length.out = n_pred),\n  X3 = rep(mean(my_df$X3), n_pred),\n  X4 = rep(mean(my_df$X4), n_pred)\n)\n\ny_pred2 = predict(m2, newdata = new_df)\n\n# Now plot:\npar(mfrow=c(1,1))\nplot(y ~ my_df$X2, pch = 19,\n     xlab = \"covariate 2\", ylab = \"y\")\nlines(y_pred2 ~ new_df$X2)\n\n\n\n\n\n\n\nNow we see that the predict() function shows a more intuitive relationship between input variable \\(x_2\\) and outcome \\(y\\), while accounting for the effects of the three other input variables.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#lecture-material",
    "href": "hypothesis.html#lecture-material",
    "title": "5  Hypothesis Testing",
    "section": "\n5.1 Lecture material",
    "text": "5.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "hypothesis.html#sec-data",
    "href": "hypothesis.html#sec-data",
    "title": "5  Hypothesis Testing",
    "section": "\n5.2 Generate the data",
    "text": "5.2 Generate the data\nHere we are going to build on the lecture material by comparing manual calculations of hypothesis tests versus the metrics reported by the lm() function.\nFirst let’s generate data, in the same way we did for multiple linear regression in (Chapter 4).\n\nn = 80\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.75\nbetas[3] = -1.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n# Create a data.frame\nmy_df = data.frame(y, xmat[,2:5])\nhead(my_df)\n\n          y         X1         X2       X3 X4\n1 -4.586806 -1.7268438 13.8180926 61.27634  8\n2 27.662332 16.0748747  6.7393185 58.15099 14\n3 26.466919 -5.0439349  0.8145552 36.82198 16\n4  0.843986  5.5611421 18.1722388 38.24042 10\n5 18.891783 18.6915270 16.7070212 51.91376 15\n6 12.660545  0.1767361 12.9778881 45.11988 16\n\n# Run the model, report the summary\nm1 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\nm1_summary = summary(m1)\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.589838   1.759860   0.903    0.369    \nX1           0.737086   0.035629  20.688   <2e-16 ***\nX2          -1.295274   0.044252 -29.270   <2e-16 ***\nX3          -0.003676   0.028481  -0.129    0.898    \nX4           1.826125   0.088971  20.525   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9497,    Adjusted R-squared:  0.9471 \nF-statistic: 354.3 on 4 and 75 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "hypothesis.html#tests-using-the-t-distribution",
    "href": "hypothesis.html#tests-using-the-t-distribution",
    "title": "5  Hypothesis Testing",
    "section": "\n5.3 Tests using the \\(t\\)-distribution",
    "text": "5.3 Tests using the \\(t\\)-distribution\nWe typically use the \\(t\\)-distribution to test the following hypothesis for a specific model coefficient (e.g., an intercept, or a slope): \\[\nH_0: \\beta_i = 0 \\\\\nH_A: \\beta_i \\ne 0\n\\] If \\(\\beta_i\\) is a slope, then we are specifically testing if input variable \\(x_i\\) has a significant linear relationship with \\(y\\). Put another way, we are testing whether \\(x_i\\) has a significant linear effect on \\(y\\).\nThe \\(t\\)-statistic is calculated as follows: \\[t_i = \\frac{\\beta_i - \\mu}{SE(\\beta_i)}, \\quad \\text{and}\\] \\[t_i \\sim t(\\nu),\\]\nwhere \\(t(\\nu)\\) is a \\(t\\)-distribution with \\(\\nu=n-p\\) degrees of freedom. Then we find \\(P(t > |t_i|) = 1 - P(t \\le |t_i|)\\), which is our \\(p\\)-value for the test.\nLet’s manually calculate the \\(t_i\\) and the \\(P(t > |t_i|)\\) for input variables \\(x_1\\), which has a significant positive effect on \\(y\\), and for \\(x_3\\), which has no detectable linear effect on \\(y\\).\n\n## Calculate SE(betas) - We did this in the OLS chapter\nest_sigma = summary(m1)$sigma\nxtx_inv = solve(crossprod(xmat))\nvarcov_betas = xtx_inv*est_sigma^2\nse_betas = sqrt(diag(varcov_betas))\n\n# Degrees of freedom\nt_df = n-p\n\n# extract coef and SE\n# for X1 and X3\ncoef_x1 = coef(m1)[2]\nse_beta_x1 = se_betas[2]\n\ncoef_x3 = coef(m1)[4]\nse_beta_x3 = se_betas[4]\n\n# Calculate t_i\nt_x1 = (coef_x1 - 0)/se_beta_x1\nt_x3 = (coef_x3 - 0)/se_beta_x3\n\n\n# Calculate P(t > |t_i|) = 1 - P(t <= |t_i|)\n# abs() calculates absolute value\np_x1 = 1 - pt(abs(t_x1), df = t_df)\np_x3 = 1 - pt(abs(t_x3), df = t_df)\n\n# Create a table\nt_table = cbind(\n    rbind(t_x1, t_x3),\n    rbind(p_x1*2, p_x3*2) #Multiply by 2 for two-tailed test\n)\ncolnames(t_table) = c(\"t value\", \"Pr(>|t|)\")\nrownames(t_table) = c(\"X1\", \"X3\")\nt_table\n\n      t value  Pr(>|t|)\nX1 20.6876284 0.0000000\nX3 -0.1290726 0.8976457\n\n# Compare to summary of lm()\n## We're extracting just the relevant rows and columns\n## from the summary table \nm1_summary$coefficients[c(2,4), 3:4]\n\n      t value     Pr(>|t|)\nX1 20.6876284 1.008681e-32\nX3 -0.1290726 8.976457e-01\n\n\nLet’s plot the \\(t_i\\) on the \\(t\\)-distribution to see if these \\(p\\)-values make sense.\n\ntseq = seq(-21, 21, by = 0.1)\nprob_tseq = dt(tseq, df = t_df)\n\nplot(prob_tseq ~ tseq, type = \"l\",\n     xlab = \"t\", ylab = \"P(t | df = n-p)\")\nabline(v = c(-1,1)*t_x1, lty = 2, col = \"blue\")\nabline(v = c(-1,1)*t_x3, lty = 2, col = \"orange\")\n\n\n\n\nWe can see that the \\(t\\)-statistic for input variable \\(x_3\\) has a very high probability density, suggesting that the chances that the null hypothesis is true (i.e., \\(\\beta_3 = 0\\)) is high. In contrast, the \\(t\\)-statistic for input variable \\(x_1\\) has a very low probability density, suggesting that we have enough evidence to reject the null in support of the alternative hypothesis (i.e., \\(\\beta_1 \\ne 0\\))."
  },
  {
    "objectID": "hypothesis.html#sec-ttest",
    "href": "hypothesis.html#sec-ttest",
    "title": "5  Hypothesis Testing",
    "section": "\n5.3 Tests using the \\(t\\)-distribution",
    "text": "5.3 Tests using the \\(t\\)-distribution\nWe typically use the \\(t\\)-distribution to test the following hypothesis for a specific model coefficient (e.g., an intercept, or a slope): \\[H_0: \\beta_i = 0\\] \\[H_A: \\beta_i \\ne 0\\] If \\(\\beta_i\\) is a slope, then we are specifically testing if input variable \\(x_i\\) has a significant linear relationship with \\(y\\). Put another way, we are testing whether \\(x_i\\) has a significant linear effect on \\(y\\).\nThe \\(t\\)-statistic is calculated as follows: \\[t_i = \\frac{\\beta_i - \\mu}{SE(\\beta_i)}, \\quad \\text{and}\\] \\[t_i \\sim t(\\nu),\\]\nwhere \\(\\mu=0\\) represents our null hypothesis, and \\(t(\\nu)\\) is a \\(t\\)-distribution with \\(\\nu=n-p\\) degrees of freedom. Then we find \\(P(t &gt; |t_i|) = 1 - P(t \\le |t_i|)\\), and we multiple this value by 2 for our two-tailed test, which is then our \\(p\\)-value for the test.\nLet’s manually calculate the \\(t_i\\) and the \\(P(t &gt; |t_i|)\\) for input variables \\(x_1\\), which has a significant positive effect on \\(y\\), and for \\(x_3\\), which has no detectable linear effect on \\(y\\).\n\n## Calculate SE(betas) - We did this in the OLS chapter\nest_sigma = summary(m1)$sigma\nxtx_inv = solve(crossprod(xmat))\nvarcov_betas = xtx_inv*est_sigma^2\nse_betas = sqrt(diag(varcov_betas))\n\n# Degrees of freedom\nt_df = n-p\n\n# extract coef and SE\n# for X1 and X3\ncoef_x1 = coef(m1)[2]\nse_beta_x1 = se_betas[2]\n\ncoef_x3 = coef(m1)[4]\nse_beta_x3 = se_betas[4]\n\n# Calculate t_i\nt_x1 = (coef_x1 - 0)/se_beta_x1\nt_x3 = (coef_x3 - 0)/se_beta_x3\n\n# Calculate P(t &gt; |t_i|) = 1 - P(t &lt;= |t_i|)\n# abs() calculates absolute value\np_x1 = 1 - pt(abs(t_x1), df = t_df)\np_x3 = 1 - pt(abs(t_x3), df = t_df)\n\n# Create a table\nt_table = cbind(\n    rbind(t_x1, t_x3),\n    rbind(p_x1*2, p_x3*2) #Multiply by 2 for two-tailed test\n)\ncolnames(t_table) = c(\"t value\", \"Pr(&gt;|t|)\")\nrownames(t_table) = c(\"X1\", \"X3\")\nt_table\n\n      t value  Pr(&gt;|t|)\nX1 20.6876284 0.0000000\nX3 -0.1290726 0.8976457\n\n# Compare to summary of lm()\n## We're extracting just the relevant rows and columns\n## from the summary table \nm1_summary$coefficients[c(2,4), 3:4]\n\n      t value     Pr(&gt;|t|)\nX1 20.6876284 1.008681e-32\nX3 -0.1290726 8.976457e-01\n\n\nAlthough our manual \\(p\\)-value calculation is zero, what this really means is that the \\(p\\)-value is so low, that it exceeds the significant digits that are allowed in (computer) memory, which is why the lm() summary output reports the notation Pr(&gt;|t|): &lt; 2.2e-16.\nLet’s plot the \\(t_i\\) on the \\(t\\)-distribution to see if these \\(p\\)-values make sense.\n\ntseq = seq(-21, 21, by = 0.1)\nprob_tseq = dt(tseq, df = t_df)\n\nplot(prob_tseq ~ tseq, type = \"l\",\n     xlab = \"t\", ylab = \"P(t | df = n-p)\")\nabline(v = c(-1,1)*t_x1, lty = 2, col = \"blue\")\nabline(v = c(-1,1)*t_x3, lty = 2, col = \"orange\")\n\n\n\n\n\n\n\nWe can see that the \\(t\\)-statistic for input variable \\(x_3\\) has a very high probability density, suggesting that the chances that the null hypothesis is true (i.e., \\(\\beta_3 = 0\\)) is high. In contrast, the \\(t\\)-statistic for input variable \\(x_1\\) has a very low probability density, suggesting that we have enough evidence to reject the null in support of the alternative hypothesis (i.e., \\(\\beta_1 \\ne 0\\)).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#sec-ftest",
    "href": "hypothesis.html#sec-ftest",
    "title": "5  Hypothesis Testing",
    "section": "\n5.4 Tests using the \\(F\\)-distribution",
    "text": "5.4 Tests using the \\(F\\)-distribution\nWe can also test the hypothesis: \\[H_0: \\beta_1 = \\beta_2 = \\dots = \\beta_{p-1} = 0\\] \\[H_A: \\beta_i \\ne 0, \\quad \\text{for at least one } i\\]\nThis test helps us understand if any of the input variables have a significant linear effect on \\(y\\), which at this point might not be the most useful test. However, later we will use a version of this test to determine which linear combinations of input variables will lead to the best explanation of \\(y\\).\nFor the above hypothesis, we calculate the \\(F\\)-statistic as: \\[F_{stat} = \\frac{ \\frac{SSE(\\text{null}) - SSE(\\text{full})}{\\text{df}_{\\text{null}}-\\text{df}_{\\text{full}}} }{ \\frac{SSE(\\text{full})}{\\text{df}_{\\text{full}}} }, \\quad \\text{and}\\] \\[F_{stat} \\sim F(\\text{df}_{\\text{numerator}}, \\text{df}_{\\text{denominator}})\\]\nThe \\(SSE(\\text{null})\\) refers to the sum of squared errors (i.e., residuals) for the null model that takes the form \\(y_i = \\beta_0 + \\epsilon_i\\), such that the \\(E[y_i] = \\beta_0 = \\bar{y}\\). Because this reduced model only has one coefficient, \\(\\beta_0\\), then the \\(\\text{df}_{\\text{null}} = n - 1\\). Note that, following the expression for \\(F_{stat}\\), the \\(\\text{df}_{\\text{numerator}}\\) is equal to \\(\\text{df}_{\\text{null}} - \\text{df}_{\\text{full}}\\), and the \\(\\text{df}_{\\text{denominator}}\\) is equal to \\(\\text{df}_{\\text{full}}\\).\nIn this test, we are essentially trying to understand if the full model, which includes all of the input variables in the model, does a better job at explaining the outcome variable compared to a null model that simply explains the data by saying that we should expect to see \\(y\\) values that are most often close to the mean of \\(y\\), which equals \\(\\bar{y}\\).\nLet’s manually calculate the the \\(F_{stat}\\) for the above multiple linear regression model, and then calculate the \\(p\\)-value from the associated \\(F\\)-distribution. First, we need to estimate a null model, which only has an intercept, which again should be estimated as \\(\\bar{y}\\).\n\nm_null = lm(y~1)\ncoef(m_null)\n\n(Intercept) \n   10.89976 \n\nmean(y)\n\n[1] 10.89976\n\n\nNow we can extract all of the information we need from the respective lm() output.\n\n# Extract the residuals (errors)\nresid_null = m_null$residuals\nresid_full = m1$residuals\n\n# Sum of Square Errors (SSE)\nsse_null = crossprod(resid_null)\nsse_full = crossprod(resid_full)\n\n# degrees of freedom\ndf_null = n-1\ndf_full = n-p\n\n# Calculate F_stat\nf_stat = ((sse_null - sse_full)/(df_null - df_full)) / (sse_full/df_full)\nf_stat\n\n         [,1]\n[1,] 354.3369\n\n# Degrees of freedom for the F distribution:\ndf_numerator = df_null - df_full\ndf_denominator = df_full\ndf_numerator\n\n[1] 4\n\ndf_denominator\n\n[1] 75\n\n# Compare this to the lm() output:\nm1_summary$fstatistic\n\n   value    numdf    dendf \n354.3369   4.0000  75.0000 \n\n# Visualize the associated F distribution\nfseq = seq(0, 10, by = 0.1)\np_fseq = df(fseq,\n            df1 = df_numerator,\n            df2 = df_denominator)\nplot(p_fseq ~ fseq, type = \"l\",\n     xlab = \"F\", ylab = \"P(F | df1, df2)\")\n\n\n\n\n\n\n\nNotice how the \\(F_{stat} &gt; 350\\), which is far outside the range of our figure above, meaning that it has a very low probability density. We can formally calculate the \\(p\\)-value below:\n\n#P(F &gt; f_stat) = 1 - P(F &lt;= f_stat)\np_f_m1 = 1 - pf(f_stat,\n                df1 = df_numerator,\n                df2 = df_denominator)\np_f_m1\n\n     [,1]\n[1,]    0\n\n\nAgain, although this \\(p\\)-value is showing zero, what this really means is that the \\(p\\)-value is so low, that it exceeds the significant digits that are allowed in (computer) memory, which is why the lm() output reports the p-value: &lt; 2.2e-16.\n\n# Reminder:\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4388 -1.4712  0.2816  1.5305  5.0032 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.589838   1.759860   0.903    0.369    \nX1           0.737086   0.035629  20.688   &lt;2e-16 ***\nX2          -1.295274   0.044252 -29.270   &lt;2e-16 ***\nX3          -0.003676   0.028481  -0.129    0.898    \nX4           1.826125   0.088971  20.525   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.388 on 75 degrees of freedom\nMultiple R-squared:  0.9497,    Adjusted R-squared:  0.9471 \nF-statistic: 354.3 on 4 and 75 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#sec-r2",
    "href": "hypothesis.html#sec-r2",
    "title": "5  Hypothesis Testing",
    "section": "\n5.5 Goodness of fit with \\(R^2\\)\n",
    "text": "5.5 Goodness of fit with \\(R^2\\)\n\nThe summary of the lm() output also reports the Multiple R-squared, which is also referred to as the coefficient of determination, or simply \\(R^2\\) of the model. \\(R^2\\) provides a metric of “goodness of fit” for the model. This value roughly equates to the fraction of the variability in outcome variable \\(y\\) that is explained by the linear combination of input variables that are included in the model.\n\\[ R^2 = 1 - \\frac{SSE(\\text{full})}{SSE(\\text{null})}\\] Thus, the \\(R^2\\) is approximately calculating the reduction in residual error that occurs when you add meaningful input variables, compared to the null model that only has an intercept (i.e., predicting \\(y\\) based on its mean value).\n\nr2 = 1 - sse_full/sse_null\nr2\n\n          [,1]\n[1,] 0.9497436\n\n# Compared to the lm() output\nm1_summary$r.squared\n\n[1] 0.9497436\n\n\nWhat we see here is that the inclusion of the four input variables in the model explains approximately 95% of the variability in outcome variable \\(y\\). This makes sense, because we simulated the data \\(y\\) from a known set of input variables with a known set of coefficients.\nWhat happens to this \\(R^2\\) value if we remove a meaningful input variable from the model? In other words, what if we didn’t actually know all of the correct input variables to measure in real life that explain \\(y\\), and we didn’t measure an important one?\n\n# Run a model that does not include input variable X2\nm2 = lm(y ~ 1 + X1 + X3 + X4, data = my_df)\nm2_summary = summary(m2)\n# Extract the R^2\nm2_summary$r.squared\n\n[1] 0.3756416\n\n\nThe \\(R^2\\) has reduced from 95% to 38%, meaning that without the inclusion of input variable \\(x_2\\) in our model, we reduce our ability to explain (i.e., predict) the outcome \\(y\\) by approximately 60%.\n\n5.5.1 Adjusted \\(R^2\\)\n\nAnother measure in the summary of lm() output is an “adjusted” value of \\(R^2\\), which penalizes the value of \\(R^2\\) for models that have a lot of parameters (\\(p\\)) compared to the number of data points (\\(n\\)).\n\\[\\text{Adjusted } R^2 = 1 - \\frac{n-1}{n-p} + \\frac{n-1}{n-p} R^2\\]\n\nadjusted_r2 = 1 - (n-1)/(n-p) + (n-1)/(n-p)*r2\nadjusted_r2\n\n          [,1]\n[1,] 0.9470633\n\n# Compared to lm() output\nm1_summary$adj.r.squared\n\n[1] 0.9470633\n\n\nIf you have more parameters (i.e., more input variables) relative to the number of observed data points, then the Adjusted \\(R^2\\) will be less than \\(R^2\\). In the model above, we have many data points (\\(n=80\\)) relative to the number of input variables (\\(p-1 = 4\\)), so there is only a small difference in the two metrics.\n\n\n\n\n\n\nCongratulations!\n\n\n\nLook at the summary() of the lm() output. You should now be able to explain (and manually calculate) every value that is printed in that output. You now have a deep understanding of the ordinary least squares (OLS) regression analysis and associated methods of hypothesis-testing.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "max-lik.html#lecture-material",
    "href": "max-lik.html#lecture-material",
    "title": "6  Maximum Likelihood",
    "section": "\n6.1 Lecture material",
    "text": "6.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "max-lik.html#sec-data",
    "href": "max-lik.html#sec-data",
    "title": "6  Maximum Likelihood",
    "section": "\n6.2 Generate some data",
    "text": "6.2 Generate some data\nFirst, let’s generate some data for the case of a simple linear regression.\n\n### PARAMS\nbeta0 = 1.5\nbeta1 = 0.5\nsigma = 0.4\nn = 30\n\n### GENERATE DATA\nset.seed(5)\nx = runif(n, -1.5, 1.5)\nexp_y = beta0 + beta1*x\ny = exp_y + rnorm(n, mean=0, sd=sigma)\n\n# Create a data frame \nmy_df = data.frame(y = y, x = x)\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")"
  },
  {
    "objectID": "max-lik.html#calculate-a-likelihood",
    "href": "max-lik.html#calculate-a-likelihood",
    "title": "6  Maximum Likelihood",
    "section": "\n6.2 Calculate a likelihood",
    "text": "6.2 Calculate a likelihood\nRemember that, for simple linear regression, the likelihood of a single data point is as follows: \\[y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\] \\[P(y_i | X_i B, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{e}^{-\\frac{1}{2}\\frac{(y_i - X_i B)^2}{\\sigma^2}}\\]\nThen, the full likelihood of the data set \\(Y\\) is computed as:\n\\[ P(Y | X B, \\sigma^2) = \\prod^n_{i=1} P(y_i | X_i B, \\sigma^2)\\] Or, on the natural logarithmic scale: \\[ ln\\left(P(Y | X B, \\sigma^2)\\right) = \\sum^n_{i=1} ln\\left(P(y_i | X_i B, \\sigma^2)\\right)\\]\n\n# How to calculate the likelihood of a single data point:\ndnorm(y[1], \n      mean = beta0 + beta1*x[1],\n      sd = sigma,\n      log = FALSE)\n\n[1] 0.987769\n\n# Calculate the full likelihood of the data, using the product\n## Vectorized:\nLH_notlog= \nprod(\n    dnorm(y, \n          mean = beta0 + beta1*x,\n          sd = sigma,\n          log = FALSE)\n)\n\n# Log-likelihood, vectorized\nLH_log = \nsum(\n    dnorm(y, \n          mean = beta0 + beta1*x,\n          sd = sigma,\n          log = TRUE)\n)\n\n# Make sure the output makes sense\nLH_notlog\n\n[1] 8.496937e-10\n\n# Indeed the log-likelihood is the log of the \n# likelihood on the raw probability scale.\nLH_log; log(LH_notlog)\n\n[1] -20.88615\n\n\n[1] -20.88615",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "max-lik.html#using-optim-to-minimize-the-negative-log-likelihood",
    "href": "max-lik.html#using-optim-to-minimize-the-negative-log-likelihood",
    "title": "6  Maximum Likelihood",
    "section": "\n6.3 Using optim() to minimize the negative log-likelihood",
    "text": "6.3 Using optim() to minimize the negative log-likelihood\nAs we discussed in lecture, it is more computationally convenient to minimize functions, rather than to maximize. Therefore, to conduct linear regression analysis with maximum likelihood methods, we will find the values of \\(\\hat{B}\\) that minimize the negative log-likelihood of the data: \\(-ln\\left(P(Y | X B, \\sigma^2)\\right)\\).\n\n6.3.1 Function to calculate the negative log-likelihood\nFirst, we need to construct a function that calculates the negative log-likelihood and that specifies the parameters of the model that eventually need to be estimated by the optim() function.\n\n### NEG LOG-LIK MINIMIZATION\nneg_log_lik = function(p, data_df){\n    beta0=p[1]\n    beta1=p[2]\n    sigma=p[3]\n    \n    mu = beta0 + beta1*data_df$x\n    \n    nll = -sum(dnorm(data_df$y, mean=mu, sd=sigma, log = TRUE))\n    return(nll)\n}\n\nHere you can see the inputs to the function: p is a vector of parameters to be estimated (i.e., optimized), and data_df is a data.frame that holds the values of outcome variable \\(y\\) and associated input variables, in this case, just one \\(x\\).\nThen, we can use the optim() function, which implements a gradient descent algorithm to estimate the values of the parameters that minimize the provided function, neg_log_lik(). We will learn more about gradient descent later, because this is a very important method used widely across machine learning and neural networks.\n\nm_nll = \n    optim(\n        par = c(0.1,0,0.1),\n        fn = neg_log_lik,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,-5,0.001),\n        upper=c(5,5,5),\n        hessian = TRUE\n    )\npar_tab_nll = rbind(m_nll$par)\ncolnames(par_tab_nll) = c(\"int\", \"slope\", \"sigma\")\npar_tab_nll\n\n          int   slope     sigma\n[1,] 1.598468 0.56205 0.4571571\n\n\nNote the optim() options. par specifies the initial guesses of the three parameters, whereas lower and upper specify the bounds across which to search for the best values of the parameters. With hessian = TRUE we are asking the function to output an estimate of the Hessian matrix of the function, which helps us to estimate the standard errors of the parameter estimates (see Footnotes 6.6.1). The method specifies the algorithm used to minimize the function, which in this case is a modified quasi-Newton method, L-BFGS-B, which is a type of gradient descent algorithm, to be discussed later.\nWe can see that the function outputs three point-estimates, which are \\(\\hat{B}\\) (i.e., slope and intercept), as well as the residual standard deviation, \\(\\hat{\\sigma}\\).\n\n6.3.2 Compare optim() results to the OLS output\n\n### COMPARE TO LM()\nm_ols = lm(y ~ 1 + x, data = my_df)\nm_ols_summary = summary(m_ols)\nm_ols_summary # Notice p-value\n\n\nCall:\nlm(formula = y ~ 1 + x, data = my_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04966 -0.37035  0.06069  0.37520  0.72646 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.59847    0.08643  18.495  &lt; 2e-16 ***\nx            0.56205    0.09864   5.698 4.14e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4732 on 28 degrees of freedom\nMultiple R-squared:  0.537, Adjusted R-squared:  0.5204 \nF-statistic: 32.47 on 1 and 28 DF,  p-value: 4.136e-06\n\npar_tab_ols = c(coef(m_ols), m_ols_summary$sigma)\nnames(par_tab_ols) = c(\"int\", \"slope\", \"sigma\")\npar_tab_ols; par_tab_nll\n\n      int     slope     sigma \n1.5984685 0.5620501 0.4732005 \n\n\n          int   slope     sigma\n[1,] 1.598468 0.56205 0.4571571\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")\n# Line from OLS\nabline(coef = coef(m_ols), col = \"black\", lwd = 2)\n# Line from MaxLikelihood\nabline(coef = m_nll$par[1:2], col = \"red\", lty = 2, lwd = 2)\n\n\n\n\n\n\n\nAs we learned in lecture, the estimates of \\(\\hat{B}\\) from least squares and maximum likelihood are equivalent. And indeed, we see the same estimates produced from lm() and optim(). Also if you look at Residual standard error in the lm() output, you see the equivalent estimate for \\(\\hat{\\sigma}\\) compared to the optim() output.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "max-lik.html#hypothesis-testing-for-maximum-likelihood-approach",
    "href": "max-lik.html#hypothesis-testing-for-maximum-likelihood-approach",
    "title": "6  Maximum Likelihood",
    "section": "\n6.5 Hypothesis-testing for maximum likelihood approach",
    "text": "6.5 Hypothesis-testing for maximum likelihood approach\nAs explained in lecture, we will use the likelihood ratio test to test:\n\\[H_0: \\beta_i = 0\\] \\[H_A: \\beta_i \\ne 0\\] In the least squares framework, we used a \\(t\\)-test. But, for maximum likelihood, we are going to base our test on the likelihood of a model that does or does not include the slope, similar to the \\(F\\)-test we learned before.\nFor the case of simple linear regression, we’re testing whether there is a significant difference between these models: \\[H_0: y_i = \\beta_0 + \\epsilon_i\\] \\[H_A: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Notice that in \\(H_0\\), the slope \\(\\beta_1\\) is assumed to be zero.\n\n6.5.1 Likelihood ratio and the \\(\\chi^2\\) test\nOur goal is to understand if the likelihood of the null model, \\(P(Y | \\beta_0, \\sigma^2)\\), is sufficiently low compared to the likelihood of the full model, \\(P(Y | \\beta_0, \\beta_1, x, \\sigma^2)\\), that we can reliably reject the null hypothesis.\nWe therefore construct a ratio of the likelihoods of the full and null model, very similar to the \\(F\\)-test framework. The log-likelihood ratio (\\(LHR\\)) becomes our test statistic: \\[LHR_{\\text{test}} = -2 ln \\left(\\frac{LH_{\\text{null}}}{LH_{\\text{full}}} \\right)\\]\nThen, folks smarter than I have done the math to prove that this test statistic is equivalent to a \\(\\chi^2\\) test statistic, such that:\n\\[LHR_{\\text{test}} \\sim  \\chi^2_k\\],\nwhere \\(\\chi^2_k\\) is a \\(\\chi^2\\) probability distribution with \\(k\\) degrees of freedom, and \\(k\\) is equal to \\(p_{\\text{full}} - p_{\\text{null}}\\) and \\(p\\) is the number of model coefficients. In the case of simple linear regression, where we are removing just one model coefficient from the full model (i.e., set slope equal to zero), then \\(k = 2-1 = 1\\).\nFinally, we can determine \\(P(\\chi^2 > LHR_{\\text{test}})\\), which gives us our \\(p\\)-value. This statistical test is known as the “likelihood ratio test,” and it is equivalently referred to as the “\\(\\chi^2\\)” test, which we’ll see in the code below.\n\n6.5.2 Manual calculation of the likelihood ratio test\nTo begin, we need to use maximum likelihood to estimate the likelihood of the “null” model. We need to adjust our function that will be used by optim() to only include two parameters: the intercept, and the residual standard deviation.\n\n# Need a null model:\nnll_null = function(p, data_df){\n    beta0=p[1]\n    sigma=p[2]\n    \n    mu = beta0\n    \n    nll = -sum(dnorm(data_df$y, mean=mu, sd=sigma, log = TRUE))\n    return(nll)\n}\n\nm_nll_null = \n    optim(\n        par = c(0.1,0.1),\n        fn = nll_null,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,0.001),\n        upper=c(5,5)\n    )\npar_tab_nll_null = rbind(m_nll_null$par)\ncolnames(par_tab_nll_null) = c(\"int\", \"sigma\")\npar_tab_nll_null\n\n          int     sigma\n[1,] 1.612056 0.6718164\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")\nabline(coef = m_nll$par[1:2], col = \"red\", lty = 1)\nabline(h = m_nll_null$par[1], col = \"red\", lty = 2)\n\n\n\n\nThe flat dashed line represents the null (intercept-only) model. Now, we calculate the likelihood ratio test statistic, and compare to the \\(\\chi^2\\) probability distribution to determine our \\(p\\)-value of the test. Note that within the optim() function’s output list, there is a numeric object called value. This value is the negative log-likelihood of the model with the estimated coefficients. We can use this to calculate our test statistic.\n\n# use exp() to convert the negative log likelihood to \n# raw probability scale\nlog_lh_full = -m_nll$value\nlog_lh_null = -m_nll_null$value\n\nlh_full = exp(log_lh_full)\nlh_null = exp(log_lh_null)\n\n# Now calculate LHR\nlhr = -2 * log(lh_null / lh_full)\nlhr\n\n[1] 23.09763\n\n# Of course, using rules of natural logs, this is equivalent:\n-2 * (log_lh_null - log_lh_full)\n\n[1] 23.09763\n\n\nNow that we have our value of \\(LHR_{\\text{test}}\\), we use the \\(\\chi^2\\)-distribution to find \\(P(\\chi^2 > LHR_{\\text{test}})\\), which is the \\(p\\)-value of the test.\n\n# How many parameters being \"removed\" (i.e., set to zero) in test:\ndf_chi = 2 - 1\n\n# Prob null is true\np_val = 1 - pchisq(lhr, df = df_chi)\np_val\n\n[1] 1.539803e-06\n\n\nBased on this low \\(p\\)-value, we would say there is sufficient evidence to reject the null hypothesis and that the slope \\(\\beta_1\\) is significantly different than zero.\nWe can compare this outcome to a built-in R function called drop1().\n\ndrop1(m_ols, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ny ~ 1 + x\n       Df Sum of Sq     RSS     AIC Pr(>Chi)    \n<none>               6.2697 -42.964             \nx       1    7.2703 13.5401 -21.866 1.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the function, we specified Chisq test, which implements the \\(\\chi^2\\) test using the likelihood ratio. What we see in this summary output is Pr(>Chi) which is equivalent to our manually computed value of \\(P(\\chi^2 > LHR_{\\text{test}})\\). This output from drop1() does not provide a whole lot of detail, but if you look at the help(), it says that if you specify test = \"Chisq\", it conducts a likelihood-ratio test. It doesn’t specifically output the likelihood ratio, but we can see the \\(p\\)-value is equivalent to our manual calculation above."
  },
  {
    "objectID": "max-lik.html#footnotes",
    "href": "max-lik.html#footnotes",
    "title": "6  Maximum Likelihood",
    "section": "\n6.6 Footnotes",
    "text": "6.6 Footnotes\n\n6.6.1 Hessian matrix\nThe optim() function provides point estimates for the maximum likelihood-derived model coefficients. Just like in least squares regression, however, we want to quantify the uncertainty in these estimates. We therefore want the standard error in the model coefficient estimates.\nIn the case of least squares, we showed how we can calculate a variance-covariance matrix for the model coefficients, and then the square-root of the diagonal of this matrix equals the standard error. For maximum likelihood we can estimate this same variance-covariance matrix, but it comes from a different matrix called the Hessian. We do not need to go into detail, but the Hessian is the matrix of second derivatives of the likelihood with respect to the parameters (I will not ask you to recall this information). Then the variance-covariance matrix of the estimated model coefficients is calculated as the inverse of the Hessian matrix that corresponds to the negative log-likelihood. If the Hessian matrix of the negative log-likelihood is \\(H\\), then \\[SE(\\hat{\\beta_i}) = \\sqrt{\\text{diag}\\left( H^{-1}\\right)_i}\\] I understand that’s complicated, but it’s easy enough to extract these values computationally from optim() output, assuming you use the option hessian = TRUE.\n\n# Extract the Hessian from the optim() output\nhessian = m_nll$hessian\n# Calculate the var-cov matrix from the inverse Hessian\n# Remember solve(X) gives X^-1\nparams_varcov = solve(hessian)\n# Then extract the diagonal and take the square root\n# This gives a vector of SE(\\param_i)\nse_params = sqrt(diag(params_varcov))\nparams_tab = cbind(m_nll$par, se_params)\ncolnames(params_tab) = c(\"Estimate\", \"Std. Error\")\nrownames(params_tab) = c(\"Intercept\", \"slope\", \"sigma\")\nparams_tab\n\n           Estimate Std. Error\nIntercept 1.5984685 0.08349686\nslope     0.5620500 0.09529343\nsigma     0.4571571 0.05901782\n\n# Same as OLS? \nm_ols_summary$coefficients[c(1:2), 1:2] # Pretty close!\n\n             Estimate Std. Error\n(Intercept) 1.5984685 0.08642710\nx           0.5620501 0.09863766\n\n\nWe can see that the standard errors for the maximum likelihood estimators are the same as the OLS estimators.\n\n6.6.2 optim() using least squares\nRemember that optim() is not specific to maximum likelihood, but rather it implements one of several optional minimization algorithms. Therefore, we can use it to minimize any quantity. To emphasize this point, remember that in least squares regression, we are finding the values of the model coefficients \\(\\hat{B}\\) that minimize the sum of squared errors, \\(\\sum_i^n \\epsilon_i^2 = \\epsilon^T\\epsilon\\). Let’s minimize this quantity using optim().\n\n### LEAST SQUARES MINIMIZATION\n\n# We need a function to calculate the sum of squared errors:\nleast_sq = function(p, data_df){\n    beta0=p[1]\n    beta1=p[2]\n    y = data_df$y\n    n = length(y)\n    \n    expected_y = beta0 + beta1*data_df$x\n    sse = 0\n    for(i in 1:n){\n        epsilon_i = y[i] - expected_y[i]\n        sse = sse + (epsilon_i)^2\n    }\n    \n    return(sse)\n}\n\n### OPTIMIZE LEAST SQUARES\nfit_least_sq = \n    optim(\n        par = c(0,0),\n        fn = least_sq,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,-5),\n        upper=c(5,5),\n        hessian = TRUE\n    )\n# Create a table of estimates:\npar_tab_least_sq = rbind(fit_least_sq$par)\ncolnames(par_tab_least_sq) = c(\"int\", \"slope\")\npar_tab_least_sq\n\n          int     slope\n[1,] 1.598469 0.5620501\n\n# Compare to original OLS estimates:\ncoef(m_ols)\n\n(Intercept)           x \n  1.5984685   0.5620501 \n\n\nYou could also use the Hessian output to calculate the standard errors of the model coefficients, but I will leave that up to you.\n\n6.6.3 Gradient descent, multiple parameters\nNow let’s suppose we want to estimate all of our model parameters (intercept, slope, residual standard deviation) simultaneously using gradient descent. Recall from lecture that we need to estimate three components of the gradient (the partial derivates of the function with respect to each model parameter).\n\n# How many params to estimate?\nn_param = 3\n\n# Set up some storage arrays:\n## Guess that the number of iterations will be 100 or less...\nparam_guess = array(0, dim=c(100,n_param))\nnll_guess = vector(\"numeric\", length = 100)\n\n# initial guesses\nparam = vector(\"numeric\", length = n_param)\nparam[1] = 0.0 # intercept\nparam[2] = 1.0\nparam[3] = 2.5\n\n# set h for approx of gradient\nh = 1e-4\n\n# set step size\nalpha = 0.005\n\n# Set gradient components to arbitrary high numbers\n## This makes the while() loop work\ngrad = rep(100, times = n_param)\n\n# initialize counter\ni = 1\n\n# While gradient is not yet \\approx zero\nwhile (norm(grad, \"2\") &gt; 10^-4) {\n    \n    # Store the current value of slope:\n    param_guess[i, ] = param\n    \n    #############\n    # Approximate the gradient of the nll\n    #############\n    \n    ## Calculate nll with all current params\n    f_x = neg_log_lik(p = param, \n                          data_df = my_df)\n    ## Store the current nll\n    nll_guess[i] = f_x\n    \n    ## One param at a time, approximate gradient component\n    ## (i.e., partial derivative)\n    for(j in 1:n_param){\n        ## Keep all but one params the same\n        param_adj = NULL\n        param_adj = param\n        param_adj[j] = param[j] + h\n        \n        f_x_adj = neg_log_lik(p = param_adj, \n                              data_df = my_df)\n        ## Calculate gradient component\n        grad[j] = (f_x_adj - f_x) / h\n    }\n    \n    # search direction\n    ## Note that 'direct' is an array of size n_param\n    direct = -grad\n    \n    # update params\n    for(j in 1:n_param){\n        param[j] = param[j] + alpha * direct[j]\n    }\n    \n    # counter for x\n    i = i + 1\n}\n\n# Output the optimal slope and associated nll\nn_iter = i-1\n\ngrad_descent_tab = cbind(rbind(param_guess[n_iter,]), \n                         nll_guess[n_iter])\ncolnames(grad_descent_tab) = \n    c(\"int\", \"slope\", \"sigma\", \"neg_log_lik\")\n\n# Compare to optim() output\noptim_nll_tab = cbind(rbind(m_nll$par), \n                      m_nll$value)\ncolnames(optim_nll_tab) = colnames(grad_descent_tab)\n\nprint(\"Gradient Descent:\");grad_descent_tab\n\n[1] \"Gradient Descent:\"\n\n\n         int     slope     sigma neg_log_lik\n[1,] 1.59842 0.5620025 0.4571052    19.08618\n\nprint(\"optim() output:\");optim_nll_tab\n\n[1] \"optim() output:\"\n\n\n          int   slope     sigma neg_log_lik\n[1,] 1.598468 0.56205 0.4571571    19.08618\n\n\nNow, plot the gradient descent:\n\npar(mfrow=c(1,3))\nplot(nll_guess[1:n_iter]~param_guess[1:n_iter, 1], \n     pch=19, type = \"b\", \n     col = \"blue\",\n     ylab = \"Neg. Log Likelihood\", \n     xlab = expression(\"Intercept, \"~beta[0]))\nplot(nll_guess[1:n_iter]~param_guess[1:n_iter, 2],\n     pch=19, type = \"b\", \n     col = \"orange\",\n     ylab = \"Neg. Log Likelihood\", \n     xlab = expression(\"Slope, \"~beta[1]))\nplot(nll_guess[1:n_iter]~param_guess[1:n_iter, 3], \n     pch=19, type = \"b\", \n     col = \"red\",\n     ylab = \"Neg. Log Likelihood\", \n     xlab = expression(\"Residual Std.Dev., \"~sigma))\n\n\n\n\n\n\n\nWe can see for the intercept, our first guess was too low, so we descended the gradient by addition (i.e., search direction was positive), whereas for the slope and the residual standard deviation, our first guess was too large, so we descended the gradients by subtraction (i.e., search direction was negative).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "max-lik.html#hypothesis-testing-for-maximum-likelihood",
    "href": "max-lik.html#hypothesis-testing-for-maximum-likelihood",
    "title": "6  Maximum Likelihood",
    "section": "\n6.4 Hypothesis-testing for maximum likelihood",
    "text": "6.4 Hypothesis-testing for maximum likelihood\nAs explained in lecture, we will use the likelihood ratio test to test:\n\\[H_0: \\beta_i = 0\\] \\[H_A: \\beta_i \\ne 0\\] In the least squares framework, we used a \\(t\\)-test. But, for maximum likelihood, we are going to base our test on the likelihood of a model that does or does not include the slope, similar to the \\(F\\)-test we learned before.\nFor the case of simple linear regression, we’re testing whether there is a significant difference between these models: \\[H_0: y_i = \\beta_0 + \\epsilon_i\\] \\[H_A: y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Notice that in \\(H_0\\), the slope \\(\\beta_1\\) is assumed to be zero.\n\n6.4.1 Likelihood ratio and the \\(\\chi^2\\) test\nOur goal is to understand if the likelihood of the null model, \\(P(Y | \\beta_0, \\sigma^2)\\), is sufficiently low compared to the likelihood of the full model, \\(P(Y | \\beta_0, \\beta_1, x, \\sigma^2)\\), that we can reliably reject the null hypothesis.\nWe therefore construct a ratio of the likelihoods of the full and null model, very similar to the \\(F\\)-test framework. The log-likelihood ratio (\\(LHR\\)) becomes our test statistic: \\[LHR_{\\text{test}} = -2 ln \\left(\\frac{LH_{\\text{null}}}{LH_{\\text{full}}} \\right)\\]\nThen, folks smarter than I have done the math to prove that this test statistic is equivalent to a \\(\\chi^2\\) test statistic, such that:\n\\[LHR_{\\text{test}} \\sim  \\chi^2_k\\]\nwhere \\(\\chi^2_k\\) is a \\(\\chi^2\\) probability distribution with \\(k\\) degrees of freedom. \\(k\\) is equal to \\(p_{\\text{full}} - p_{\\text{null}}\\), where \\(p\\) is the number of model coefficients. In the case of simple linear regression, where we are removing just one model coefficient from the full model (i.e., set slope equal to zero), then \\(k = 2-1 = 1\\).\nFinally, we can determine \\(P(\\chi^2 &gt; LHR_{\\text{test}})\\), which gives us our \\(p\\)-value. This statistical test is known as the “likelihood ratio test,” and it is equivalently referred to as the “\\(\\chi^2\\)” test, which we’ll see in the code below.\n\n6.4.2 Manual calculation of the likelihood ratio test\nTo begin, we need to use maximum likelihood to estimate the likelihood of the “null” model. We need to adjust our function that will be used by optim() to only include two parameters: the intercept, and the residual standard deviation.\n\n# Need a null model:\nnll_null = function(p, data_df){\n    beta0=p[1]\n    sigma=p[2]\n    \n    mu = beta0\n    \n    nll = -sum(dnorm(data_df$y, mean=mu, sd=sigma, log = TRUE))\n    return(nll)\n}\n\nm_nll_null = \n    optim(\n        par = c(0.1,0.1),\n        fn = nll_null,\n        data_df = my_df,\n        method = \"L-BFGS-B\",\n        lower=c(-5,0.001),\n        upper=c(5,5)\n    )\npar_tab_nll_null = rbind(m_nll_null$par)\ncolnames(par_tab_nll_null) = c(\"int\", \"sigma\")\npar_tab_nll_null\n\n          int     sigma\n[1,] 1.612056 0.6718164\n\nplot(my_df$y ~ my_df$x, pch = 19,\n     xlab = \"x\", ylab = \"y\")\nabline(coef = m_nll$par[1:2], col = \"red\", lty = 1)\nabline(h = m_nll_null$par[1], col = \"red\", lty = 2)\n\n\n\n\n\n\n\nThe flat dashed line represents the null (intercept-only) model. Now, we calculate the likelihood ratio test statistic, and compare to the \\(\\chi^2\\) probability distribution to determine our \\(p\\)-value of the test. Note that within the optim() function’s output list, there is a numeric object called value. This value is the negative log-likelihood of the model with the estimated coefficients. We can use this to calculate our test statistic.\n\n# use exp() to convert the negative log likelihood to \n# raw probability scale\nlog_lh_full = -m_nll$value\nlog_lh_null = -m_nll_null$value\n\nlh_full = exp(log_lh_full)\nlh_null = exp(log_lh_null)\n\n# Now calculate LHR\nlhr = -2 * log(lh_null / lh_full)\nlhr\n\n[1] 23.09763\n\n# Of course, using rules of natural logs, this is equivalent:\n-2 * (log_lh_null - log_lh_full)\n\n[1] 23.09763\n\n\nNow that we have our value of \\(LHR_{\\text{test}}\\), we use the \\(\\chi^2\\)-distribution to find \\(P(\\chi^2 &gt; LHR_{\\text{test}})\\), which is the \\(p\\)-value of the test.\n\n# How many parameters being \"removed\" (i.e., set to zero) in test:\ndf_chi = 2 - 1\n\n# Prob null is true\np_val = 1 - pchisq(lhr, df = df_chi)\np_val\n\n[1] 1.539803e-06\n\n\nBased on this low \\(p\\)-value, we would say there is sufficient evidence to reject the null hypothesis and that the slope \\(\\beta_1\\) is significantly different than zero.\nWe can compare this outcome to a built-in R function called drop1().\n\ndrop1(m_ols, test = \"Chisq\")\n\nSingle term deletions\n\nModel:\ny ~ 1 + x\n       Df Sum of Sq     RSS     AIC Pr(&gt;Chi)    \n&lt;none&gt;               6.2697 -42.964             \nx       1    7.2703 13.5401 -21.866 1.54e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the function, we specified Chisq test, which implements the \\(\\chi^2\\) test using the likelihood ratio. What we see in this summary output is Pr(&gt;Chi) which is equivalent to our manually computed value of \\(P(\\chi^2 &gt; LHR_{\\text{test}})\\). This output from drop1() does not provide a whole lot of detail, but if you look at the help(), it says that if you specify test = \"Chisq\", it conducts a likelihood-ratio test. It doesn’t specifically output the likelihood ratio, but we can see the \\(p\\)-value is equivalent to our manual calculation above.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "max-lik.html#gradient-descent-algorithm",
    "href": "max-lik.html#gradient-descent-algorithm",
    "title": "6  Maximum Likelihood",
    "section": "\n6.5 Gradient descent algorithm",
    "text": "6.5 Gradient descent algorithm\nHow does the optim() function work? There are various optimization algorithms that can be implemented by optim() that are specified by the user in the method option. Several of these are gradient-based algorithms, which is a family of algorithms that are very common in engineering problems (e.g., optimal control of drones) and machine learning (e.g., neural networks, reinforcement learning). These algorithms generally minimize an “objective function”: \\(\\min_{{x\\in {\\mathbb  R}^{n}}}\\;f(x)\\).\nA gradient descent algorithm is the most common algorithm for minimizing a function. There are many varieties of these algorithms that employ various “tricks” to make the algorithms more efficient (e.g., find the solution in a smaller number of iterations) or more reliable. In lecture we outlined the foundational gradient descent algorithm, upon which many more advanced algorithms are based. For a function \\(f(x)\\), find the value of \\(x\\) that solves the problem: \\(\\min _{{x\\in {\\mathbb  R}^{n}}}\\;f(x)\\)\n\nChoose a starting value of \\(x\\)\n\nEvaluate \\(\\nabla f(x)\\). With \\(h=10^{-4}\\): \\[\\text{grad} = \\frac{f(x+h) - f(x)}{h}\\]\n\nSet the search direction as \\(\\text{direct} = -\\nabla f(x)\\)\n\nSet the step size as a fraction of \\(\\nabla f(x)\\): \\(\\text{alpha} = 0.005\\)\n\nUpdate \\(x\\) for next iteration: \\[x = x + \\text{alpha}* \\text{direct}\\]\n\nRepeat Steps 2-5 until \\(\\nabla f(x) \\approx 0\\) (i.e., \\(||\\text{grad}|| \\le 10^{-4}\\))\n\nLet’s implement this algorithm for the maximum likelihood solution of simple linear regression, using the data set above. To make this easier, we’re going to assume that we know the intercept and the residual standard deviation perfectly, so we are only trying to estimate the slope. See Footnotes 6.6.3 for the case in which we estimate all three model parameters simultaneously using gradient descent.\n\n# Set up some storage arrays:\nslope_guess = NULL\nnll_guess = NULL\n\n# initial guess\nslope = 0.1\n\n# set h for approx of gradient\nh = 1e-4\n\n# set step size\nalpha = 0.005\n\n# Set gradient to arbitrary high number\n## This makes the while() loop work\ngrad = 100\n\n# initialize counter\ni = 1\n\n# While gradient is not yet \\approx zero\nwhile (norm(grad, \"2\") &gt; 10^-4) {\n    \n    # Store the current value of slope:\n    slope_guess[i] = slope\n    \n    #############\n    # Approximate the gradient of the nll\n    #############\n    ## Adjust the slope by a small number, h\n    slope_adj = slope + h\n    ## Calculate f(slope)\n    f_slope = neg_log_lik(p = c(beta0, slope, sigma), \n                          data_df = my_df)\n    ## Store the current nll\n    nll_guess[i] = f_slope\n    ## Calculate f(slope + h)\n    f_slope_adj = neg_log_lik(p = c(beta0, slope_adj, sigma), \n                              data_df = my_df)\n    ## Calculate gradient\n    grad = (f_slope_adj - f_slope) / h\n    \n    # search direction\n    direct = -grad\n    \n    # update slope\n    slope = slope + alpha * direct\n    \n    # counter for x\n    i = i + 1\n}\n\n# Output the optimal slope and associated nll\nn_iter = i-1\n\ngrad_descent_tab = cbind(slope_guess[n_iter], nll_guess[n_iter])\ncolnames(grad_descent_tab) = c(\"slope\", \"neg_log_lik\")\n\n# Compare to optim() outpu\noptim_nll_tab = cbind(m_nll$par[2], m_nll$value)\ncolnames(optim_nll_tab) = c(\"slope\", \"neg_log_lik\")\n\nprint(\"Gradient Descent:\");grad_descent_tab\n\n[1] \"Gradient Descent:\"\n\n\n         slope neg_log_lik\n[1,] 0.5651002    20.58064\n\nprint(\"optim() output:\");optim_nll_tab\n\n[1] \"optim() output:\"\n\n\n       slope neg_log_lik\n[1,] 0.56205    19.08618\n\n\nNote that the estimates are not exactly the same, especially because when we used optim() we were estimating all three parameters simultaneously: intercept, slope, residual standard error.\nNow, let’s visualize the gradient descent.\n\nplot(nll_guess ~ slope_guess, \n     pch = 19, type = \"b\", \n     ylab = \"Neg. Log. Likelihood\",\n     xlab = expression(hat(beta)[1]),\n     xlim = c(0.1, 0.6), ylim = c(15, 40))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Maximum Likelihood</span>"
    ]
  },
  {
    "objectID": "model-select.html#lecture-material",
    "href": "model-select.html#lecture-material",
    "title": "7  Model selection",
    "section": "7.1 Lecture material",
    "text": "7.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "model-select.html#sec-data",
    "href": "model-select.html#sec-data",
    "title": "7  Model selection",
    "section": "7.2 Generate the data",
    "text": "7.2 Generate the data\nHere we will demonstrate two approaches to model comparison. But first let’s generate data, in the same way we did for multiple linear regression in (Chapter 4). Note that in this case, we will specify that two of the input variables have zero slope (i.e., no linear association with the outcome variable).\n\nn = 40\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.0\nbetas[3] = -0.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n# Create a data.frame\nmy_df = data.frame(y, xmat[,2:5])\nhead(my_df)\n\n          y         X1        X2       X3 X4\n1 28.492690 -1.7268438 18.788730 38.39431 18\n2 14.221411 16.0748747 16.474910 60.76146 10\n3  9.064956 -5.0439349  4.223082 33.40577  5\n4  9.366421  5.5611421  1.832589 41.76465  5\n5 18.874673 18.6915270  9.405498 40.26706 11\n6 17.706978  0.1767361  1.001228 46.05881 10\n\n# Run the model, report the summary\nm1 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\nm1_summary = summary(m1)\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3257 -1.4053 -0.4331  1.3299  4.3178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.41757    1.82632   1.871  0.06968 .  \nX1           0.03245    0.03810   0.852  0.40016    \nX2          -0.26989    0.07297  -3.698  0.00074 ***\nX3          -0.01823    0.03267  -0.558  0.58050    \nX4           1.68543    0.11083  15.207  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.94 on 35 degrees of freedom\nMultiple R-squared:  0.8901,    Adjusted R-squared:  0.8775 \nF-statistic: 70.86 on 4 and 35 DF,  p-value: 2.741e-16"
  },
  {
    "objectID": "model-select.html#parsimony-via-model-simplification",
    "href": "model-select.html#parsimony-via-model-simplification",
    "title": "7  Model Selection",
    "section": "\n7.2 Parsimony via model simplification",
    "text": "7.2 Parsimony via model simplification\nWe will successively simplify the model until we find a “minimally acceptable” model that explains the most variability in the outcome variable.\nThere are several built-in functions in R that can help us make quantitatively justified decisions about which input variables can be dropped from the full model to determine our minimally acceptable model. First, we can use the \\(F\\)-test as described in lecture. This can be implemented by the anova() function.\nBased on the summary() output, we see that input variable 3 (\\(x_3\\)) has the least significant effect on \\(y\\), so we will drop that first and proceed from there.\n\n# The full model lives in object m1\nformula(m1)\n\ny ~ 1 + X1 + X2 + X3 + X4\n\n# Create a new model with the update() function\n# This function has a strange notation, but so it goes...\nm2 = update(m1, .~. -X3)\nformula(m2)\n\ny ~ X1 + X2 + X4\n\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ X1 + X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4492 -1.3713 -0.3323  1.2450  4.3718 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.54371    0.92986   2.736 0.009605 ** \nX1           0.02863    0.03712   0.771 0.445496    \nX2          -0.26530    0.07181  -3.694 0.000728 ***\nX4           1.68217    0.10962  15.346  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.921 on 36 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8799 \nF-statistic: 96.22 on 3 and 36 DF,  p-value: &lt; 2.2e-16\n\n# Use anova() to test if the drop of X3 is justified\nanova(m2, m1)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X1 + X2 + X4\nModel 2: y ~ 1 + X1 + X2 + X3 + X4\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     36 132.91                           \n2     35 131.73  1    1.1712 0.3112 0.5805\n\n\nRemember that the hypothesis tested is: \\[H_0:\\text{simple model}\\] \\[H_A:\\text{complex model}\\] So if the \\(p \\ge 0.05\\), as usual, we cannot reject the null hypothesis. In this case, it means that the simple model is just as good as the more complex model. Therefore, we are justified in dropping \\(x_3\\). From the data, we could not detect that \\(x_3\\) has a statistically meaningful linear relationship with the outcome data \\(y\\).\nLet’s manually calculate that \\(F\\) test statistic and associated \\(p\\)-value to verify that we understand how the test works.\n\n# Extract the residuals (errors)\nresid_null = m2$residuals\nresid_full = m1$residuals\n\n# Sum of Square Errors (SSE)\nsse_null = crossprod(resid_null)\nsse_full = crossprod(resid_full)\n\n# degrees of freedom\ndf_null = n-(p-1) # we dropped one input variable\ndf_full = n-p\n\n# Calculate F_stat\nf_test = ((sse_null - sse_full)/(df_null - df_full)) / (sse_full/df_full)\n\n# Degrees of freedom for the F distribution:\ndf_numerator = df_null - df_full\ndf_denominator = df_full\n\np_m1vm2 = 1 - pf(f_test,\n                df1 = df_numerator,\n                df2 = df_denominator)\n\n# Compare to anova()\nf_test; p_m1vm2\n\n          [,1]\n[1,] 0.3111883\n\n\n          [,1]\n[1,] 0.5805028\n\nanova_m1vm2 = anova(m2,m1)\nanova_m1vm2$`F`; anova_m1vm2$`Pr(&gt;F)`\n\n[1]        NA 0.3111883\n\n\n[1]        NA 0.5805028\n\n\nLet’s continue with the simplification process, using the anova() function.\n\n# model 2 is the current best.\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ X1 + X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4492 -1.3713 -0.3323  1.2450  4.3718 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.54371    0.92986   2.736 0.009605 ** \nX1           0.02863    0.03712   0.771 0.445496    \nX2          -0.26530    0.07181  -3.694 0.000728 ***\nX4           1.68217    0.10962  15.346  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.921 on 36 degrees of freedom\nMultiple R-squared:  0.8891,    Adjusted R-squared:  0.8799 \nF-statistic: 96.22 on 3 and 36 DF,  p-value: &lt; 2.2e-16\n\n# Now, drop x1 and check\nm3 = update(m2, .~. -X1)\n\n# Check:\nanova(m3, m2)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X2 + X4\nModel 2: y ~ X1 + X2 + X4\n  Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)\n1     37 135.1                           \n2     36 132.9  1    2.1969 0.5951 0.4455\n\n# The p-value is not significant, so we\n# can accept the null (simpler model)\n\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4900 -1.4021 -0.1473  1.3871  4.3194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46667    0.91941   2.683 0.010847 *  \nX2          -0.25788    0.07077  -3.644 0.000819 ***\nX4           1.69901    0.10683  15.904  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.911 on 37 degrees of freedom\nMultiple R-squared:  0.8873,    Adjusted R-squared:  0.8812 \nF-statistic: 145.6 on 2 and 37 DF,  p-value: &lt; 2.2e-16\n\n# Remove X2 and check\nm4 = update(m3, .~. -X2)\nanova(m4, m3)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X4\nModel 2: y ~ X2 + X4\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     38 183.58                                  \n2     37 135.10  1    48.478 13.277 0.0008195 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Ok now the p-value is significant\n# We need to reject the null (simpler model)\n# We *cannot* reliably remove X2\n\n# Try X4 just in case:\n\nm5 = update(m3, .~. -X4)\nanova(m3, m5)\n\nAnalysis of Variance Table\n\nModel 1: y ~ X2 + X4\nModel 2: y ~ X2\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     37  135.1                                  \n2     38 1058.6 -1   -923.53 252.92 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# p-value is significant again\n# need to reject the null\n# we cannot drop X4\n\n# Therefore, m3 is most parsimonious\nsummary(m3)\n\n\nCall:\nlm(formula = y ~ X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4900 -1.4021 -0.1473  1.3871  4.3194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46667    0.91941   2.683 0.010847 *  \nX2          -0.25788    0.07077  -3.644 0.000819 ***\nX4           1.69901    0.10683  15.904  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.911 on 37 degrees of freedom\nMultiple R-squared:  0.8873,    Adjusted R-squared:  0.8812 \nF-statistic: 145.6 on 2 and 37 DF,  p-value: &lt; 2.2e-16\n\n\nTherefore, model 3, is the minimum acceptable model: \\[y_i = \\beta_0 + \\beta_2 x_{2,i} + \\beta_4 x_{4,i} + \\epsilon_i\\]\nWe could actually come to the same result, using a different, more automated function, step(). However, this function uses a different metric to test the null vs. full model hypothesis, the Akaike nformation criterion (AIC), which is calculated as: \\[\\text{AIC} = - 2ln(\\text{Model Likelihood}) + 2k\\] And \\(k\\) is the number of estimated parameters in the model. We can then compare the AIC values to decide which models are “best”. We will learn more about AIC later.\nLet’s use the step() function and verify it gives us the same final outcome.\n\nm1_step = step(m1)\n\nStart:  AIC=57.68\ny ~ 1 + X1 + X2 + X3 + X4\n\n       Df Sum of Sq     RSS     AIC\n- X3    1      1.17  132.90  56.030\n- X1    1      2.73  134.46  56.497\n&lt;none&gt;               131.73  57.676\n- X2    1     51.48  183.22  68.871\n- X4    1    870.38 1002.11 136.839\n\nStep:  AIC=56.03\ny ~ X1 + X2 + X4\n\n       Df Sum of Sq     RSS     AIC\n- X1    1      2.20  135.10  54.686\n&lt;none&gt;               132.90  56.030\n- X2    1     50.39  183.29  66.888\n- X4    1    869.43 1002.34 134.848\n\nStep:  AIC=54.69\ny ~ X2 + X4\n\n       Df Sum of Sq     RSS     AIC\n&lt;none&gt;               135.10  54.686\n- X2    1     48.48  183.58  64.951\n- X4    1    923.53 1058.63 135.034\n\nsummary(m1_step)\n\n\nCall:\nlm(formula = y ~ X2 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4900 -1.4021 -0.1473  1.3871  4.3194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46667    0.91941   2.683 0.010847 *  \nX2          -0.25788    0.07077  -3.644 0.000819 ***\nX4           1.69901    0.10683  15.904  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.911 on 37 degrees of freedom\nMultiple R-squared:  0.8873,    Adjusted R-squared:  0.8812 \nF-statistic: 145.6 on 2 and 37 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see the selected model only includes \\(x_2\\) and \\(x_4\\), just like our decision based on the \\(F\\)-test.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "model-select.html#model-averaging",
    "href": "model-select.html#model-averaging",
    "title": "7  Model Selection",
    "section": "\n7.3 Model averaging",
    "text": "7.3 Model averaging\nRecall from lecture that model averaging represents another philosophical approach to model selection and model comparison. In this case, the idea is that we cannot know with certainty which model of a nested sub-set of models is “true”. Therefore, instead of reporting the slopes and intercepts from the single “best” model that based on parsimony, we should report “averaged” values of slopes and intercepts. These averages will take into account all of the possible nested subset of models in which those slopes and intercepts could have been calculated. This averaging procedure can produce slope and intercept estimates (as well as estimates of their uncertainty) that are less biased, and can perhaps yield better predictions of future data.\nWe will use the MuMIn package (Multimodel Inference) to do model averaging later, but first we will do it manually.\n\n7.3.1 Required calculations\nRecall that to come up with averaged estimates of model parameters (e.g., model-averaged slopes) we need to calculate weighted averages. These averages are weighted by how well sub-models explain the data. Following lecture, we will use the corrected \\(AIC\\), noted as \\(AIC_c\\), to calculate how well a model explains the data.\n\\[\\text{AIC}_c = - 2ln(\\text{Model Likelihood}) + 2k + \\frac{2K(K+1)}{n-K-1}\\] where \\(n\\) is the number of data observations. Then, to calculate the weights we need to see how much each sub-model deviates from the best model. For this deviation we calculate, for sub-model \\(i\\):\n\\[\\Delta \\text{AIC}_{c,i} = \\text{AIC}_{c,\\text{best}} - \\text{AIC}_{c,i}\\] The weight of sub-model \\(i\\) is:\n\\[w_i = \\frac{\\text{exp}(-\\Delta \\text{AIC}_{c,i} / 2)}{\\sum_{r=1}^{R} \\text{exp}(-\\Delta \\text{AIC}_{c,r} / 2)} \\] And \\(R\\) is the number of submodels being examined.\nWe are now ready to calculate the weighted average of any parameter of interest in the full model, \\(\\theta\\):\n\\[\\hat{\\bar{\\theta}} = \\sum_{r=1}^{R} w_r \\hat{\\theta}_{r}\\] Here, \\(\\hat{\\bar{\\theta}}\\) is the model-averaged estimate of parameter \\(\\theta\\), \\(w_r\\) is the weight of sub-model \\(r\\), and \\(\\hat{\\theta}_r\\) is the parameter estimate derived from sub-model \\(r\\).\nWe can also calculate the new averaged uncertainty in the parameter estimate:\n\\[\\hat{\\text{var}}(\\hat{\\bar{\\theta}}) =  \\sum_{r=1}^{R} w_r \\left( \\hat{\\text{var}}(\\hat{\\theta})_r + (\\hat{\\theta} - \\hat{\\bar{\\theta}})^2 \\right) \\] Here, \\(\\hat{\\text{var}}(\\hat{\\bar{\\theta}})\\) is the standard error of the averaged model parameter, whereas \\(\\hat{\\text{var}}(\\hat{\\theta})_r\\) is the standard error of model parameter \\(\\theta\\) estimated from sub-model \\(r\\).\n\n7.3.2 Manual calculation\nLet’s see if we can manually calculate all of this from a less complex example model. Imagine our full model is a model that only has two input variables. We’ll use our simulated data set from above. (Of course, we know this is a poor model, but we’re just doing a case-study here.)\n\n# Full model:\nfull_mod = lm(y~1+X1+X2, data = my_df)\n\nIf this full model has two inputs, then the number of sub-models is \\(2^2 = 4\\), which iteratively drop one or both input variables. Now, run each sub-model. I know, this is tedious.\n\nsub_m2 = update(full_mod, .~. -X2)\nsub_m3 = update(full_mod, .~. -X1)\nsub_m4 = update(full_mod, .~. -X2-X1) # Intercept only\n\n# Store all models in a list for easy looping later:\nmodel_list = list(\n    full_mod, sub_m2, sub_m3, sub_m4\n)\n\n# how many models?\nn_mod = 4\n\nTo get the model-averaged slopes of inputs \\(x_1\\) and \\(x_2\\), we’ll need to calculate \\(AIC_c\\) values and model weights. We’ll store calculations in arrays as much as possible, so we can loop through.\nLet’s start with \\(AIC_c\\). Fortunately, there’s a built-in function for this in the MuMIn package, but we’ll do one manually first.\n\n# Extract neg-log-likelihood from full model:\nnll_full = -1*logLik(full_mod)\n# This is in a weird format, so we'll convert:\nnll_full = as.numeric(nll_full)\nk_full = 4 # two slopes + 1 intercept + residual sigma\n\n# Calculate AIC_c\naic_c_full = 2*nll_full + 2*k_full + (2*k_full*(k_full + 1))/(n - k_full - 1)\naic_c_full\n\n[1] 251.5063\n\n# Check with built-in\nlibrary(MuMIn)\nAICc(full_mod)\n\n[1] 251.5063\n\n# Now calculate all:\nAICc_vec = NULL\nfor(i in 1:n_mod){\n    AICc_vec[i] = AICc(model_list[[i]])\n}\nAICc_vec\n\n[1] 251.5063 252.3124 251.2157 253.8381\n\n\nWe can see that the ‘best’ model, according to \\(AIC_c\\) is the sub_m3, which includes the intercept and only input \\(x_2\\). This makes sense, because we know that the slope of \\(x_1\\) was simulated as zero.\nLet’s now calculate the \\(\\Delta \\text{AIC}_c\\).\n\n# Best AICc\nAICc_best = min(AICc_vec)\n\n#\\Delta AIC_c\nDelta_AICc_vec = AICc_vec - AICc_best\nDelta_AICc_vec\n\n[1] 0.290611 1.096742 0.000000 2.622409\n\n\nNow we can calculate the model weights (i.e., the value representing how “good” each model is, relative to the best model).\n\n# Calculate the denominator of the weight calculation\nweight_denom = 0\nfor(i in 1:n_mod){\n    weight_denom = \n        weight_denom +\n        exp( -Delta_AICc_vec[i] / 2 )\n}\nweight_denom\n\n[1] 2.712144\n\n# Now the individual weights:\nweight = NULL\nfor(i in 1:n_mod){\n    weight[i] = \n        exp( -Delta_AICc_vec[i] / 2) / weight_denom\n}\nweight\n\n[1] 0.31884668 0.21307516 0.36871201 0.09936614\n\n# Sum to 1?\nsum(weight)\n\n[1] 1\n\n\nWe can see the “better” models, based on \\(AIC_c\\) have higher weights, and the weight vector should add to 1.\nLet’s calculate the model-averaged slope estimate for input \\(x_2\\). To do this, we’ll first need to extract the estimate from each sub-model. This is a little tedious, because we need to know which coefficient refers to \\(x_2\\) in each sub-model object (or if the coefficient is absent and therefore equal to zero).\n\ncoef_x2 = NULL\ncoef_x2[1] = coef(model_list[[1]])[3]\ncoef_x2[2] = 0 # Absent from this sub-model\ncoef_x2[3] = coef(model_list[[3]])[2]\ncoef_x2[4] = 0 # Absent from this sub-model\ncoef_x2\n\n[1] 0.2975017 0.0000000 0.3649074 0.0000000\n\n# Averaged, based on model weight:\navg_coef_x2 = 0\nfor(i in 1:n_mod){\n    avg_coef_x2 = \n        avg_coef_x2 +\n        weight[i] * coef_x2[i]\n}\n\navg_coef_x2\n\n[1] 0.2294032\n\n\nWe can see the model-averaged slope estimate for input \\(x_2\\) is slightly less than the estimate from the full model.\n\nsummary(full_mod)\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.9147 -3.6749  0.6359  3.4050 10.5608 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.58750    1.78926   7.035 2.55e-08 ***\nX1           0.14204    0.09854   1.441   0.1579    \nX2           0.29750    0.16725   1.779   0.0835 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.205 on 37 degrees of freedom\nMultiple R-squared:  0.1637,    Adjusted R-squared:  0.1185 \nF-statistic: 3.621 on 2 and 37 DF,  p-value: 0.03662\n\ncoef(full_mod)[3]\n\n       X2 \n0.2975017 \n\n\nI’ll leave calculating the model-averaged standard error of the slopes as an exercise for you as a student.\nAs I mentioned above, fortunately someone created a package to do this model averaging for us and remove a lot of the tedium.\nFirst, run all sub-models using the MuMIn::dredge() function.\n\n# Required for MuMIn::dredge functionality\noptions(na.action = \"na.fail\")\n\n# Fit all sub-models:\ndredge_test = dredge(full_mod)\n\nFixed term is \"(Intercept)\"\n\ndredge_test\n\nGlobal model call: lm(formula = y ~ 1 + X1 + X2, data = my_df)\n---\nModel selection table \n  (Intrc)    X1     X2 df   logLik  AICc delta weight\n3   12.71       0.3649  3 -122.275 251.2  0.00  0.369\n4   12.59 0.142 0.2975  4 -121.182 251.5  0.29  0.319\n2   15.26 0.191         3 -122.823 252.3  1.10  0.213\n1   16.31               2 -124.757 253.8  2.62  0.099\nModels ranked by AICc(x) \n\n\nSee how this output has run all sub-models, calculated the likelihoods, the \\(AIC_c\\), the \\(\\Delta AIC_c\\), and the model weights.\nNow, we can average all of the models.\n\n# Average the models:\ntest_average = model.avg(dredge_test, fit = TRUE)\nsummary(test_average)\n\n\nCall:\nmodel.avg(object = get.models(object = dredge_test, subset = NA))\n\nComponent model call: \nlm(formula = y ~ &lt;4 unique rhs&gt;, data = my_df)\n\nComponent models: \n       df  logLik   AICc delta weight\n2       3 -122.27 251.22  0.00   0.37\n12      4 -121.18 251.51  0.29   0.32\n1       3 -122.82 252.31  1.10   0.21\n(Null)  2 -124.76 253.84  2.62   0.10\n\nTerm codes: \nX1 X2 \n 1  2 \n\nModel-averaged coefficients:  \n(full average) \n            Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)  13.5710     2.1113      2.1512   6.308   &lt;2e-16 ***\nX2            0.2294     0.2083      0.2113   1.086    0.278    \nX1            0.0860     0.1092      0.1108   0.776    0.438    \n \n(conditional average) \n            Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)  13.5710     2.1113      2.1512   6.308   &lt;2e-16 ***\nX2            0.3336     0.1683      0.1737   1.921   0.0547 .  \nX1            0.1617     0.1009      0.1041   1.553   0.1205    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis summary() statement shows the model-averaged values of slopes of \\(x_1\\) and \\(x_2\\), and the intercept. We care about the “full average”. You can see the averaged estimate of the slope for \\(x_2\\) matches our manual calculation. For emphasis:\n\ntest_average$coefficients[1,2];\n\n[1] 0.2294032\n\navg_coef_x2\n\n[1] 0.2294032\n\n\n\n7.3.3 Back to more complex model\nOk, but our full model had four input variables, which means the number of sub-models is \\(4^2 = 16\\). Let’s not do that manually, but instead use the MuMIn::model.avg() function.\n\n# Reminder, m1 was our full model:\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3257 -1.4053 -0.4331  1.3299  4.3178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.41757    1.82632   1.871  0.06968 .  \nX1           0.03245    0.03810   0.852  0.40016    \nX2          -0.26989    0.07297  -3.698  0.00074 ***\nX3          -0.01823    0.03267  -0.558  0.58050    \nX4           1.68543    0.11083  15.207  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.94 on 35 degrees of freedom\nMultiple R-squared:  0.8901,    Adjusted R-squared:  0.8775 \nF-statistic: 70.86 on 4 and 35 DF,  p-value: 2.741e-16\n\n# Fit all sub-models:\ndredge_m1 = dredge(m1)\n\nFixed term is \"(Intercept)\"\n\n# Average the models:\nm1_average = model.avg(dredge_m1, fit = TRUE)\nsummary(m1_average)\n\n\nCall:\nmodel.avg(object = get.models(object = dredge_m1, subset = NA))\n\nComponent model call: \nlm(formula = y ~ &lt;16 unique rhs&gt;, data = my_df)\n\nComponent models: \n       df  logLik   AICc delta weight\n24      4  -81.10 171.34  0.00   0.56\n124     5  -80.77 173.31  1.97   0.21\n234     5  -81.01 173.78  2.43   0.17\n1234    6  -80.60 175.74  4.39   0.06\n4       3  -87.23 181.13  9.79   0.00\n14      4  -87.20 183.55 12.20   0.00\n34      4  -87.23 183.60 12.26   0.00\n134     5  -87.19 186.15 14.81   0.00\n2       3 -122.27 251.22 79.87   0.00\n12      4 -121.18 251.51 80.16   0.00\n1       3 -122.82 252.31 80.97   0.00\n23      4 -122.21 253.55 82.21   0.00\n(Null)  2 -124.76 253.84 82.49   0.00\n123     5 -121.18 254.12 82.78   0.00\n13      4 -122.82 254.78 83.44   0.00\n3       3 -124.73 256.12 84.77   0.00\n\nTerm codes: \nX1 X2 X3 X4 \n 1  2  3  4 \n\nModel-averaged coefficients:  \n(full average) \n             Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)  2.642223   1.219137    1.258700   2.099 0.035802 *  \nX2          -0.258803   0.074396    0.076727   3.373 0.000743 ***\nX4           1.693799   0.109648    0.113293  14.951  &lt; 2e-16 ***\nX1           0.007999   0.023509    0.024078   0.332 0.739716    \nX3          -0.003320   0.016617    0.017119   0.194 0.846219    \n \n(conditional average) \n            Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)  2.64222    1.21914     1.25870   2.099 0.035802 *  \nX2          -0.26062    0.07141     0.07385   3.529 0.000417 ***\nX4           1.69380    0.10965     0.11329  14.951  &lt; 2e-16 ***\nX1           0.02940    0.03744     0.03875   0.759 0.448057    \nX3          -0.01452    0.03232     0.03345   0.434 0.664272    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice how we see the model with inputs \\(x_2\\) and \\(x_4\\) is the best, based on \\(AIC_c\\) (note this is the model labeled as 24 meaning it inclues inputs 2 and 4).\nWe can also plot the model parameter estimates with their confidence intervals:\n\n# Plot the coefficient estimates (from the averaged model)\nplot(m1_average)\n\n\n\n\n\n\n\nFinally, we can use the predict() function as we have before to visualize the effect of each input variable on the outcome. Here, we will show the independent, model-averaged effect of \\(x_2\\), when all other input variables are held at their average values. Then, we’ll do the same for \\(x_4\\).\n\n# Predict from the average model:\n# How does y change as a function of x2, while \n# other inputs held at their average?\nnew_df = data.frame(\n    X1 = rep(mean(my_df$X1), 100),\n    X2 = seq(0, 19, length.out = 100),\n    X3 = rep(mean(my_df$X3), 100),\n    X4 = rep(mean(my_df$X4), 100)\n)\n\npred_m1_avg_x2 = \n    predict(m1_average,\n            newdata = new_df,\n            se.fit = TRUE)\n\nplot(my_df$y ~ my_df$X2,\n     xlab = \"input x2\", ylab = \"y\", pch = 19)\nlines(pred_m1_avg_x2$fit ~ new_df$X2)\nlines(pred_m1_avg_x2$fit-2*pred_m1_avg_x2$se.fit ~ new_df$X2, lty = 2)\nlines(pred_m1_avg_x2$fit+2*pred_m1_avg_x2$se.fit ~ new_df$X2, lty = 2)\n\n\n\n\n\n\n# Predict from the average model:\n# How does y change as a function of x4, while \n# other inputs held at their average?\nnew_df = data.frame(\n    X1 = rep(mean(my_df$X1), 100),\n    X2 = rep(mean(my_df$X2), 100),\n    X3 = rep(mean(my_df$X3), 100),\n    X4 = seq(0, 19, length.out = 100)\n)\n\npred_m1_avg_x4 = \n    predict(m1_average,\n            newdata = new_df,\n            se.fit = TRUE)\n\nplot(my_df$y ~ my_df$X4,\n     xlab = \"input x4\", ylab = \"y\", pch = 19)\nlines(pred_m1_avg_x4$fit ~ new_df$X4)\nlines(pred_m1_avg_x4$fit-2*pred_m1_avg_x4$se.fit ~ new_df$X4, lty = 2)\nlines(pred_m1_avg_x4$fit+2*pred_m1_avg_x4$se.fit ~ new_df$X4, lty = 2)\n\n\n\n\n\n\n\nAnother, perhaps simpler way to vizualize how well a model matches the data is to plot the model predictions of the data versus the observed data. We can even compare this to the non-averaged model.\n\nraw_predict_avg = predict(m1_average)\nraw_predict_nonavg = predict(m1)\nraw_predict_bad = predict(sub_m2)\n\nplot(my_df$y ~ raw_predict_avg,\n     xlab = \"Model Prediction\", ylab = \"Data, y\", pch = 19, col = \"red\")\npoints(my_df$y ~ raw_predict_nonavg, pch = 19, col = \"black\")\npoints(my_df$y ~ raw_predict_bad, pch = 19, col = \"orange\")\n# 1-to-1 line\nabline(a = 0, b = 1)\n\n\n\n\n\n\n\nIt is hard to see, and likely not significant in this case, but the red points (model-averaged) tend to be closer to the 1:1 line, compared to the black points, meaning the averaged model makes slightly better predictions of the observed data. What is more clear, is that the “bad” model (which only included covariate \\(x_1\\)), does not match the 1:1 line at all; it’s more of a shot-gun of points. Therefore, this clearly indicates the model is not predictive of the \\(y\\) data. This is a good visualization of how well your models’ within-sample prediction (i.e., how close the model predictions of observed data match the observed data).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "anova.html#lecture-material",
    "href": "anova.html#lecture-material",
    "title": "8  ANOVA",
    "section": "\n8.1 Lecture material",
    "text": "8.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "anova.html#one-way-anova",
    "href": "anova.html#one-way-anova",
    "title": "8  ANOVA",
    "section": "",
    "text": "8.1.1 Manually calculate \\(F\\)\n\nAs we can see, the aov() function is using an \\(F\\)-test to determine if any of the group-wise means differ from the global mean (see lecture materials on this topic). Indeed, based on the \\(p\\)-value, at least one group-wise mean is different. To verify our understand, as usual, let’s manually calculate the \\(F\\) test statistic and \\(p\\)-value.\n\n# First, we need to run the \"null\" model (intercept only)\nm_one_way_null = lm(y ~ 1, \n                    data = one_way_df)\n    \n# Extract the residuals (errors)\nresid_full = m_one_way$residuals\nresid_null = m_one_way_null$residuals\n\n# Sum of Square Errors (SSE)\nsse_full = crossprod(resid_full)\nsse_null = crossprod(resid_null)\n\n# degrees of freedom\nn_obs = length(y)\ndf_full = n_obs - n_levels\ndf_null = n_obs - 1\n\n# Calculate F_stat\nf_test = ((sse_null - sse_full)/(df_null - df_full)) / (sse_full/df_full)\n\n# Degrees of freedom for the F distribution:\ndf_numerator = df_null - df_full\ndf_denominator = df_full\n\np_one_way = 1 - pf(f_test,\n                   df1 = df_numerator,\n                   df2 = df_denominator)\n\n# Compare to anova()\nf_test; p_one_way\n\n         [,1]\n[1,] 10.31654\n\n\n             [,1]\n[1,] 0.0001149183\n\nsummary_aov = summary(m_one_way)\nsummary_aov[[1]]$`F value`; summary_aov[[1]]$`Pr(&gt;F)`\n\n[1] 10.31654       NA\n\n\n[1] 0.0001149183           NA\n\n\nWe see that our manual calculation of \\(F\\) and the corresponding \\(p\\)-value match with the output of the aov() function, so we can verify we are understanding what aov() is doing.\n\n8.1.2 Tukey HSD\nBut, which specific means differ from each other? To know this, we need to compute all pairwise differences between the group-wise means. Remember that for \\(L\\) number of groups, we have \\(L(L-1)/2\\) number of pairwise comparisons. We want to correct for multiple comparisons, so we don’t inflate our risk of Type I errors. Therefore, we’ll conduct a Tukey Honest Significant Difference (HSD) test.\n\nTukeyHSD(m_one_way)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ 1 + x, data = one_way_df)\n\n$x\n          diff       lwr        upr     p adj\n2-1  2.9796804  1.310442  4.6489184 0.0001710\n3-1  0.5570861 -1.112152  2.2263241 0.7050400\n3-2 -2.4225942 -4.091832 -0.7533562 0.0024902\n\n\nThis test is looking at pairwise differences. For instance 2-1 is the difference between \\(x\\) levels 2 and 1, and so on. The output reports the raw difference in means as diff, and then reports the lower and upper confidence limits of this difference as lwr and upr. These default to 95% confidence intervals, though the user can specify an option in the function to change this percentage confidence. Then the output reports the p adj, which is the adjusted \\(p\\)-value, adjusting for multiple comparisons. Here, we can conclude that while levels 1 and 3 are not different from one another, level 2 is different from both levels 1 and 3. This makes sense when we look at the boxplot (again).\n\nplot(y~x, data = one_way_df)\n\n\n\n\n\n\n\n\n8.1.3 Manually calculate \\(q\\)\n\nHow does the Tukey HSD adjust for multiple comparisons? It uses a special hypothesis test, which has an associated probability distribution, tukey. Remember that we calculate a test statistic, \\(q\\) for the difference between levels \\(i\\) and \\(j\\): \\[q_{i,j} = \\frac{|\\bar{y}_i - \\bar{y}_j|}{\\sqrt{\\hat{\\sigma}^2_p/n}}\\] Here, \\(\\hat{\\sigma}^2_p\\) is the pooled variance of the whole outcome data set, \\(y\\), and \\(n\\) is the number of observations per level. This is why a balanced design is important; the test assumes that each level has the same number of observations.\nLet’s manually calculate the \\(p\\)-value for the difference between group levels 2 and 1.\n\n# Need to extract the data observations associated with all x levels, separately\nthese_x_1 = which(x == 1)\nthese_x_2 = which(x == 2)\nthese_x_3 = which(x == 3)\ny_1 = y[these_x_1]\ny_2 = y[these_x_2]\ny_3 = y[these_x_3]\n\n# Calculate pooled variance for whole data set\n# Notice how this is not the same as var(y)\npooled_var = (var(y_1)+var(y_2)+var(y_3))/3\n\n# Calculate q test statistic\nq_test = \n    abs(mean(y_1) - mean(y_2)) /\n    sqrt(pooled_var/n_obs_per_level)\n\nq_test\n\n[1] 6.041317\n\n# Degrees of freedom for q test stat\ndf_q = n_obs - n_levels\n\n# Calculate p-value\np_2v1 = ptukey(q_test, \n               nmeans = 3,\n               df = df_q, \n               lower.tail = FALSE)\np_2v1\n\n[1] 0.0001710377\n\n\nWe can see this \\(p\\)-value matches the first p adj from the TukeyHSD() output.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "anova.html#two-way-anova",
    "href": "anova.html#two-way-anova",
    "title": "8  ANOVA",
    "section": "\n8.2 Two-way ANOVA",
    "text": "8.2 Two-way ANOVA\nTwo-way ANOVA is the special case in which we have exactly two input variables, each of which has two or more “levels”. The two-way ANOVA linear model can be written in several ways. We’ll start with the easiest case in which each of the two input variables only has two levels. This refers to a “2x2” experimental design. To be concrete, we’ll use an example. We will simulate data for plant growth in which we manipulate Temperature (Low or High) and soil Moisture (Dry or Wet). We apply the 2x2 combination of these treatments which leads to four total treatments (e.g., Low-Dry, Low-Wet, etc.). We will apply each treatment combination to 25 plants and measure the outcome of Growth.\nIn the following model structure, we will code Temperature as Low == 0 and High == 1, and we will code soil Moisture as Dry == 0 and Wet == 1.\n\\[y_{i} = \\mu + \\beta_{\\text{Temp}}\\text{Temp} + \\beta_{\\text{Moist}}\\text{Moist} + \\beta_{\\text{Intx}}\\text{Temp}\\text{Moist} + \\epsilon_{i,l}\\] \\[\\epsilon \\sim N(0, \\sigma^2 I)\\]\nIn the model structure, \\(\\mu\\) is a global mean for \\(y\\) (i.e., across all treatments). Then, this global mean can be altered (i.e. affected by) the treatment combinations. \\(\\beta_{\\text{Temp}}\\) and \\(\\beta_{\\text{Moist}}\\) are the “main” effects and \\(\\beta_{\\text{Intx}}\\) is the interactive effect. Temp and Moist are binary indicator variables (0/1), so, for instance, if Temperature is Low, Temp == 0, and so on. This means that \\(\\beta_{\\text{Temp}}\\) only gets added to the global mean when Temp == 1 (i.e., Temperature is High), and so on. The \\(\\beta_{\\text{Intx}}\\) would get added to the global mean if both Temp and Moist are 1, so High Temperature and Wet Moisture.\n\n8.2.1 Only main effects\nFirst, let’s simulate a case in which we only have main effects, no interaction. Specifically, we will assume that the plant Growth declines under High Temperature, but that there is no effect of soil Moisture. Also, for data visualization, we will use the ggplot2 package, because it is easier to customize.\n\nlibrary(ggplot2)\n\n# 2x2 design\n# Replicated 25 times\n# Low, High (0, 1)\nn_reps = 25\nTemp = rep(0:1, each = n_reps*2)\n\n# Low, High (0, 1)\nMoisture = c(\n    rep(0:1, each = n_reps),\n    rep(0:1, each = n_reps)\n)\n\n# Simulate different effects:\nset.seed(8)\n# Just main effect\nglobal = 5\nbeta_t = -1.25\nbeta_m = 0\nbeta_intx = 0\nsigma = 1.0\n\ny = NULL\nfor(i in 1:length(Temp)){\n    y[i] = global + \n        beta_t * Temp[i] +\n        beta_m * Moisture[i] +\n        beta_intx * Temp[i] * Moisture[i] +\n        rnorm(1, mean = 0, sd = sigma)\n}\n\n# Store as data frame\ntwo_way_df1 = data.frame(\n    Growth = y,\n    Temp = factor(Temp, levels = c(0,1), labels = c(\"Low\", \"High\")),\n    Moisture = factor(Moisture, levels = c(0,1), labels = c(\"Dry\", \"Wet\"))\n)\n\n\nggplot(two_way_df1) +\n    geom_boxplot(aes(x = Temp, y = Growth, color = Moisture))\n\n\n\n\n\n\n\nHere, it is visually clear that higher temperatures lead to lower plant growth, but there is no clear effect of soil moisture, just as we simulated.\nLet’s run the ANOVA model and see if the output makes sense.\n\nsummary(aov(Growth ~ Temp + Moisture + Temp:Moisture,\n            data = two_way_df1))\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemp           1  43.31   43.31  36.134 3.31e-08 ***\nMoisture       1   0.00    0.00   0.002    0.963    \nTemp:Moisture  1   0.00    0.00   0.000    0.999    \nResiduals     96 115.06    1.20                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIndeed, we only see a main effect of Temp, and no main effect of Moisture, and no interactive effect (Temp:Moisture).\n\n8.2.2 Interactive effect\nNow will simulate a case in which we have a main effect of temperature, similar to the above (Growth declines at High Temp). We will also add a positive interactive effect. This means that the effect of specific effects of temperature on growth will depend on the soil moisture content. Specifically, with a positive interactive effect, if Temperature is High and Moisture is Wet, then we will get an increase in Growth, rather than a decline. Let’s see what this looks like visually.\n\n# INTERACTION\nglobal = 5\nbeta_t = -1.25\nbeta_m = 0\nbeta_intx = 2.5\nsigma = 1.0\n\nset.seed(5)\ny = NULL\nfor(i in 1:length(Temp)){\n    y[i] = global + \n        beta_t * Temp[i] +\n        beta_m * Moisture[i] +\n        beta_intx * Temp[i] * Moisture[i] +\n        rnorm(1, mean = 0, sd = sigma)\n}\n\n# Store as data frame\ntwo_way_df2 = data.frame(\n    Growth = y,\n    Temp = factor(Temp, levels = c(0,1), labels = c(\"Low\", \"High\")),\n    Moisture = factor(Moisture, levels = c(0,1), labels = c(\"Dry\", \"Wet\"))\n)\n\nggplot(two_way_df2) +\n    geom_boxplot(aes(x = Temp, y = Growth, color = Moisture))\n\n\n\n\n\n\n\nWhat wee see is that the effect of Temperature depends on the value of soil Moisture. In this case, as Temperature moves from Low to High, plant Growth declines if the soil is Dry, but Growth increases if the soil is Wet.\nBut those trends are just visual at this point. How do we quantify whether specific comparisons are statistically significant? We will again use the Tukey HSD! First, run the ANOVA and verify that there is a significant interaction.\n\naov_intx = aov(Growth ~ Temp + Moisture + Temp:Moisture,\n               data = two_way_df2)\nsummary(aov_intx)\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemp           1   0.11    0.11   0.122    0.727    \nMoisture       1  54.20   54.20  59.746 1.06e-11 ***\nTemp:Moisture  1  41.00   41.00  45.190 1.28e-09 ***\nResiduals     96  87.09    0.91                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIndeed, the interaction is significant, so now we need to figure out which specific differences among covariate levels exist.\n\nTukeyHSD(aov_intx)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Growth ~ Temp + Moisture + Temp:Moisture, data = two_way_df2)\n\n$Temp\n                diff        lwr       upr     p adj\nHigh-Low -0.06659871 -0.4447271 0.3115297 0.7273973\n\n$Moisture\n            diff      lwr      upr p adj\nWet-Dry 1.472439 1.094311 1.850567     0\n\n$`Temp:Moisture`\n                        diff        lwr        upr     p adj\nHigh:Dry-Low:Dry  -1.3471656 -2.0515400 -0.6427911 0.0000152\nLow:Wet-Low:Dry    0.1918721 -0.5125023  0.8962466 0.8920281\nHigh:Wet-Low:Dry   1.4058403  0.7014658  2.1102147 0.0000062\nLow:Wet-High:Dry   1.5390377  0.8346633  2.2434121 0.0000007\nHigh:Wet-High:Dry  2.7530058  2.0486314  3.4573803 0.0000000\nHigh:Wet-Low:Wet   1.2139681  0.5095937  1.9183426 0.0001084\n\n\nThe important part of the out put is $'Temp:Moisture', which shows the pairwise tests of the interactive effects. See if you can understand which differences are statistically significant, after accounting for multiple comparisons.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "anova.html#ancova",
    "href": "anova.html#ancova",
    "title": "8  ANOVA",
    "section": "\n8.3 ANCOVA",
    "text": "8.3 ANCOVA\nANCOVA is a variant of ANOVA, which stands for “Analysis of Covariance”. For one-way ANCOVA, we are analyzing the effect of one discrete (i.e., categorical) input variable, plus one continuous input variable. The “covariance” part refers to the fact that we’re trying to understand if the slope of the continuous input variable is the same or different between the levels of the discrete input variable.\nFor one-way ANCOVA, we have a single discrete input variable, and let’s assume the levels are 0 or 1. The model for one-way ANCOVA can then be written as: \\[y_i = \\mu_0 + \\alpha_1 X_i + \\beta_1 Z_i + \\beta_2 X_i Z_i + \\epsilon_i\\] Here, \\(\\beta_1\\) is the slope of \\(Z\\) when \\(X=0\\), but \\(\\beta_1 + \\beta_2\\) is the slope of \\(Z\\) when \\(X=1\\). If there is no interaction (i.e., \\(\\beta_2=0\\)), then the slope of \\(Z\\) is the same for the two levels of \\(X\\).\nLet’s continue with our example from two-way ANOVA, where we have Temperature and soil Moisture. However, for ANCOVA, we will assume that Temperature is a continuous input variable, rather than a discrete one.\n\n8.3.1 Create data set and model function\nWe are going to explore all four possible outcomes of the simple one-way ANCOVA. We will therefore create a static data set, as well as a function to simulate the outcome variable \\(Y\\), depending on the model parameters.\n\nlibrary(ggplot2)\nset.seed(7)\nn_rep = 35\n# Dry, Wet (0, 1)\nMoisture = rep(0:1, each = n_rep)\n# Continuous Temperature\nTemp = c(\n    runif(n_rep, min = 5, max = 25),\n    runif(n_rep, min = 5, max = 25)\n)\n\n# Function to simulate observed data\n\ncalc_y_func = function(\n        mu_0 = 0,\n        alpha_1 = 0,\n        beta_1 = 0,\n        beta_2 = 0,\n        sigma = 1.0){\n    y = NULL\n    for(i in 1:length(Temp)){\n        y[i] = mu_0 + \n            alpha_1 * Moisture[i] +\n            beta_1 * Temp[i] +\n            beta_2 * Temp[i] * Moisture[i] +\n            rnorm(1, mean = 0, sd = sigma)\n    }\n    \n    return(y)\n}\n\n\n8.3.2 Main effect of Temperature\nFirst, we’ll see what the data look like when we only have an effect of the continuous input variable, in this case Temperature.\n\nset.seed(2)\n# 1. Only slope effect\nmu_0 = 5\nalpha_1 = 0\nbeta_1 = 0.75\nbeta_2 = 0\nsigma = 2.25\n\ny1 = calc_y_func(\n    mu_0,alpha_1,beta_1,\n    beta_2,sigma\n)\n\n# Store as data frame\nancova_df1 = data.frame(\n    Growth = y1,\n    Temp = Temp,\n    Moisture = factor(Moisture, levels = c(0,1), labels = c(\"Dry\", \"Wet\"))\n)\n\nggplot(ancova_df1) +\n    geom_point(aes(x = Temp, y = Growth, \n                   shape = Moisture, color = Moisture))\n\n\n\n\n\n\n\nWe can see that while there is clearly a linear effect of Temperature on plant Growth, there is no obvious difference between Dry and Wet soil Moisture.\nNow, let’s use model simplification to statistically validate our visual interpretation of the data.\n\nm1 = aov(Growth ~ Temp + Moisture + Temp*Moisture, data = ancova_df1)\nsummary(m1)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTemp           1 1422.8  1422.8 214.231 &lt;2e-16 ***\nMoisture       1   17.0    17.0   2.559  0.114    \nTemp:Moisture  1   14.6    14.6   2.204  0.142    \nResiduals     66  438.3     6.6                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# We can try dropping the interaction\nm2 = update(m1, .~. -Temp:Moisture)\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Growth ~ Temp + Moisture + Temp * Moisture\nModel 2: Growth ~ Temp + Moisture\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     66 438.35                           \n2     67 452.99 -1   -14.641 2.2044 0.1424\n\n# Notice how this F and p-value is the same from\n# the summary() table of m1. \nm3 = update(m2, .~. -Moisture)\nsummary(m3)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTemp         1   1423  1422.8   205.9 &lt;2e-16 ***\nResiduals   68    470     6.9                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Check the slope and intercept\nm3$coefficients\n\n(Intercept)        Temp \n   4.487613    0.787559 \n\n\nThe model simplification validates that there is only an effect of Temperature, and the estimated slope matches our simulated value pretty closely.\n\n8.3.3 Main effect of Moisture\nNow, let’s assume there is no effect of Temperature, but there is a difference in the levels of Moisture.\n\nset.seed(5)\n# 2. Only factor effect\nmu_0 = 5\nalpha_1 = 4\nbeta_1 = 0\nbeta_2 = 0\nsigma = 2.0\n\ny2 = calc_y_func(\n    mu_0,alpha_1,beta_1,\n    beta_2,sigma\n)\n\n# Store as data frame\nancova_df2 = ancova_df1\nancova_df2$Growth = y2\n\nggplot(ancova_df2) +\n    geom_point(aes(x = Temp, y = Growth, \n                   shape = Moisture, color = Moisture))\n\n\n\n\n\n\n\nWe can see the difference in the means of Moisture levels, but no clear, linear Temperature effect. We will validate this with model simplification.\n\nm1 = aov(Growth ~ Temp + Moisture + Temp*Moisture, data = ancova_df2)\nsummary(m1)\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemp           1   7.73    7.73   1.875    0.176    \nMoisture       1 173.80  173.80  42.164 1.28e-08 ***\nTemp:Moisture  1   0.04    0.04   0.009    0.923    \nResiduals     66 272.06    4.12                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 = update(m1, .~. - Temp:Moisture)\nsummary(m2)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemp         1   7.73    7.73   1.904    0.172    \nMoisture     1 173.80  173.80  42.797 9.92e-09 ***\nResiduals   67 272.10    4.06                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm3 = update(m2, .~. - Temp)\nsummary(m3)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nMoisture     1  181.3   181.3   45.27 4.37e-09 ***\nResiduals   68  272.3     4.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIndeed, the model with only Moisture is best. We can then use Tukey HSD to validate the specific differences in the Moisture levels.\n\nTukeyHSD(m3)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Growth ~ Moisture, data = ancova_df2)\n\n$Moisture\n            diff      lwr      upr p adj\nWet-Dry 3.218663 2.264057 4.173269     0\n\n\nThe test shows that Wet soil leads to an approximately 3.2 value increase in plant Growth over dry soil, and this is a statistically significant difference (p-value is very low, near zero).\nWe can use ggplot2 to help us visualize this differnce more clearly.\n\nggplot(ancova_df2, aes(x = Temp, y = Growth, \n                       color = Moisture)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n8.3.4 Main effects of Temperature and Moisture\nNow, let’s assume there is a linear effect of Temperature, and there is a difference in the levels of Moisture. But, there is still no interaction.\n\nset.seed(1)\n# 3. Factor and Slope effect\nmu_0 = 3\nalpha_1 = 5 \nbeta_1 = 0.5\nbeta_2 = 0\nsigma = 2.0\n\ny3 = calc_y_func(\n    mu_0,alpha_1,beta_1,\n    beta_2,sigma\n)\n\n# Store as data frame\nancova_df3 = ancova_df1\nancova_df3$Growth = y3\n\nggplot(ancova_df3) +\n    geom_point(aes(x = Temp, y = Growth, \n                   shape = Moisture, color = Moisture))\n\n\n\n\n\n\n\nWe can see the difference in the means of Moisture levels, and an obvious linear Temperature effect. Again, we will validate this with model simplification.\n\nm1 = aov(Growth ~ Temp + Moisture + Temp*Moisture, \n         data = ancova_df3)\nsummary(m1)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTemp           1  697.2   697.2 197.926 &lt;2e-16 ***\nMoisture       1  491.1   491.1 139.408 &lt;2e-16 ***\nTemp:Moisture  1    0.3     0.3   0.082  0.776    \nResiduals     66  232.5     3.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm2 = update(m1, .~. - Temp:Moisture)\nsummary(m2)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nTemp         1  697.2   697.2   200.7 &lt;2e-16 ***\nMoisture     1  491.1   491.1   141.3 &lt;2e-16 ***\nResiduals   67  232.8     3.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# m2 is best\nm2$coefficients\n\n(Intercept)        Temp MoistureWet \n  3.5666389   0.4710967   5.3763016 \n\n\nThe test shows that Wet soil leads to an approximately 5.3 value increase in plant Growth over dry soil, and that Temperature has a similar positive, linear effect on plant Growth for both wet and dry soil.\nWe can use ggplot2 to help us visualize this difference more clearly.\n\nggplot(ancova_df3, aes(x = Temp, y = Growth, \n                       color = Moisture)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n8.3.5 Interaction between Temperature and Moisture\nFinally, let’s assume there is an interaction between Moisture and Temperature.\n\n########################\nset.seed(1)\n# 4. Positive interaction\nmu_0 = 15\nalpha_1 = 0\nbeta_1 = -0.45\nbeta_2 = 0.75\nsigma = 2.0\n\ny4 = calc_y_func(\n    mu_0,alpha_1,beta_1,\n    beta_2,sigma\n)\n\n# Store as data frame\nancova_df4 = ancova_df1\nancova_df4$Growth = y4\n\nggplot(ancova_df4) +\n    geom_point(aes(x = Temp, y = Growth, \n                   shape = Moisture, color = Moisture))\n\n\n\n\n\n\n\nWe can visually notice how the slope of Temperature depends on whether soil Moisture is Wet or Dry. But we need to conduct model simplification to validate this.\n\nm1 = aov(Growth ~ Temp + Moisture + Temp*Moisture, \n         data = ancova_df4)\nsummary(m1)\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemp           1    3.0     3.0   0.857    0.358    \nMoisture       1 2466.5  2466.5 700.191  &lt; 2e-16 ***\nTemp:Moisture  1  303.6   303.6  86.190 1.37e-13 ***\nResiduals     66  232.5     3.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# cannot drop interaction\n# full model is best\n\nm1$coefficients\n\n     (Intercept)             Temp      MoistureWet Temp:MoistureWet \n    15.686070436     -0.487258985      0.005302987      0.773837781 \n\n\nWe see the model coefficients match well to our simulated values. Specifically, there is a positive interaction: the slope of Temperature when soil is Dry is negative (\\(-0.49\\)), but when the soil is Wet, the slope of temperature increases and becomes positive (\\(-0.49 + 0.77 = 0.28\\)).\nLet’s visualize this more clearly with ggplot2.\n\nggplot(ancova_df4, aes(x = Temp, y = Growth, \n                       color = Moisture)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>ANOVA</span>"
    ]
  },
  {
    "objectID": "bayesian.html#lecture-material",
    "href": "bayesian.html#lecture-material",
    "title": "9  Bayesian inference",
    "section": "\n9.1 Lecture material",
    "text": "9.1 Lecture material\nPlease download and print the lecture materials from Bblearn. After lectures, the recordings will appear in the Bblearn Collaborate Ultra section."
  },
  {
    "objectID": "bayesian.html#priors-and-posteriors",
    "href": "bayesian.html#priors-and-posteriors",
    "title": "9  Bayesian inference",
    "section": "\n9.2 Priors and Posteriors",
    "text": "9.2 Priors and Posteriors\nAs a reminder, suppose we are estimating a single parameter in a model, \\(\\theta\\). In Bayesian inference, we have:\n\\[P(\\theta | \\text{Data}) \\propto P(\\theta)P(\\text{Data}|\\theta)\\] In words, this means that the posterior probability distribution of the parameter, \\(P(\\theta | \\text{Data})\\), is proportional to the prior probability distribution of the parameter, \\(P(\\theta)\\), multiplied by the likelihood of the data, given the parameter, \\(P(\\text{Data}|\\theta)\\).\nThe prior probability distribution of the parameter quantifies what we believe the parameter’s true value may be, prior to collecting data. Remember that this prior probability distribution can be “vague,” meaning that we don’t have high confidence in what the parameter value is prior to collecting data. Or, the prior can be “informative,” meaning that we have some level of certainty in what values are most likely for the parameter.\nThe likelihood is the same quantity that we discussed in the sections on Maximum Likelihood. The data likelihood represents how well a model matches the data, given a particular parameter value.\nFinally, the posterior probability distribution represents a type of weighted likelihood - the likelihood weighted by our prior knowledge of what the parameter value might be.\n\n9.2.1 Estimating the mean of a sample\nHere is an example of how we can visualize the relationship between the prior, the likelihood, and the posterior of a parameter. Imagine that we collect a sample of data, \\(y\\), from the population \\(Y\\). Our goal is to estimate the mean of that population, \\(Y\\). Obviously this can be done by calculating the mean outright, but here we want to quantify the posterior probability distribution of the mean, so that we can simultaneously understand the central estimate as well as the uncertainty around that estimate. To do this, we must specify the likelihood of the data. We’ll assume that: \\[y_i \\sim N(\\mu, \\sigma)\\] We’ll assume we know \\(\\sigma\\) with certainty. Our goal then is to estimate the posterior of \\(\\mu\\), \\(P(\\mu | y)\\).\nFirst we’ll generate data points \\(y\\) from a “known” distribution.\n\nset.seed(3)\nn_obs = 15\nmu_known = 8.2\nsigma_fixed = 2.5\ny = rnorm(n_obs, mean = mu_known, sd = sigma_fixed)\n\nhist(y)\n\n\n\n\nObviously in this easy example we could calculate a point estimate of the mean of \\(Y\\) using the mean of sample \\(y\\):\n\nmean(y)\n\n[1] 7.723497\n\n\nBut, this estimate leads to no understanding of the certainty in our estimate of the true mean of population \\(Y\\). Instead, let’s use Bayesian inference. For instance, we know that the true mean of \\(Y\\) is \\(8.2\\), which we simulated. So this estimate of \\(\\mu\\) has error, especially because of low sample size.\nFirst, let’s specify a vague prior probability distribution for \\(\\mu\\). We’ll assume: \\[\\mu \\sim N(0, 50)\\] We can visualize this prior probability density function.\n\nmu_guess = seq(0, 20, length.out = 200)\n# Prior prob distribution\nmu_prior = dnorm(mu_guess, 0, 50, log = TRUE)\n\nplot(exp(mu_prior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\")\"))\n\n\n\n\nNotice that because we made the prior so “vague”, all of the possible values of \\(\\mu\\) that we plotted (ranging from 0 to 20), all have very low probabilities, because basically all possible values of \\(\\mu\\) (ranging negative to positive infinity) have equally low probability with this vauge prior. I’m exaggerating a little bit to make a point.\nNow, we need to create a function to calculate the likelihood of any particular “guess” of \\(\\mu\\).\n\nmu_likelihood = function(this_mu, data){\n    \n    log_lhood = \n        sum(\n            dnorm(data, \n                  mean = this_mu,\n                  sd = sigma_fixed,\n                  log = TRUE)\n        )\n    return(log_lhood)\n}\n\nWe’ve seen these sorts of functions before. Now, let’s calculate the likelihood for each of our “guesses” of \\(\\mu\\).\n\n# Store the likelihoods:\nmu_lhood = NULL\nfor(i in 1:length(mu_guess)){\n    mu_lhood[i] = mu_likelihood(mu_guess[i], y)\n}\n\n# Plot on ln scale:\nplot(exp(mu_lhood) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~y~\"|\"~mu~\")\"))\n\n\n\n\nNow we can calculate the posterior as the product of the prior and the likelihood (or the sum of the log-scale values of these distributions).\n\nmu_posterior = mu_prior + mu_lhood\n\nplot(exp(mu_posterior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\"|\"~y~\")\"))\nabline(v = mu_known, lty = 1)\nabline(v = mean(y), lty = 2)\n\n\n\n\nWhat we see here is that with the vague prior, the posterior basically reflects the data likelihood. The prior gave no additional information to the analysis.\nNow let’s see how the posterior might change with a more informative prior that is actually biased to the incorrect value of \\(\\mu\\), such as \\(\\mu \\sim N(2, 0.75)\\).\n\n# Prior prob distribution\nmu_prior = dnorm(mu_guess, 2, 0.75, log = TRUE)\n\nplot(exp(mu_prior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\")\"))\n\n\n\nmu_posterior = mu_prior + mu_lhood\n\nplot(exp(mu_posterior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\"|\"~y~\")\"))\nabline(v = mu_known, lty = 1)\nabline(v = mean(y), lty = 2)\n\n\n\n\nNow what we see more clearly is that the posterior is the data likelihood weighted by the prior. In this case, because we have very few data points, the posterior is particularly sensitive to the prior of \\(\\mu\\).\n\n9.2.2 Using Monte Carlo sampling\nIn reality, we are estimating more than one parameter in a model. Therefore, estimating the posterior of the model means estimating the joint posterior of the model parameters, so that we can quantify the marginal posterior estimate of each model parameter.\nTherefore we use an algorithm to “sample from” the joint posterior. We don’t have enough time to explain the available algorithms in detail (see INF626: Applied Bayesian Modeling). However, these algorithms typically employ a variant of Monte Carlo sampling (e.g., Markov chain Monte Carlo (MCMC) or Hamiltonian Monte Carlo (HMC)). The statistical programming language Stan uses HMC, and we will employ Stan via the R package rstan.\nWe’ll demonstrate rstan in class, because it is difficult to run on the website code here."
  },
  {
    "objectID": "bayesian.html#background",
    "href": "bayesian.html#background",
    "title": "9  Bayesian inference",
    "section": "\n9.2 Background",
    "text": "9.2 Background\nAs a reminder, suppose we are estimating a single parameter in a model, \\(\\theta\\). In Bayesian inference, we have:\n\\[P(\\theta | \\text{Data}) \\propto P(\\theta)P(\\text{Data}|\\theta)\\] In words, this means that the posterior probability distribution of the parameter, \\(P(\\theta | \\text{Data})\\), is proportional to the prior probability distribution of the parameter, \\(P(\\theta)\\), multiplied by the likelihood of the data, given the parameter, \\(P(\\text{Data}|\\theta)\\).\nThe prior probability distribution of the parameter quantifies what we believe the parameter’s true value may be, prior to collecting data. Remember that this prior probability distribution can be “vague,” meaning that we don’t have high confidence in what the parameter value is prior to collecting data. Or, the prior can be “informative,” meaning that we have some level of certainty in what values are most likely for the parameter.\nThe likelihood is the same quantity that we discussed in the sections on Maximum Likelihood. The data likelihood represents how well a model matches the data, given a particular parameter value.\nFinally, the posterior probability distribution represents a type of weighted likelihood - the likelihood weighted by our prior knowledge of what the parameter value might be."
  },
  {
    "objectID": "bayesian.html#estimating-the-mean-of-a-sample",
    "href": "bayesian.html#estimating-the-mean-of-a-sample",
    "title": "9  Bayesian Inference",
    "section": "\n9.2 Estimating the mean of a sample",
    "text": "9.2 Estimating the mean of a sample\nHere is an example of how we can visualize the relationship between the prior, the likelihood, and the posterior of a parameter. Imagine that we collect a sample of data, \\(y\\), from the population \\(Y\\). Our goal is to estimate the mean of that population, \\(Y\\). Obviously this can be done by calculating the mean outright, but here we want to quantify the posterior probability distribution of the mean, so that we can simultaneously understand the central estimate as well as the uncertainty around that estimate. To do this, we must specify the likelihood of the data. We’ll assume that: \\[y_i \\sim N(\\mu, \\sigma)\\] We’ll assume we know \\(\\sigma\\) with certainty. Our goal then is to estimate the posterior of \\(\\mu\\), \\(P(\\mu | y)\\).\nFirst we’ll generate data points \\(y\\) from a “known” distribution.\n\nset.seed(3)\nn_obs = 15\nmu_known = 8.2\nsigma_fixed = 2.5\ny = rnorm(n_obs, mean = mu_known, sd = sigma_fixed)\n\nhist(y)\n\n\n\n\n\n\n\nObviously in this easy example we could calculate a point estimate of the mean of \\(Y\\) using the mean of sample \\(y\\):\n\nmean(y)\n\n[1] 7.723497\n\n\nBut, this estimate leads to no understanding of the certainty in our estimate of the true mean of population \\(Y\\). Instead, let’s use Bayesian inference. For instance, we know that the true mean of \\(Y\\) is \\(8.2\\), which we simulated. So this estimate of \\(\\mu\\) has error, especially because of low sample size.\nFirst, let’s specify a vague prior probability distribution for \\(\\mu\\). We’ll assume: \\[\\mu \\sim N(0, 50)\\] We can visualize this prior probability density function.\n\nmu_guess = seq(0, 20, length.out = 200)\n# Prior prob distribution\nmu_prior = dnorm(mu_guess, 0, 50, log = TRUE)\n\nplot(exp(mu_prior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\")\"))\n\n\n\n\n\n\n\nNotice that because we made the prior so “vague”, all of the possible values of \\(\\mu\\) that we plotted (ranging from 0 to 20), all have very low probabilities, because basically all possible values of \\(\\mu\\) (ranging negative to positive infinity) have equally low probability with this vauge prior. I’m exaggerating a little bit to make a point.\nNow, we need to create a function to calculate the likelihood of any particular “guess” of \\(\\mu\\).\n\nmu_likelihood = function(this_mu, data){\n    \n    log_lhood = \n        sum(\n            dnorm(data, \n                  mean = this_mu,\n                  sd = sigma_fixed,\n                  log = TRUE)\n        )\n    return(log_lhood)\n}\n\nWe’ve seen these sorts of functions before. Now, let’s calculate the likelihood for each of our “guesses” of \\(\\mu\\).\n\n# Store the likelihoods:\nmu_lhood = NULL\nfor(i in 1:length(mu_guess)){\n    mu_lhood[i] = mu_likelihood(mu_guess[i], y)\n}\n\n# Plot on ln scale:\nplot(exp(mu_lhood) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~y~\"|\"~mu~\")\"))\n\n\n\n\n\n\n\nNow we can calculate the posterior as the product of the prior and the likelihood (or the sum of the log-scale values of these distributions).\n\nmu_posterior = mu_prior + mu_lhood\n\nplot(exp(mu_posterior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\"|\"~y~\")\"))\nabline(v = mu_known, lty = 1)\nabline(v = mean(y), lty = 2)\n\n\n\n\n\n\n\nWhat we see here is that with the vague prior, the posterior basically reflects the data likelihood. The prior gave no additional information to the analysis.\nNow let’s see how the posterior might change with a more informative prior that is actually biased to the incorrect value of \\(\\mu\\), such as \\(\\mu \\sim N(2, 0.75)\\).\n\n# Prior prob distribution\nmu_prior = dnorm(mu_guess, 2, 0.75, log = TRUE)\n\nplot(exp(mu_prior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\")\"))\n\n\n\n\n\n\nmu_posterior = mu_prior + mu_lhood\n\nplot(exp(mu_posterior) ~ mu_guess,\n     type = \"l\",\n     xlab = expression(mu),\n     ylab = expression(\"P(\"~mu~\"|\"~y~\")\"))\nabline(v = mu_known, lty = 1)\nabline(v = mean(y), lty = 2)\n\n\n\n\n\n\n\nNow what we see more clearly is that the posterior is the data likelihood weighted by the prior. In this case, because we have very few data points, the posterior is particularly sensitive to the prior of \\(\\mu\\).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#using-monte-carlo-sampling",
    "href": "bayesian.html#using-monte-carlo-sampling",
    "title": "9  Bayesian Inference",
    "section": "\n9.3 Using Monte Carlo sampling",
    "text": "9.3 Using Monte Carlo sampling\nIn reality, we are estimating more than one parameter in a model. Therefore, estimating the posterior of the model means estimating the joint posterior of the model parameters, so that we can quantify the marginal posterior estimate of each model parameter.\nTherefore we use an algorithm to “sample from” the joint posterior. As discussed in lecture, these algorithms typically employ a variant of Monte Carlo sampling (e.g., Markov chain Monte Carlo (MCMC) or Hamiltonian Monte Carlo (HMC)). The statistical programming language Stan uses HMC, and we will employ Stan via the R package rstan. See Footnotes 9.4.1 for a coded Metropolis-Hastings MCMC, which we described in lecture.\nNote that because we are using Quarto documents, we need to set up the Stan model in a very particular way, to get it to work with code chunks. Usually, when using a .R file, we create a separate .stan file, and then we run the rstan::stan() function in the .R file, while referencing the .stan file that should be saved in our working directory. Here, we will create and compile the .stan file in one place (in the Quarto document). I will put an example set of .R and .stan files on BBLearn that compliment the following examples.\n\n9.3.1 Estimating the mean of a sample using Stan\nFirst, we will load rstan and set some required options.\n\nlibrary(rstan)\n\nWarning: package 'rstan' was built under R version 4.3.1\n\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.3 (Stan version 2.26.1)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nNext, we will create a .stan model that allows us to estimate the mean only.\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  real&lt;lower=0&gt; sigma_fixed;\n}\n\n// The parameters accepted by the model. \nparameters {\n  real mu;\n}\n\n// The model to be estimated.\nmodel {\n  // priors\n  mu ~ normal(0, 50.0);\n  \n  // likelihood\n  y ~ normal(mu, sigma_fixed);\n}\n\nWhen the chunk above is run, it compiles the Stan model into C++ code that gets run in the background. Next, we will use R code to set up and run the Stan model to estimate the parameter \\(\\mu\\).\n\n# create a list for stan\nmu_fit_data = list(\n    N = length(y),\n    y = y,\n    sigma_fixed = sigma_fixed\n)\n\n# Fit the compiled model using stan defaults\nfit = sampling(estimate_mu, # this is generated from the previous code-chunk\n               data = mu_fit_data)\n\n# Summarize the output\nprint(fit)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nmu    7.71    0.02 0.65  6.46  7.28  7.71  8.15  8.99  1439    1\nlp__ -4.85    0.02 0.69 -6.87 -5.04 -4.58 -4.40 -4.35  2085    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jan  9 17:27:37 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nThis print() format shows us the mean and median (50%) estimates of \\(\\mu\\). The output also shows various levels of the “credible intervals”. For instance, the 2.5% and 97.5% would give us the ends of the “95% credible interval”, whereas the 25% and 75% would give us the ends of the “50% credible interval”.\nWe can also generate various summary visualizations.\n\nplot(fit)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\nplot(fit, show_density = TRUE)\n\nci_level: 0.8 (80% intervals)\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\nplot(fit, plotfun = \"hist\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThese three plots summarize the posterior estimate of \\(\\mu\\) in various ways.\nNext, we can observe the “traceplot” which shows the outcome of the 4 HMC chains that sample from the posterior.\n\nplot(fit, plotfun = \"trace\")\n\n\n\n\n\n\n\nWe can also show the “warmup” phase, which emphasizes how the initial “proposal” can be far outside of the true posterior.\n\n# Include the warmup phase\nplot(fit, plotfun = \"trace\", inc_warmup = TRUE)\n\n\n\n\n\n\n\n\n9.3.2 Estimating the mean and the standard deviation using Stan\nNow let’s assume the more likely case in which we do not know the mean nor the standard deviation of the sample, so we need to estimate \\(\\mu\\) and \\(\\sigma\\) of the normal distribution.\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\n\n// The parameters accepted by the model. \nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\n\n// The model to be estimated.\nmodel {\n  // priors\n  mu ~ normal(0, 50.0);\n  sigma ~ cauchy(0, 1);\n  \n  // likelihood\n  y ~ normal(mu, sigma);\n}\n\nNotice how now we have two parameters in the parameters code block. We also are specifying two prior distributions, one for \\(\\mu\\) and one for \\(\\sigma\\). We are using the cauchy probability distribution for \\(\\sigma\\), which is useful in part because this distribution ensures that \\(\\sigma &gt; 0\\).\nNext, we will use R code to set up and run the Stan model.\n\n# create a list for stan\nmu_sigma_fit_data = list(\n    N = length(y),\n    y = y\n)\n\n# Fit the \"model\" using stan defaults\nfit2 = sampling(estimate_mu_sigma, \n                data = mu_sigma_fit_data)\n\n# Summarize the output\nprint(fit2)\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nmu      7.74    0.01 0.54   6.67   7.39   7.74   8.09   8.82  2279    1\nsigma   2.05    0.01 0.42   1.42   1.76   1.98   2.26   3.01  2687    1\nlp__  -19.14    0.03 1.10 -22.09 -19.58 -18.80 -18.35 -18.06  1458    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jan  9 17:27:57 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nNow the print() statement shows the output for both \\(\\mu\\) and \\(\\sigma\\). And we can see that the true values of both parameters lie within the 95% credible interval of the posterior.\nWe can also generate various summary visualizations.\n\nplot(fit2, plotfun = \"hist\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nplot(fit2, show_density = TRUE)\n\nci_level: 0.8 (80% intervals)\nouter_level: 0.95 (95% intervals)\n\n\n\n\n\n\n\nplot(fit2, plotfun = \"trace\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "bayesian.html#footnotes",
    "href": "bayesian.html#footnotes",
    "title": "9  Bayesian Inference",
    "section": "\n9.4 Footnotes",
    "text": "9.4 Footnotes\n\n9.4.1 An MH-MCMC algorithm\nBelow is a coded MH-MCMC algorithm for estimating one parameter. In this case the mean of a normal distribution: \\[y \\sim N(\\theta, \\sigma^2)\\] Here \\(\\theta\\) is the mean of the normal distribution, which is unknown, but \\(\\sigma\\) is known.\n\nset.seed(10)\n\n# We will estimate true_theta\n# Assume:\ntrue_theta = 3.0\nsd_known = 2.0\n#-----------------------------------\n#-----------------------------------\n# Test data set\ny_test = rnorm(150, true_theta, sd_known)\n#-----------------------------------\n#-----------------------------------\n\n# SET UP THE MCMC \n\n# Parameters of the normal prior on true_theta\n# i.e. true_theta ~ Normal(mu_prior, sd_prior)\nmu_prior = 3.0\nsd_prior = 5.0\n\n# Same with the proposal distribution\n## Notice that the width of the proposal \n## is 25% greater than the prior\nmu_prop = 3.0\nsd_prop = sd_prior * 1.25\n\n# Number of MCMC iterations\nn_iter = 15000\n\n# Set up storage (vector)\nchosen_theta = vector(\"numeric\", length = n_iter)\n\nfor(i in 1:n_iter){\n  \n  if(i == 1){ # First iteration\n    # Choose theta from proposal distribution\n    old_theta = rnorm(1, mu_prop, sd_prop)\n  }\n  \n  # Draw new theta from proposal:\n  new_theta = rnorm(1, mu_prop, sd_prop)\n  \n  # Calculate proposal adjustment:\n  old_prop_adj = dnorm(old_theta, mu_prop, sd_prop, log = TRUE)\n  new_prop_adj = dnorm(new_theta, mu_prop, sd_prop, log = TRUE)\n  \n  # Calculate prior prob:\n  old_prior = dnorm(old_theta, mu_prior, sd_prior, log = TRUE)\n  new_prior = dnorm(new_theta, mu_prior, sd_prior, log = TRUE)\n  \n  # Calculate data likelihood:\n  old_lik = sum(dnorm(y_test, old_theta, sd_known, log = TRUE))\n  new_lik = sum(dnorm(y_test, new_theta, sd_known, log = TRUE))\n  \n  # Calculate posterior density:\n  old_post = old_prior + old_lik\n  new_post = new_prior + new_lik\n  \n  # Calculate acceptance ratio:\n  log_ratio = (new_post - new_prop_adj) - (old_post - old_prop_adj)\n  ratio = exp(log_ratio)\n  \n  # Make decision:\n  if(ratio &gt; 1){\n    # Keep new theta\n    chosen_theta[i] = new_theta\n  }else{\n    \n    rand = runif(1, min = 0, max = 1)\n    \n    if(ratio &lt;= rand){\n      # Reject new theta (i.e., keep old_theta)\n      chosen_theta[i] = old_theta\n    }else{\n      chosen_theta[i] = new_theta\n    }\n    \n  }\n  \n  # Update what is \"old\" value\n  old_theta = chosen_theta[i]\n  \n}\n\n#-----------------------------------\n#-----------------------------------\n\n# Plot the trace:\nplot(chosen_theta ~ c(1:n_iter), type = \"l\",\n     xlab = \"Iteration\", ylab = expression(theta~\"|\"~y))\n\n\n\n\n\n\n# Plot the histogram:\n## Cut out the warmup or \"burn-in\" phase\nn_burn = n_iter / 2\nhist(chosen_theta[n_burn:n_iter], breaks = 25)\nabline(v = true_theta, lwd = 3)\n\n\n\n\n\n\n# Quantiles:\nquantile(chosen_theta[n_burn:n_iter],\n         probs = c(0.025, 0.5, 0.975))\n\n    2.5%      50%    97.5% \n2.473926 2.847643 3.144811 \n\nsd(chosen_theta[n_burn:n_iter])\n\n[1] 0.1565706\n\n\nWe can see that the true value of \\(\\theta\\) is within the 95% credible interval of the marginal posterior distribution.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Bayesian Inference</span>"
    ]
  },
  {
    "objectID": "ols.html",
    "href": "ols.html",
    "title": "4  Ordinary Least Squares",
    "section": "",
    "text": "4.1 In-class Code\nRemember that our goal is to estimate the linear relationship between data observations of response variable, \\(y\\), and its measured covariate, \\(x\\), following: \\(Y = XB + \\epsilon\\), where \\(\\epsilon \\sim N(0, \\sigma^2 I).\\) Our coefficients to estimate are therefore \\(\\hat{B}\\), which is a column vector of the intercept and slope. We also estimate the standard deviation of the residuals (i.e., residual error), \\(\\hat{\\sigma}\\). To estimate the coefficients, we are attempting to minimize the residual sum of squares, \\(|| \\epsilon || ^ 2\\). See Footnotes 4.12.1 for more information regarding this notation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Ordinary Least Squares</span>"
    ]
  },
  {
    "objectID": "Rintro.html",
    "href": "Rintro.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "2.1 Load a package\n# The library() function loads R packages that are not supplied in the \"base\" software\n# You need to install a package once before loading\n# install.packages('dplyr')\n# Load the 'dplyr' package which has many convenient data manipulation functions\nlibrary(dplyr)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Appendix A: Syllabus",
    "section": "",
    "text": "A.1 Canvas & Recorded Lectures\nWe will use the learning management system, Canvas, to conduct some course business, including assignment disbursement and submitting. I will use Canvas to record lectures for future viewing.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Syllabus</span>"
    ]
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "3  Probability Distributions",
    "section": "",
    "text": "3.1 Gaussian (Normal) distribution\nAs we learned in lecture, the normal distribution is defined by two parameters, the mean \\(\\mu\\) and the standard deviation \\(\\sigma\\). Here, we will use the normal distribution to demonstrate some of R’s functions to describe probability distributions and to draw random numbers from probability distributions. Let’s assume that random variable \\(x\\) follows a normal distribution, \\(x_i \\sim N(\\mu, \\sigma)\\).\n# Define the parameters\nmu = 10\nsigma = 2.5\n\n# Visualize the probability density function (pdf)\nx_vals = seq(0, 50, by = 0.1)\nnorm_pdf = dnorm(x_vals, mean = mu, sd = sigma)\n\n# Let's use some of the other R functions to describe the distribution\n\n## What is the probability density of specific values?\n## mean\np_mu = dnorm(mu, mean = mu, sd = sigma)\n## The next two values will describe the 95% probability density bounds\n## (Low) 2.5% cut off \nx_low95 = qnorm(0.025, mean = mu, sd = sigma)\np_low95 = dnorm(x_low95, mean = mu, sd = sigma)\n## (High) 97.5% cut off\nx_high95 = qnorm(0.975, mean = mu, sd = sigma)\np_high95 = dnorm(x_high95, mean = mu, sd = sigma)\n\n# So, what is the P(x &lt;= x_high95)??\npnorm(x_high95, mean = mu, sd = sigma)\n\n[1] 0.975",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "model-select.html",
    "href": "model-select.html",
    "title": "7  Model Selection",
    "section": "",
    "text": "7.1 Generate the data\nHere we will demonstrate two approaches to model comparison. But first let’s generate data, in the same way we did for multiple linear regression in (Chapter 4). Note that in this case, we will specify that two of the input variables have zero slope (i.e., no linear association with the outcome variable).\nn = 40\nn_covariate = 4\np = n_covariate + 1\n\nbetas = vector(\"numeric\", length = p)\nxmat = matrix(0, nrow = n, ncol = p)\nsigma = 2.25\n\n# Column for intercept\nxmat[,1] = 1\n\n# Generate the covariate data randomly:\nset.seed(5)\nxmat[,2] = rnorm(n, mean = 5, sd = 8)\nxmat[,3] = runif(n, min = 0, max = 20)\nxmat[,4] = rchisq(n, df = 50)\nxmat[,5] = rpois(n, lambda = 10)\n\n# Set the betas:\nbetas[1] = 1.0\nbetas[2] = 0.0\nbetas[3] = -0.2\nbetas[4] = 0.0\nbetas[5] = 1.8\n\n# Calculate the observed 'y', adding residual error\ny = xmat %*% betas + rnorm(n, mean = 0, sd = sigma)\n\npar(mfrow=c(2,2))\nfor(i in 2:p){\n    plot(y ~ xmat[,i],\n         xlab = paste(\"covariate \", i-1))\n}\n\n\n\n\n\n\n# Create a data.frame\nmy_df = data.frame(y, xmat[,2:5])\nhead(my_df)\n\n          y         X1        X2       X3 X4\n1 28.492690 -1.7268438 18.788730 38.39431 18\n2 14.221411 16.0748747 16.474910 60.76146 10\n3  9.064956 -5.0439349  4.223082 33.40577  5\n4  9.366421  5.5611421  1.832589 41.76465  5\n5 18.874673 18.6915270  9.405498 40.26706 11\n6 17.706978  0.1767361  1.001228 46.05881 10\n\n# Run the model, report the summary\nm1 = lm(y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\nm1_summary = summary(m1)\nm1_summary\n\n\nCall:\nlm(formula = y ~ 1 + X1 + X2 + X3 + X4, data = my_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3257 -1.4053 -0.4331  1.3299  4.3178 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.41757    1.82632   1.871  0.06968 .  \nX1           0.03245    0.03810   0.852  0.40016    \nX2          -0.26989    0.07297  -3.698  0.00074 ***\nX3          -0.01823    0.03267  -0.558  0.58050    \nX4           1.68543    0.11083  15.207  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.94 on 35 degrees of freedom\nMultiple R-squared:  0.8901,    Adjusted R-squared:  0.8775 \nF-statistic: 70.86 on 4 and 35 DF,  p-value: 2.741e-16",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Model Selection</span>"
    ]
  },
  {
    "objectID": "hypothesis.html#interpretation",
    "href": "hypothesis.html#interpretation",
    "title": "5  Hypothesis Testing",
    "section": "\n5.2 Interpretation",
    "text": "5.2 Interpretation\nBelow, we will review how the t value and Pr(&gt;|t|) are generated manually. But what’s most important is that we understand how to interpret the results in the summary table. The Pr(&gt;|t|) represents the \\(p\\)-value for the hypothesis test related to the slopes. Specifically, the null hypothesis is that a particular slope (\\(\\hat{\\beta}_i\\)) is equal to zero, meaning there is no linear association between input variable \\(i\\) and the outcome variable \\(y\\). If the \\(p\\)-value is less than our conventional threshold (\\(\\alpha = 0.05\\)) than the probability that our null hypothesis is true is sufficiently low that we reject this null hypothesis. Rejecting the null hypothesis allows us to state, with statistical support, that there is a linear effect of input variable \\(i\\) on outcome \\(y\\).\nIn the above example, we see that the \\(p\\)-value is less than 0.05 for input variables 1, 2, and 4. We can report these results as follows.\n“Our multiple linear regression analysis detected positive linear associations between \\(y\\) and input variables 1 (\\(t_{75} = 20.69; p &lt; 0.001\\)) and 4 (\\(t_{75} = 20.52; p &lt; 0.0001\\)), and we also discovered a negative linear effect of input variable 2 on \\(y\\) (\\(t_{75} = -29.27; p &lt; 0.0001\\)). We did not detect a significant effect of input variable 3 (\\(t_{75} = -0.129; p = 0.898\\)).”\nWhat you see in the interpretation above is that we report which input variables have significant linear effects (based on the \\(p\\)-values), and we also describe the direction of the effect. For instance a “negative” effect implies that as the input variable increases, the outcome variable decreases. This is simply determined by the direction of the slope.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  }
]